This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.github/
  workflows/
    ci.yml
    release.yml
docs/
  ADR/
    ADR-001-cli-rag-toml.md
    ADR-002-TUI-planning.md
    ADR-003-CLI-review-&-refactor.md
    ADR-004-toml-config-versioning.md
    ADR-DRAFT-NeoRAG-planning.md
    ADR-DRAFT-org-agenda.md
    ADR-DRAFT-simple-query-dsl.md
    Opus Advice --Distilled Down.md
src/
  bin/
    adr-rag.rs
  commands/
    cluster.rs
    completions.rs
    doctor.rs
    get.rs
    graph.rs
    group.rs
    init.rs
    mod.rs
    output.rs
    path.rs
    search.rs
    topics.rs
    validate_cmd.rs
    watch_cmd.rs
  cli.rs
  config.rs
  discovery.rs
  graph.rs
  index.rs
  lib.rs
  model.rs
  protocol.rs
  util.rs
  validate.rs
  watch.rs
tests/
  integration_cluster.rs
  integration_config_precedence.rs
  integration_doctor.rs
  integration_group_validate.rs
  integration_help.rs
  integration_init.rs
  integration_search_topics.rs
  integration_validate_json.rs
  integration_validate_write_groups.rs
.adr-rag.toml
.gitignore
AGENTS.md
Cargo.toml
CLAUDE.md
Justfile
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(cargo build:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="docs/ADR/ADR-001-cli-rag-toml.md">
---
id: ADR-001
tags: [config, toml, cli, tera]
status: proposed
depends_on: none
created_date: 2025-08-24
last_modified: 2025-08-24
related_files: [.cli-rag.toml]
---

# cli-rag-toml

## Example `.toml` Block 

```TOML
[file.paths]
# --- The root directories that cli-rag will scan for file changes. --- 
# --- Mutliple inputs are accepted and will be merged. ---
filepaths = [
"file/path",
"file/other/path",
]
  
# --- By default the index will be created at `bases-path(s)/index/*.json`. ---
# --- File paths used below must be present at the same directory level or below this .toml file. --- 
# index_relative = "alternate/filepath/adr-index.json"
# groups_relative = "alternate/filepath/semantic-groups.json"

# --- Remove directories or patterns from scanning to improve speed in large projects. ---     
ignore_globs  = ["**/node_modules/**", "**/src/**", "**/lib/**", "**/dist/**"]
 
# --- Control the default retrevial settings for adr-rag cluster etc --- 
[defaults]
default_editor = "micro"
depth = 2
include_bidirectional = true
include_content = true

# --- Define a title format. legal are `kebab-case`, `camelCase`, `snake_case`, `SCREAMING_SNAKE_CASE`, and `PascalCase`. Default is `kebab-case` ---
# --- This setting is applied globally but is overridden by if called again in a specific schema ---
# title.format = "kebab-case"

# --- SCHEMAS ---
# 1. Define `[[schema]]` blocks to create a note type and map how it's asvalidated. 
# 2. Set one or more `file_patterns`. These will determine what notes your rules are applied to. Notes must be under your defined filepaths to be discovered. 
# 3. Set `identity_source` to "frontmatter" for rich yaml notes or "title" for simple applications that only require wiki style linking. 
# 4. Define the plain and validated frontmatter if present plus any regex or wasm rules 
# 5. Use `template` to define the full structure of the final note. You can use placeholders like {{id}}, {{title}} and ((frontmatter)) for dynamic entry 

I should make [[table]] here to show all variable options once i know what that list ! ! ! 

# --- For a cleaner .`cli-rag.toml` you can provide a full schema in one or multiple dedicated documents ---  
# import = "templates/adr.toml"
# import = "templates/lore.toml"


# --- Front Matter Defined Notes --- 
[[schema]]
name = "ADR"
file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
identity_source = "frontmatter" 

# --- Required frontmatter will require the item be present and not empty in order to succeed without error --- 
# --- specific regex rules can be manually defined --- 
required_frontmatter = ["id", "tags", "status", "related_files"]

# --- Determines if undefined frontmatter should thrown an error if detected. Use if your notes should have ONLY the required frontmatter. Useful for limiting llm creativity.  ---
# -- Accepts `strict` and `loose`. Defaults to loose if undefined. ---
unknown_frontmatter_policy = "strict" 

# --- `validated_frontmatter` are built tools that will run validation logic beyond simply checking if it exist and has content --- 
# |-----------------|--------------------------------------------------------------------------------|
# | `depends_on`    | validates the id exists and is valid or throws and error                       |
# |-----------------|--------------------------------------------------------------------------------|
# | `created_date`  | Updates the note with system time when created using the `new` command         |
# |-----------------|--------------------------------------------------------------------------------|
# | `last_modified` | if `watch` is active the note will be updated with a new modified time on edit |
# |-----------------|--------------------------------------------------------------------------------|
validated_frontmatter = ["depends_on", "created_date", "last_modified"]

# --- Determine if missing headings from the template should throw an error. ---
# -- Accepts `strict` and `loose`. Defaults to loose if undefined. ---
heading_policy = "loose"

# --- Set specific rules for legal frontmatter ---  
# --- Severity can be set to Error | Warning | Ignore -- 
[schema.rules.status]
allowed_plain = ["draft", "proposed", "accepted", "superseded", "cancelled"]
severity = "error"

[schema.rules.related_files]
allowed_regex = ["*.ex*", "*.py", "*.js", "*.md", "*.toml"]
severity = "warning"

# --- ADVANCED USERS: Create a WASM module that handles all validation for your schema. Buyer beware. --- 
# validator_wasm = "validators/ADR.wasm"

# --- Complete template for new notes. This is used by the `new` command. Treat this as an LLM prompt. ---  
template = """
((required_frontmatter))

# {{id}}-{{title}}

## Objective
<!-- A concise statement explaining the goal of this decision. -->

## Context
<!-- What is the issue that we're seeing that is motivating this decision or change? -->

## Decision
<!-- What is the change that we're proposing and/or doing? -->

## Consequences
<!-- What becomes easier or more difficult to do because of this change? -->

## Updates
<!-- Changes that happened when the rubber met the road -->
"""

# --- Alterantively you can create simple file name defined notes with minimal linking --- 
# [[schema]]
# name = "Lore"
# file_patterns = ["lore-*.md"]
# identity_source = "filename"
# minimum.links = "2"
# minimum.backlinks = "0"

# template = """
# {{title}}

## Persona
# <!-- The core of the character -->

## Story so Far 
# <!-- Compressed recent events --> 

## Related
# <!-- Links to related content --> 
# """
```

## Tech Details 

- We can create the final notes from the .toml using `Tera` | https://github.com/Keats/tera
- Titles can be forced to the correct case using either `heck` or `convert_case`. 
- A **very large** number of names will need to be changed across the codebase to reflect the above. 
- The `validator_wasm` is more v2.0 north star in concept. The number of users likely to use it is vanishingly small but it's a compelling idea. 


## Scratchpad 

- The defaults section isn't good and should probably be broken up logically and also just expanded out sensibly. I'm not sold on these categories but the idea feels broadly fair.  

```toml
# --- Settings related to creating and editing notes ---
[authoring]
# The editor to launch for new or existing notes.
# Uses $EDITOR or $VISUAL if not set.
editor = "micro"

# The case format for titles when creating new notes from a template.
# legal are `kebab-case`, `camelCase`, `snake_case`, etc.
# This can be overridden per-schema.
title_format = "kebab-case"

# Default status to apply to a new note if not specified in the template.
# default_status = "draft"

# --- Default settings for graph traversal commands (cluster, graph, path) ---
[graph]
# Default depth for traversing dependencies.
depth = 2

# Whether to include dependents (backlinks) in traversals.
include_bidirectional = true

# --- Default settings for content retrieval commands (get, group) ---
[retrieval]
# Whether to include the full markdown content in the output.
include_content = true
```

## Update Log
</file>

<file path="docs/ADR/ADR-002-TUI-planning.md">
---
id: ADR-002
tags:
  - TUI
  - NeoVIM
  - rataTUI
status: proposed
depends_on:
  - ADR-01
  - ADR-003
created_date: 2025-08-24
last_modified: 2025-08-24
related_files: []
---

# CLI-RAG Workflow Planning

## TUI (Initial Concept)

1. The user opens a new repo and types `cli-rag init` which checks for a .cli-rag.toml and if not present opens their editor with a marked up config file. They can either save this or exit the process. 
   
   See [[ADR-001-cli-rag-toml]] for a full example. 

2. Once the .cli-rag.toml is created it opens the TUI "master control" screen e.g. Magit/org style tab collapsible lists. This expands as tracked notes are added. 
   
```bash
v **Templates** 
	1 - IMP-*  
	2 - ADR-*
	3 - ADR-DB-*
		
v **Tracked Notes** 
	- ADR-001-my-first-plan
	- ADR-002-my-happy-place
	- ADR-003-oh-god-oh-fuck
	- ADR-004-panic-refactor
	* ADR-DB-001-event-store
	* ADR-DB-002-idempotency
	+ IMP-001-database-spike
	+ IMP-002-websocket-spike  
	...	        
TAB: Collapse/Expand | RETURN: Select | SPACE+F: FuzzyFind | G: GraphView
```

3.  The user can begin creating notes populated with expected (empty) frontmatter and template. Selecting a note or a template opens the editor for the user. When the close the editor the TUI catches the exit code and rebuilds the index and adds the new tracked notes to the master control screen. 

4. Ideally we would have a simple fuzzy finder that is accessible in all windows with the same keystrokes. It only indexes tracked notes. This and the master control screen let you fly around the knowledge base. 
   
5. GraphView the leans on graphviz dot view's ability to render out ascii. Could use the [[ADR-DRAFT-simple-query-dsl]] and let the user navigate the graph by pulling different clusters. I'm not sure in the real world if it would feel super useful but the lift should be low and it's fun and worth trying. 
   
## Implementation Details 

- RataTUI is the most likely back-end here. 
- The `watch` command is active while the TUI is running
   
## The NeoVim Advantage

All of the above can be much smoother in neovim. In theory we could 

- Define a consistent naviation UI using a leader key. 
- Lean on Existing fuzzy finding and implement less ourselves. 
- Offer a first class editing experience. We could in theory parse our index and have tree-sitter powered live linting of valid id's and note names
- make [[links]] and ID: ADR-005 directly navigable from the editor. 

This is ultimately inching much closer to a programmer friendly obsidian that lives in a repo without fuss. I'm not sure if that's a good or a bad thing. It would without a doubt be the most comfortable and fully featured version. 
  
However I do think asking people to configure a full neovim environment is a notably higher ask.
</file>

<file path="docs/ADR/ADR-003-CLI-review-&-refactor.md">
---
id: ADR-003
tags:
  - cli
  - refactor
  - TUI
  - neovim
status: draft
depends_on:
  - ADR-001
  - ADR-004
created_date: 2025-08-24
last_modified: 2025-08-25
related_files:
  - ~/cli-rag/src/commands
---

# ADR-004-CLI-review-&-refactor

## Objective
<!-- A concise statement explaining the goal of this decision. -->

Until this point CLI-RAG has grown adhoc out of a a dog-fooded .js script. As the workflow is becoming more clear the existing commands should be reviewed and adjusted to ensure everything is coherent. 

## Context
<!-- What is the issue that we're seeing that is motivating this decision or change? -->

The current command list 


- `init` — Create `.adr-rag.toml` in the current repo and open it in an editor by default. Flags:
    - `--force` overwrite if exists
    - `--print-template` print template to stdout
    - `--silent` do not open the config after creating/detecting it
- `doctor` — Show resolved config, bases, discovery mode (index vs scan), and quick stats.
    - Reports per-type counts when schemas are defined, and unknown-key stats.
    - JSON: use global `--format json`.
- `search --query <substr>` — Fuzzy search by ID/title across discovered ADR files.
- `topics` — List semantic groups derived from front matter (`groups` in ADRs).
- `group --topic "<name>" [--include-content]` — Show ADRs in a group; optionally include full content.
- `get --id ADR-021 [--include-dependents]` — Print an ADR with dependencies (and dependents if requested).
- `cluster --id ADR-021 [--depth N] [--include-bidirectional]` — Traverse dependencies (and dependents) to depth.
- `graph --id ADR-021 [--depth N] [--include-bidirectional] [--format mermaid|dot|json]` — Export a dependency graph around an ADR.
- `path --from ADR-011 --to ADR-038 [--max-depth N]` — Find a dependency path if any.
- `validate [--format json] [--dry-run] [--full-rescan] [--write-groups]` — Validate front matter/refs; on success writes indexes (unless `--dry-run`).
- Incremental by default: only reparses changed files using mtime/size. Use `--full-rescan` to force scanning all.
- Exits non-zero if validation fails.
- `watch [--debounce-ms 400] [--dry-run] [--full-rescan]` — Watch bases for changes and incrementally validate + update indexes.
- Debounces rapid events; writes on success (unless `--dry-run`).

## Decision
<!-- What is the change that we're proposing and/or doing? -->

### The Universal Flag

#### `--format Plain|Json|Ndjson`

All command EXCEPT for `graph` will accept a `--format` flag. This is designed for integrating the library with tooling and the eventual Model Context Protocol Server. 

### The Essential Functions 

#### Init 
Create `.adr-rag.toml` in the current repo and open it in an editor by default.

**Flags** 
- `--silent`: Write the default template directly to disk without opening the config in the editor 
- `--force`: Overwrite `.cli-rag.toml` even if it exists
- `--print-template`: Print template to stdout

#### validate 

Lint's notes based on the various [[schema]] configured in `.cli-rag.toml`. By default all index scans are incremental reparse's using `mtime` and `size`. Exits with an error code in the event validation fails. 
  
**Flags** 
- `--dry-run`: Validate the index and print errors without writing or updating the index on disk. 
- `--full-rescan`: Scans all files regardless of tracked `mtime` or `size`
- `--write-groups`: 

#### new (added from above)
`[--schema <S>] [--title <T>] [--id ...] [--edit]` 

- The primary tool for creating a new note. Can open the defined editor with the `--edit` flag 

#### watch 
`[--debounce-ms 400] [--dry-run] [--full-rescan]`

- Watch bases for changes and incrementally validate + update indexes.
- Debounces rapid events; writes on success (unless `--dry-run`).


### The Syntactic Sugar

#### doctor 
Show resolved config, bases, discovery mode (index vs scan), and quick stats.
- Reports per-type counts when schemas are defined, and unknown-key stats.

#### search 
`[--query <substr>]`
Fuzzy search by ID/title across discovered ADR files.

#### path --from ABC --to XYZ 
`[--max-depth N]` 
- Find a dependency path if any.

### Problematic/Underdeveloped User Story

#### group

```markdown 
#### topics
- List semantic groups derived from frontmatter (`groups` in ADRs).

#### group 
`[--topic "<name>"] [--include-content]` 
- Show ADRs in a group; defaults to full content.
```

These tools are currently not logically implemented and are are holdover from the original `masterplan-validate.js` this library was built out of. The issue is currently we do not have a clear user story around how these groups would be created or defined. 

The purpose of this tool is primarily to give an AI agent a map to understand the code-base. 

One option would be to define `groups` as `validated_frontmatter` with hard-coded logic in the [[ADR-001-cli-rag-toml]];

```toml
# --- `validated_frontmatter` are built tools that will run validation logic beyond simply checking if it exist and has content --- 
# |-----------------|--------------------------------------------------------------------------------|
# |  `depends_on`   | validates the id exists and is valid or throws and error                       |
# |-----------------|--------------------------------------------------------------------------------|
# | `created_date`  | Updates the note with system time when created using the `new` command         |
# |-----------------|--------------------------------------------------------------------------------|
# | `last_modified` | if `watch` is active the note will be updated with a new modified time on edit |
# |-----------------|--------------------------------------------------------------------------------|
# |     `groups`     | Defines subgroups for note types. These can generate a human/AI readable Index |
# |-----------------|--------------------------------------------------------------------------------|
validated_frontmatter = ["depends_on", "created_date", "last_modified", "group"]
```

This would allow the user to dynamically define groups as required. 

The second issue is the split name for `topics` and `groups` is not especially intuitive or ergonomic. Maybe it could be arranged in a single command like so; 

```markdown 
#### groups
`[--list] [--] []`

- `--list` print all semantic groups tracked by `validated_frontmatter` on the current graph.
```

---

#### graph

```markdown
#### graph 
`[--id ABCD] [--depth N] [--include-bidirectional] [--format mermaid|dot|json]`
- Export a dependency graph around an ADR.
```

There are two places the graph tool currently introduces uncertainty. 

Firstly the `--format` command collides with the global `--format plain|json|ndjson` formatting command. This should be changed eg `--graph-output`. This is a relatively minor concern. 

The deeper concern is just "Is this actually useful?" It's pretty commonly agreed that the graphview in discord is more visual fun and games than something truly productive and useful. There is a chance this is just not a feature that's a large value add (even if it's like, neat, and i want to see it.)

The thing that jump out is that it makes seeing "orphan" notes trivial. That said, we can just surface this information in doctor as well. 

An idea that keeps coming to mind is using the local `--include-bidirectional` graph as a navigation system in the TUI/NVIM. 

[Notably, graphiz directly supports ascii output.](https://graphviz.org/docs/outputs/ascii/) An imaginable workflow is we pull a local graph and append the ID's so it's 

1. ADR-001 
2. ADR-002 
   
You are then shown a screen like this where you can with a single key press to fly around notes e.g. 

```ascii
     ┌−−−−−−−−−−−−−−−−−−−−−−−−−−−−−┐
     ╎             adr             ╎
     ╎                             ╎
     ╎ ┌─────────┐     ┌─────────┐ ╎       ┌─────────┐
  ┌─ ╎ │ ADR-001 │ ◀── │ ADR-000 │ ╎ ◀──   │  start  │
  │  ╎ └─────────┘     └─────────┘ ╎       └─────────┘
  │  ╎   │               ▲         ╎         │
  │  ╎   │               │         ╎         │
  │  ╎   │               │         ╎         ▼
  │  ╎   │               │         ╎     ┌−−−−−−−−−−−−−┐
  │  ╎   │               │         ╎     ╎     imp     ╎
  │  ╎   ▼               │         ╎     ╎             ╎
  │  ╎ ┌─────────┐       │         ╎     ╎ ┌─────────┐ ╎
  │  ╎ │ ADR-002 │       │         ╎     ╎ │ IMP-001 │ ╎
  │  ╎ └─────────┘       │         ╎     ╎ └─────────┘ ╎
  │  ╎   │               │         ╎     ╎   │         ╎
  │  ╎   │               │         ╎     ╎   │         ╎
  │  ╎   │               │         ╎     ╎   ▼         ╎
  │  ╎   │               │         ╎     ╎ ┌─────────┐ ╎
  │  ╎   │               │         ╎     ╎ │ IMP-002 │ ╎
  │  ╎   │               │         ╎     ╎ └─────────┘ ╎
  │  ╎   │               │         ╎     ╎   │         ╎
  │  ╎   │               │         ╎     ╎   │         ╎
  │  ╎   │               │         ╎     ╎   ▼         ╎
  │  ╎   │             ┌─────────┐ ╎     ╎ ┌─────────┐ ╎
  │  ╎   └───────────▶ │ ADR-003 │ ╎ ◀── ╎ │ IMP-003 │ ╎
  │  ╎                 └─────────┘ ╎     ╎ └─────────┘ ╎
  │  ╎                             ╎     ╎   │         ╎
  │  └−−−−−−−−−−−−−−−−−−−−−−−−−−−−−┘     ╎   │         ╎
  │                      │               ╎   │         ╎
  │                      │               ╎   │         ╎
  │                      │               ╎   ▼         ╎
  │                      │               ╎ ┌─────────┐ ╎
  └──────────────────────┼─────────────▶ ╎ │ IMP-004 │ ╎
                         │               ╎ └─────────┘ ╎
                         │               ╎             ╎
                         │               └−−−−−−−−−−−−−┘
                         │                   │
                         │                   │
                         │                   ▼
                         │                 ┌─────────┐
                         └─────────────▶   │   end   │
                                           └─────────┘
```

It is to be completely honest still not entirely clear what the relative value here actually is. Worth thinking about further. We need the ability to filter this down. so 

- Only [[wikistyle links]]
- Only hard `superscedes` or `depends`. In order to not write custom logic it could be any frontmatter that matches a note title or ID based on the validation type defined in the schema. 

I think this is "in-theory" useful but it needs to be a very fast and intuitive UI to beat just using telescope or directly opening the notes in obsidian. 

---

```markdown 
#### get 
`--id ADR-021 [--include-dependents]` 
- Traverse a note (and it's dependencies) and print the contents. This tool is primarily designed for an AI consumer and is the primary 'RAG' in CLI-RAG. 

#### cluster 
`[--id ABCD] [--depth N] [--include-bidirectional]` 
```

Currently the user story here is a bit unclear in the prior version of the tool 

## LLM Sourced Feature Ideas/Changes 

- Deep Refactor Support (very useful for agents) 
	- `rename --id ADR-012 --new-id ADR-012R` updates filename and all `depends_on` references in repo (front matter and wiki-links in bodies).
	- `supersede --old ADR-008 --new ADR-031` writes both sides of the link (`supersedes`/`superseded_by`) 

- **Improved Output and Formatting**:
    - **Why?** Plain output is functional but can be noisy (e.g., long paths in `doctor`). JSON is great for scripting, but humans need summaries.
    - **How**: For plain mode, use tables (e.g., via `comfy_table` crate: add to Cargo.toml). In `doctor`, output like:
       
        ```
        +---------+-------+
        | Base    | Mode  |
        +---------+-------+
        | docs/   | index |
        +---------+-------+
        ```
        
	- Colorize errors/warnings in `validate` (use `anstream` for cross-platform coloring—already in your deps).

- **Autocompletion and ID Suggestions**:
    
    - **Why?** Commands like `get --id ADR-022` are error-prone if IDs are mistyped. Your `completions` command is a start—enhance with dynamic suggestions.
    - **How**: In `get`/`cluster`/`path`, if `--id` is invalid, fuzzy-search IDs (like your `search`) and suggest: "Did you mean ADR-022? (y/n)". Use `clap` 's built-in validation for runtime checks.
- **Config Ergonomics and Defaults**:
    
    - **Why?** The `.adr-rag.toml` is comprehensive but overwhelming (e.g., long schema comments). `init --print-template` is good—make it interactive.
    - **How**: In `init`, prompt for key values (e.g., "Enter bases (comma-separated):") using `dialoguer` crate (add to Cargo.toml: `dialoguer = "0.11"`). Add a `config edit` command that opens the file (like your `try_open_editor`).
- **Error Handling and UX**:
    
    - **Why?** Rust's `anyhow` is used well, but user-facing errors could be friendlier (e.g., "Config not found—run init?").
    - **How**: In `load_config`, if no config, prompt to run `init`. Add verbose logging with `--debug` (use `env_logger` crate). For `validate`, group errors by file/schema for easier debugging.
    - 
- **Advanced Validation Rules**:
    
    - **Why?** Your schemas are already powerful (e.g., required fields, allowed statuses, regex for `depends_on`). Extend to cyclic dependency detection or status consistency (e.g., warn if a "draft" ADR depends on a "superseded" one).
    - **How**: In `validate_docs`, after building `id_to_docs` and `id_set`, add a graph traversal to detect cycles (using your existing `compute_cluster`). For status checks, walk dependencies and compare against `allowed_statuses` or schema rules.
    - **Ergonomic Twist**: Add a `--fix` flag to `validate` that auto-updates statuses (e.g., propagate "superseded") or suggests resolutions in JSON output.

## Consequences
<!-- What becomes easier or more difficult to do because of this change? -->

A more ergonomic and intuitive developer experience.
</file>

<file path="docs/ADR/ADR-004-toml-config-versioning.md">
---
id: ADR-
tags:
  - NeoVIM
status: draft
depends_on: []
created_date: 2025-08-25
last_modified: 2025-08-25
related_files: []
---

## Objective
<!-- A concise statement explaining the goal of this decision. -->

As we change schema processing we need a way to support and deprecate older config versions. 

## Context
<!-- What is the issue that we're seeing that is motivating this decision or change? -->

we need:
  - Version field in the TOML (config_version = 1)
  - Migration commands or at least clear migration guides

## Decision
<!-- What is the change that we're proposing and/or doing? -->

## Consequences
<!-- What becomes easier or more difficult to do because of this change? -->

## Updates
<!-- Changes that happened when the rubber met the road -->
</file>

<file path="docs/ADR/ADR-DRAFT-NeoRAG-planning.md">
---
id: ADR-
tags:
  - NeoVIM
status: draft
depends_on: []
created_date: 2025-08-25
last_modified: 2025-08-25
related_files: []
---

# NeoRAG-planning

## Objective
<!-- A concise statement explaining the goal of this decision. -->

Until this point CLI-RAG has grown adhoc out of a a dog-fooded .js script. As the workflow is becoming more clear the existing commands should be reviewed and adjusted to ensure everything is coherent. 

## Context
<!-- What is the issue that we're seeing that is motivating this decision or change? -->

## Decision
<!-- What is the change that we're proposing and/or doing? -->

## Consequences
<!-- What becomes easier or more difficult to do because of this change? -->
</file>

<file path="docs/ADR/ADR-DRAFT-org-agenda.md">
---
id: ADR-
staus: draft
depends: ADR-002
---

# Org Agenda 

## Objective
<!-- A concise statement explaining the goal of this decision. -->

Goal: The main TUI screen shows a agenda somehow created across notes in front matter maybe even a simple "due now" or a data entry flag 

This could be structured as a `validated_frontmatter` with a check against a due date field. It's logical but I don't honestly work that way. I more would want to be able to flag things in notes and somehow have that create a todolist with priority buckets. 

## Context
<!-- What is the issue that we're seeing that is motivating this decision or change? -->

## Decision
<!-- What is the change that we're proposing and/or doing? -->

## Consequences
<!-- What becomes easier or more difficult to do because of this change? -->

## Updates
<!-- Changes that happened when the rubber met the road -->
</file>

<file path="docs/ADR/ADR-DRAFT-simple-query-dsl.md">
---
id: ADR-
status: draft
depends: ADR-001
---

# advanced-query-dsl
  
Query Language

  Simple predicate-based queries would be powerful:
  Commands::Query {
      expr: String  // "status:accepted AND tags:security"
  }

I see this more likely as an MCP tool where an AI can just handle the complexity.
</file>

<file path="docs/ADR/Opus Advice --Distilled Down.md">
```
  What's Missing

  Given "teams who think in git":
  - Merge conflict resolution for IDs (when two
  branches create same ID)
  - Pre-commit hooks (validate before commit)
  - Diff-friendly output (sorted JSON, stable ordering)
  - Team migration tools (import from
  Confluence/Notion/Google Docs?)
 ```

```
  The system is positioning itself as a
  "programmer-friendly Obsidian that lives in a repo" -
   this is actually a strong identity! Lean into it:
   
  - Git-native knowledge management
  - CI/CD friendly validation
  - Team-shared architectural decisions
  - No proprietary formats or lock-in
```

```
  This Tera-based approach could expand to:
  - Template inheritance (extends = "base-adr")
  - Conditional sections based on frontmatter
  - Auto-generated sections from git history or code
  analysis
```

```
  Complexity Creep: The config is getting
  sophisticated. Consider:
  - Ship with 2-3 battle-tested preset configs users
  can extend
  - cli-rag init --preset=adr-simple vs
  --preset=full-featured
```

This is just a good idea yeah. create a preset folder of example use cases.
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        rust: [stable]
        include:
          - os: ubuntu-latest
            rust: beta
          - os: ubuntu-latest
            rust: nightly
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ matrix.rust }}
          components: rustfmt, clippy
      
      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: ~/.cargo/registry
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Cache cargo index
        uses: actions/cache@v4
        with:
          path: ~/.cargo/git
          key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Cache cargo build
        uses: actions/cache@v4
        with:
          path: target
          key: ${{ runner.os }}-cargo-build-target-${{ matrix.rust }}-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Check formatting
        run: cargo fmt --all -- --check
        if: matrix.rust == 'stable' && matrix.os == 'ubuntu-latest'
      
      - name: Run clippy
        run: cargo clippy --all-targets --all-features -- -D warnings
        if: matrix.rust == 'stable'
      
      - name: Build
        run: cargo build --verbose
      
      - name: Run tests
        run: cargo test --verbose
      
      - name: Build release
        run: cargo build --release --verbose
        if: matrix.rust == 'stable'

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Run integration tests
        run: |
          cargo test --test integration_* --verbose
      
      - name: Test CLI commands
        run: |
          cargo build --release
          ./target/release/adr-rag --version
          ./target/release/adr-rag init --print-template > /tmp/test-template.toml
          ./target/release/adr-rag completions bash > /tmp/test-completions.bash
          ./target/release/adr-rag completions zsh > /tmp/test-completions.zsh

  msrv:
    name: Minimum Supported Rust Version
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Extract MSRV
        id: msrv
        run: |
          msrv=$(grep -E "^rust-version" Cargo.toml | grep -oE "[0-9]+\.[0-9]+\.[0-9]+")
          if [ -z "$msrv" ]; then
            echo "msrv=1.70.0" >> $GITHUB_OUTPUT
          else
            echo "msrv=$msrv" >> $GITHUB_OUTPUT
          fi
      
      - name: Install MSRV toolchain
        uses: dtolnay/rust-toolchain@master
        with:
          toolchain: ${{ steps.msrv.outputs.msrv }}
      
      - name: Check MSRV
        run: cargo check --all-features
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      tag:
        description: 'Release tag (e.g., v0.1.0)'
        required: true

permissions:
  contents: write

jobs:
  create-release:
    name: Create Release
    runs-on: ubuntu-latest
    outputs:
      upload_url: ${{ steps.create_release.outputs.upload_url }}
      version: ${{ steps.get_version.outputs.version }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Get version
        id: get_version
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            VERSION="${{ github.event.inputs.tag }}"
          else
            VERSION="${GITHUB_REF#refs/tags/}"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
      
      - name: Create Release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ steps.get_version.outputs.version }}
          release_name: Release ${{ steps.get_version.outputs.version }}
          draft: true
          prerelease: false

  build-release:
    name: Build Release
    needs: create-release
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu
            archive: tar.gz
          - os: ubuntu-latest
            target: x86_64-unknown-linux-musl
            archive: tar.gz
          - os: ubuntu-latest
            target: aarch64-unknown-linux-gnu
            archive: tar.gz
          - os: macos-latest
            target: x86_64-apple-darwin
            archive: tar.gz
          - os: macos-latest
            target: aarch64-apple-darwin
            archive: tar.gz
          - os: windows-latest
            target: x86_64-pc-windows-msvc
            archive: zip
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: ${{ matrix.target }}
      
      - name: Install cross-compilation tools
        if: matrix.target == 'aarch64-unknown-linux-gnu'
        run: |
          sudo apt-get update
          sudo apt-get install -y gcc-aarch64-linux-gnu
      
      - name: Install musl tools
        if: matrix.target == 'x86_64-unknown-linux-musl'
        run: |
          sudo apt-get update
          sudo apt-get install -y musl-tools
      
      - name: Build
        run: |
          if [[ "${{ matrix.target }}" == "aarch64-unknown-linux-gnu" ]]; then
            export CARGO_TARGET_AARCH64_UNKNOWN_LINUX_GNU_LINKER=aarch64-linux-gnu-gcc
            export CC_aarch64_unknown_linux_gnu=aarch64-linux-gnu-gcc
            export CXX_aarch64_unknown_linux_gnu=aarch64-linux-gnu-g++
          fi
          cargo build --release --target ${{ matrix.target }}
      
      - name: Package (Unix)
        if: matrix.os != 'windows-latest'
        run: |
          cd target/${{ matrix.target }}/release
          tar czf ../../../adr-rag-${{ needs.create-release.outputs.version }}-${{ matrix.target }}.tar.gz adr-rag
          cd ../../../
      
      - name: Package (Windows)
        if: matrix.os == 'windows-latest'
        run: |
          cd target/${{ matrix.target }}/release
          7z a ../../../adr-rag-${{ needs.create-release.outputs.version }}-${{ matrix.target }}.zip adr-rag.exe
          cd ../../../
      
      - name: Upload Release Asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ needs.create-release.outputs.upload_url }}
          asset_path: ./adr-rag-${{ needs.create-release.outputs.version }}-${{ matrix.target }}.${{ matrix.archive }}
          asset_name: adr-rag-${{ needs.create-release.outputs.version }}-${{ matrix.target }}.${{ matrix.archive }}
          asset_content_type: application/octet-stream

  publish-crate:
    name: Publish to crates.io
    needs: build-release
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
      
      - name: Publish to crates.io
        run: |
          cargo publish --token ${{ secrets.CARGO_REGISTRY_TOKEN }} --allow-dirty
        continue-on-error: true  # Don't fail the whole release if crates.io publish fails
</file>

<file path="src/protocol.rs">
use serde::{Deserialize, Serialize};

// Stable protocol types for JSON/NDJSON outputs. Keep additions backward compatible.

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    pub id: String,
    pub title: String,
    pub file: std::path::PathBuf,
    pub tags: Vec<String>,
    pub status: Option<String>,
    pub groups: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TopicCount {
    pub topic: String,
    pub count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GroupMember {
    pub id: String,
    pub title: String,
    pub status: Option<String>,
    pub groups: Vec<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub file: Option<std::path::PathBuf>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidateHeader {
    pub ok: bool,
    pub doc_count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidateIssue {
    #[serde(rename = "type")]
    pub kind: String, // "error" | "warning"
    #[serde(skip_serializing_if = "Option::is_none")]
    pub file: Option<String>,
    pub message: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClusterMember {
    pub id: String,
    pub title: String,
    pub status: Option<String>,
    pub groups: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Edge {
    pub from: String,
    pub to: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphMinimal {
    pub root: String,
    pub nodes: Vec<ClusterMember>,
    pub edges: Vec<Edge>,
}
</file>

<file path="tests/integration_cluster.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

fn write_adr(file: &assert_fs::fixture::ChildPath, id: &str, title: &str, depends: &[&str]) {
    let deps = if depends.is_empty() {
        String::from("[]")
    } else {
        format!(
            "[{}]",
            depends
                .iter()
                .map(|d| format!("\"{}\"", d))
                .collect::<Vec<_>>()
                .join(", ")
        )
    };
    let content = format!(
        "---\nid: {id}\ntags: [x]\nstatus: draft\ngroups: [\"G\"]\ndepends_on: {deps}\n---\n\n# {id}: {title}\n\nBody\n"
    );
    file.write_str(&content).unwrap();
}

#[test]
fn cluster_json_includes_members() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();
    write_adr(&base.child("ADR-100.md"), "ADR-100", "Root", &[]);
    write_adr(&base.child("ADR-101.md"), "ADR-101", "Child", &["ADR-100"]);

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let out = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("cluster")
        .arg("--id")
        .arg("ADR-101")
        .arg("--format")
        .arg("json")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();

    let v: serde_json::Value = serde_json::from_slice(&out).unwrap();
    assert_eq!(v["root"], "ADR-101");
    assert!(v["members"]
        .as_array()
        .unwrap()
        .iter()
        .any(|m| m["id"] == "ADR-100"));

    temp.close().unwrap();
}
</file>

<file path="tests/integration_config_precedence.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

fn write_adr(file: &assert_fs::fixture::ChildPath, id: &str) {
    let content = format!(
        "---\nid: {id}\ntags: [x]\nstatus: draft\ngroups: [\"G\"]\ndepends_on: []\n---\n\n# {id}: Title\n\nBody\n"
    );
    file.write_str(&content).unwrap();
}

#[test]
fn env_overrides_config_and_cli_overrides_env() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base1 = temp.child("notes1");
    let base2 = temp.child("notes2");
    base1.create_dir_all().unwrap();
    base2.create_dir_all().unwrap();
    write_adr(&base1.child("ADR-001.md"), "ADR-001");
    write_adr(&base2.child("ADR-002.md"), "ADR-002");

    // Config points to base1
    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base1.path().display()))
        .unwrap();

    // Case A: env overrides config to base2
    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let out = cmd
        .env("ADR_RAG_BASES", base2.path())
        .arg("--config")
        .arg(cfg.path())
        .arg("search")
        .arg("-q")
        .arg("ADR-002")
        .arg("--format")
        .arg("json")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();
    let v: serde_json::Value = serde_json::from_slice(&out).unwrap();
    assert_eq!(v.as_array().unwrap().len(), 1);
    assert_eq!(v.as_array().unwrap()[0]["id"], "ADR-002");

    // Case B: CLI --base overrides env back to base1
    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let out = cmd
        .env("ADR_RAG_BASES", base2.path())
        .arg("--base")
        .arg(base1.path())
        .arg("--config")
        .arg(cfg.path())
        .arg("search")
        .arg("-q")
        .arg("ADR-001")
        .arg("--format")
        .arg("json")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();
    let v: serde_json::Value = serde_json::from_slice(&out).unwrap();
    assert_eq!(v.as_array().unwrap().len(), 1);
    assert_eq!(v.as_array().unwrap()[0]["id"], "ADR-001");

    temp.close().unwrap();
}
</file>

<file path="tests/integration_doctor.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

#[test]
fn doctor_json_on_empty_base_reports_structure() {
    // Setup isolated temp repo with a minimal config and empty base
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    // Run `adr-rag doctor --format json --config <cfg>`
    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let assert = cmd
        .arg("doctor")
        .arg("--format")
        .arg("json")
        .arg("--config")
        .arg(cfg.path())
        .assert()
        .success();

    // Parse JSON and assert a few stable fields
    let output = String::from_utf8(assert.get_output().stdout.clone()).unwrap();
    let v: serde_json::Value = serde_json::from_str(&output).unwrap();
    assert!(v
        .get("config")
        .and_then(|x| x.as_str())
        .unwrap()
        .ends_with(".adr-rag.toml"));
    assert_eq!(v["counts"]["docs"].as_u64().unwrap_or(999), 0);
    let per_base = v["per_base"].as_array().unwrap();
    assert_eq!(per_base.len(), 1);
    assert_eq!(per_base[0]["mode"].as_str().unwrap(), "scan");

    temp.close().unwrap();
}
</file>

<file path="tests/integration_group_validate.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

fn write_simple_adr(file: &assert_fs::fixture::ChildPath, id: &str, group: &str) {
    let content = format!(
        "---\nid: {id}\ntags: [x]\nstatus: draft\ngroups: [\"{group}\"]\ndepends_on: []\n---\n\n# {id}: Title\n\nBody\n"
    );
    file.write_str(&content).unwrap();
}

#[test]
fn group_json_wraps_members() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();
    write_simple_adr(&base.child("ADR-010.md"), "ADR-010", "Tools");

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let out = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("group")
        .arg("--topic")
        .arg("Tools")
        .arg("--format")
        .arg("json")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();

    let v: serde_json::Value = serde_json::from_slice(&out).unwrap();
    assert_eq!(v["topic"], "Tools");
    assert_eq!(v["count"], 1);
    assert_eq!(v["adrs"].as_array().unwrap()[0]["id"], "ADR-010");

    temp.close().unwrap();
}

#[test]
fn validate_ndjson_header_only_on_empty() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let output = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("validate")
        .arg("--format")
        .arg("ndjson")
        .arg("--dry-run")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();

    let s = String::from_utf8(output).unwrap();
    // Should contain a single header line with ok true and doc_count 0
    let mut lines = s.lines();
    let header = lines.next().unwrap();
    assert!(header.contains("\"ok\":true"));
    assert!(header.contains("\"doc_count\":0"));
    assert!(lines.next().is_none());

    temp.close().unwrap();
}

#[test]
fn group_ndjson_emits_header_then_members() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();
    write_simple_adr(&base.child("ADR-020.md"), "ADR-020", "Tools");

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let out = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("group")
        .arg("--topic")
        .arg("Tools")
        .arg("--format")
        .arg("ndjson")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();
    let s = String::from_utf8(out).unwrap();
    let mut lines = s.lines();
    let header = lines.next().unwrap();
    assert!(header.contains("\"topic\":\"Tools\""));
    assert!(header.contains("\"count\":1"));
    let first = lines.next().unwrap();
    assert!(first.contains("\"id\":\"ADR-020\""));
    assert!(lines.next().is_none());

    temp.close().unwrap();
}
</file>

<file path="tests/integration_help.rs">
use assert_cmd::prelude::*;
use std::process::Command;

#[test]
fn cli_help_prints_usage() {
    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    cmd.arg("--help")
        .assert()
        .success()
        .stdout(predicates::str::contains("Usage"))
        .stdout(predicates::str::contains("validate"))
        .stdout(predicates::str::contains("doctor"));
}
</file>

<file path="tests/integration_init.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

#[test]
fn init_writes_config_silently() {
    let temp = assert_fs::TempDir::new().unwrap();
    // Run `adr-rag init --silent --force` in a temp dir
    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    cmd.current_dir(temp.path())
        .arg("init")
        .arg("--silent")
        .arg("--force")
        .assert()
        .success();

    // Verify file exists and contains recognizable header
    let cfg = temp.child(".adr-rag.toml");
    cfg.assert(predicates::path::exists());
    cfg.assert(predicates::str::contains("Repo-local ADR CLI config"));

    temp.close().unwrap();
}
</file>

<file path="tests/integration_search_topics.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

fn write_simple_adr(file: &assert_fs::fixture::ChildPath, id: &str, group: &str) {
    let content = format!(
        "---\nid: {id}\ntags: [x]\nstatus: draft\ngroups: [\"{group}\"]\ndepends_on: []\n---\n\n# {id}: Title\n\nBody\n"
    );
    file.write_str(&content).unwrap();
}

#[test]
fn search_returns_protocol_with_groups() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();
    let adr = base.child("ADR-001.md");
    write_simple_adr(&adr, "ADR-001", "G1");

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let output = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("search")
        .arg("-q")
        .arg("ADR-001")
        .arg("--format")
        .arg("json")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();

    let v: serde_json::Value = serde_json::from_slice(&output).unwrap();
    let arr = v.as_array().unwrap();
    assert_eq!(arr.len(), 1);
    assert_eq!(arr[0]["id"].as_str().unwrap(), "ADR-001");
    assert_eq!(
        arr[0]["groups"].as_array().unwrap()[0].as_str().unwrap(),
        "G1"
    );

    temp.close().unwrap();
}

#[test]
fn topics_counts_groups_from_docs() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();
    write_simple_adr(&base.child("ADR-001.md"), "ADR-001", "Tools");

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let output = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("topics")
        .arg("--format")
        .arg("json")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();

    let v: serde_json::Value = serde_json::from_slice(&output).unwrap();
    let arr = v.as_array().unwrap();
    assert!(arr.iter().any(|e| e["topic"] == "Tools" && e["count"] == 1));

    temp.close().unwrap();
}
</file>

<file path="tests/integration_validate_json.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

#[test]
fn validate_json_shape_and_writes_groups() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();

    // Minimal config pointing to empty base
    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    // Run validate in JSON mode and request writing groups
    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    let out = cmd
        .arg("--config")
        .arg(cfg.path())
        .arg("validate")
        .arg("--format")
        .arg("json")
        .arg("--write-groups")
        .assert()
        .success()
        .get_output()
        .stdout
        .clone();

    let v: serde_json::Value = serde_json::from_slice(&out).unwrap();
    assert!(v["ok"].as_bool().unwrap());
    assert_eq!(v["doc_count"].as_u64().unwrap(), 0);
    assert!(v["errors"].as_array().unwrap().is_empty());
    assert!(v["warnings"].as_array().unwrap().is_empty());

    // Groups file should exist (empty sections array)
    let groups_path = base.path().join("index/semantic-groups.json");
    assert!(groups_path.exists());
    let groups: serde_json::Value =
        serde_json::from_str(&std::fs::read_to_string(groups_path).unwrap()).unwrap();
    assert!(groups["sections"].is_array());

    temp.close().unwrap();
}
</file>

<file path="tests/integration_validate_write_groups.rs">
use assert_cmd::prelude::*;
use assert_fs::prelude::*;
use std::process::Command;

fn write_adr(file: &assert_fs::fixture::ChildPath, id: &str, group: &str) {
    let content = format!(
        "---\nid: {id}\ntags: [x]\nstatus: draft\ngroups: [\"{group}\"]\ndepends_on: []\n---\n\n# {id}: Title\n\nBody\n"
    );
    file.write_str(&content).unwrap();
}

#[test]
fn validate_writes_groups_with_ids() {
    let temp = assert_fs::TempDir::new().unwrap();
    let base = temp.child("notes");
    base.create_dir_all().unwrap();
    write_adr(&base.child("ADR-030.md"), "ADR-030", "A");
    write_adr(&base.child("ADR-031.md"), "ADR-031", "B");

    let cfg = temp.child(".adr-rag.toml");
    cfg.write_str(&format!("bases = [\n  \"{}\"\n]\n", base.path().display()))
        .unwrap();

    let mut cmd = Command::cargo_bin("adr-rag").unwrap();
    cmd.arg("--config")
        .arg(cfg.path())
        .arg("validate")
        .arg("--format")
        .arg("json")
        .arg("--write-groups")
        .assert()
        .success();

    let groups_path = base.path().join("index/semantic-groups.json");
    assert!(groups_path.exists());
    let groups: serde_json::Value =
        serde_json::from_str(&std::fs::read_to_string(groups_path).unwrap()).unwrap();
    let sections = groups["sections"].as_array().unwrap();
    // Titles include A and B, and ids include ADR-030 and ADR-031
    assert!(sections.iter().any(|s| s["title"] == "A"
        && s["selectors"][0]["anyIds"]
            .as_array()
            .unwrap()
            .iter()
            .any(|id| id == "ADR-030")));
    assert!(sections.iter().any(|s| s["title"] == "B"
        && s["selectors"][0]["anyIds"]
            .as_array()
            .unwrap()
            .iter()
            .any(|id| id == "ADR-031")));

    temp.close().unwrap();
}
</file>

<file path=".adr-rag.toml">
# Repo-local ADR CLI config (adr-rag)

# One or more directories to scan or read an index from.
bases = [
  "docs/masterplan",
  # "docs/notes",
]

# Where to read/write the index and semantic groups (paths are relative to each base).
index_relative = "index/adr-index.json"
groups_relative = "index/semantic-groups.json"

# Discovery and semantics
file_patterns = ["ADR-*.md", "ADR-DB-*.md", "IMP-*.md"]
ignore_globs  = ["**/node_modules/**", "**/.obsidian/**"]
allowed_statuses = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]

[defaults]
depth = 2
include_bidirectional = true
include_content = true

[output]
format = "plain" # plain | json

# Note Types (Schema) — Optional, per-type rules and validation
#
# Define one or more [[schema]] blocks to validate different note types
# (e.g., ADR vs IMP). Matching is by file_patterns; first match wins.
# Unknown keys policy lets you treat unexpected front-matter as ignore|warn|error.
#
# [[schema]]
# name = "ADR"
# file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
# required = ["id", "tags", "status", "depends_on"]
# unknown_policy = "ignore"   # ignore | warn | error (default: ignore)
# allowed_keys = ["produces", "files_touched"]  # optional pass-through keys
#
# [schema.rules.status]
# allowed = [
#   "draft", "incomplete", "proposed", "accepted",
#   "complete", "design", "legacy-reference", "superseded"
# ]
# severity = "error"          # error | warn
#
# [schema.rules.depends_on]
# type = "array"
# items = { type = "string", regex = "^(ADR|IMP)-\\d+" }
# refers_to_types = ["ADR", "IMP"]
# severity = "error"
#
# [[schema]]
# name = "IMP"
# file_patterns = ["IMP-*.md"]
# required = ["id","tags","depends_on","status","completion_date"]
# unknown_policy = "warn"
#
# [schema.rules.status]
# allowed = ["in-progress","blocked","on-hold","cancelled","done"]
# severity = "error"
#
# [schema.rules.completion_date]
# type = "date"
# format = "%Y-%m-%d"
# severity = "warn"
</file>

<file path=".gitignore">
# Rust build artifacts
target/
debug/
**/*.rlib
**/*.rmeta

# Coverage and profiling
coverage/
*.profraw
*.profdata
tarpaulin-report.*

# Logs and temporary files
*.log
logs/
tmp/
.tmp/
*.out
*.err

# OS / Editor noise
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp
*.swo
.history/
.direnv/

# Node leftovers (project started as npm script)
node_modules/
.pnpm-store/
dist/

# Local env/config
.env
.env.*

# Generated examples (keep real sources tracked)
index/*.json
</file>

<file path="AGENTS.md">
# Repository Guidelines

## Project Structure & Module Organization
- `src/bin/adr-rag.rs`: Thin binary wiring CLI to library.
- `src/cli.rs` and `src/commands/*`: Clap definitions and subcommand handlers.
- `src/{config,model,discovery,index,validate,graph,watch,util}.rs`: Core modules (config I/O, front matter parsing, scanning/indexing, validation, graph ops, watcher, helpers).
- Config file: `.adr-rag.toml` at repo root (or discovered upward).
- Build artifacts in `target/`; temporary notes in `tmp/`.

## Build, Test, and Development Commands
- With Cargo:
  - `cargo build` | `cargo build --release`
  - `cargo run --bin adr-rag -- <subcommand>`
  - `cargo test`
- Shell completions: `adr-rag completions bash|zsh|fish` (e.g., `adr-rag completions zsh > ~/.zsh/completions/_adr-rag`).

## Coding Style & Naming Conventions
- Rust edition 2021; 4‑space indentation; wrap at ~100 cols.
- Names: modules/functions `snake_case`, types/traits `CamelCase`, constants `SCREAMING_SNAKE_CASE`.
- Prefer small, focused modules under `src/commands/` for each subcommand.
- Always run `cargo fmt` and `cargo clippy` before pushing.

## Testing Guidelines
- Unit tests live inline using `mod tests { ... }` within modules (see `model.rs`, `validate.rs`, `commands/graph.rs`).
- Name tests descriptively; assert on exact messages where practical.
- Add tests next to changed code and cover common error paths.
- Run the full suite: `cargo test`.

## Commit & Pull Request Guidelines
- Commits: imperative mood, concise subject; include a focused body when changing behavior (why + brief what).
- Reference issues with `Fixes #123` when applicable.
- PRs must include: summary, rationale, notable tradeoffs, and a short demo (e.g., command and output snippet) for CLI changes.
- CI hygiene: ensure `cargo fmt`, `cargo clippy`, and `cargo test` pass locally.

## Security & Configuration Tips
- Do not commit machine‑specific paths in `.adr-rag.toml`; prefer relative `bases`, `index_relative`, and `groups_relative`.
- Validate before relying on indexes: `adr-rag validate --format json`.
- Use `--config` or `ADR_RAG_CONFIG` to test alternate configs without editing the default.
</file>

<file path="CLAUDE.md">
# Repository Guidelines

## Project Structure & Module Organization
- `src/bin/adr-rag.rs`: Thin binary wiring CLI to library.
- `src/cli.rs` and `src/commands/*`: Clap definitions and subcommand handlers.
- `src/{config,model,discovery,index,validate,graph,watch,util}.rs`: Core modules (config I/O, front matter parsing, scanning/indexing, validation, graph ops, watcher, helpers).
- Config file: `.adr-rag.toml` at repo root (or discovered upward).
- Build artifacts in `target/`; temporary notes in `tmp/`.

## Build, Test, and Development Commands
- With Cargo:
  - `cargo build` | `cargo build --release`
  - `cargo run --bin adr-rag -- <subcommand>`
  - `cargo test`
- Shell completions: `adr-rag completions bash|zsh|fish` (e.g., `adr-rag completions zsh > ~/.zsh/completions/_adr-rag`).

## Coding Style & Naming Conventions
- Rust edition 2021; 4‑space indentation; wrap at ~100 cols.
- Names: modules/functions `snake_case`, types/traits `CamelCase`, constants `SCREAMING_SNAKE_CASE`.
- Prefer small, focused modules under `src/commands/` for each subcommand.
- Always run `cargo fmt` and `cargo clippy` before pushing.

## Testing Guidelines
- Unit tests live inline using `mod tests { ... }` within modules (see `model.rs`, `validate.rs`, `commands/graph.rs`).
- Name tests descriptively; assert on exact messages where practical.
- Add tests next to changed code and cover common error paths.
- Run the full suite: `cargo test`.

## Commit & Pull Request Guidelines
- Commits: imperative mood, concise subject; include a focused body when changing behavior (why + brief what).
- Reference issues with `Fixes #123` when applicable.
- PRs must include: summary, rationale, notable tradeoffs, and a short demo (e.g., command and output snippet) for CLI changes.
- CI hygiene: ensure `cargo fmt`, `cargo clippy`, and `cargo test` pass locally.

## Security & Configuration Tips
- Do not commit machine‑specific paths in `.adr-rag.toml`; prefer relative `bases`, `index_relative`, and `groups_relative`.
- Validate before relying on indexes: `adr-rag validate --format json`.
- Use `--config` or `ADR_RAG_CONFIG` to test alternate configs without editing the default.
</file>

<file path="Justfile">
# Use bash for recipes
set shell := ["bash", "-cu"]

default: help

help:
    @just --list

# Build
dev-build:
    cargo build

build:
    cargo build --release

# Linting / formatting
fmt:
    cargo fmt --all

fmt-check:
    cargo fmt --all -- --check

lint:
    cargo clippy -- -D warnings

# Tests
test:
    cargo test

# Run binary with arbitrary args after `--`
run *args:
    cargo run --bin adr-rag -- {{args}}

# Handy shortcuts
doctor-json:
    just run doctor --format json

init-dry:
    just run init --silent --force
</file>

<file path="src/bin/adr-rag.rs">
use anyhow::Result;
use clap::{CommandFactory, Parser};

use adr_rag::cli::{Cli, Commands};
use adr_rag::config::load_config;

fn main() -> Result<()> {
    let cli = Cli::parse();
    match cli.command {
        Commands::Init {
            path,
            force,
            print_template,
            silent,
        } => {
            adr_rag::commands::init::run(path, force, print_template, silent)?;
        }
        Commands::Doctor {} => {
            let (cfg, cfg_path) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::doctor::run(&cfg, &cfg_path, &cli.format)?;
        }
        Commands::Completions { shell } => {
            let cmd = Cli::command();
            adr_rag::commands::completions::run_completions(cmd, shell);
        }
        Commands::Search { query } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::search::run(&cfg, &cli.format, query)?;
        }
        Commands::Topics {} => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::topics::run(&cfg, &cli.format)?;
        }
        Commands::Group {
            topic,
            include_content,
        } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::group::run(&cfg, &cli.format, topic, include_content)?;
        }
        Commands::Get {
            id,
            include_dependents,
        } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::get::run(&cfg, &cli.format, id, include_dependents)?;
        }
        Commands::Cluster {
            id,
            depth,
            include_bidirectional,
        } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::cluster::run(&cfg, &cli.format, id, depth, include_bidirectional)?;
        }
        Commands::Path {
            from,
            to,
            max_depth,
        } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::path::run(&cfg, &cli.format, from, to, max_depth)?;
        }
        Commands::Graph {
            id,
            depth,
            include_bidirectional,
            format,
        } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::graph::run(&cfg, &format, id, depth, include_bidirectional)?;
        }
        Commands::Watch {
            full_rescan,
            debounce_ms,
            dry_run,
        } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::watch_cmd::run(&cfg, full_rescan, debounce_ms, dry_run)?;
        }
        Commands::Validate(args) => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::validate_cmd::run(
                &cfg,
                &args.format,
                args.write_groups,
                args.dry_run,
                args.full_rescan,
            )?;
        }
    }
    Ok(())
}
</file>

<file path="src/commands/completions.rs">
use clap::Command;
use clap_complete::{
    generate,
    shells::{Bash, Fish, Zsh},
};

pub fn run_completions<S: AsRef<str>>(mut cmd: Command, shell: S) {
    let shell = shell.as_ref();
    match shell {
        "bash" => generate(Bash, &mut cmd, "adr-rag", &mut std::io::stdout()),
        "zsh" => generate(Zsh, &mut cmd, "adr-rag", &mut std::io::stdout()),
        "fish" => generate(Fish, &mut cmd, "adr-rag", &mut std::io::stdout()),
        _ => {
            eprintln!("Unsupported shell: {} (supported: bash|zsh|fish)", shell);
            std::process::exit(2);
        }
    }
}
</file>

<file path="src/commands/doctor.rs">
use anyhow::Result;
use std::collections::{BTreeMap, BTreeSet, HashMap};
use std::path::PathBuf;

use crate::cli::OutputFormat;
use crate::commands::output::print_json;
use crate::config::{Config, SchemaCfg};
use crate::discovery::load_docs;

pub(crate) fn build_report(
    cfg: &Config,
    cfg_path: &Option<PathBuf>,
    docs: &Vec<crate::model::AdrDoc>,
) -> serde_json::Value {
    let mut id_to_docs: HashMap<String, Vec<&crate::model::AdrDoc>> = HashMap::new();
    for d in docs {
        if let Some(ref id) = d.id {
            id_to_docs.entry(id.clone()).or_default().push(d);
        }
    }
    let mut conflicts = Vec::new();
    for (id, lst) in &id_to_docs {
        if lst.len() > 1 {
            let mut titles: BTreeSet<String> = BTreeSet::new();
            let mut statuses: BTreeSet<String> = BTreeSet::new();
            for d in lst.iter() {
                let doc = *d;
                titles.insert(doc.title.clone());
                if let Some(ref s) = doc.status {
                    statuses.insert(s.clone());
                }
            }
            if titles.len() > 1 || statuses.len() > 1 {
                conflicts.push(id.clone());
            }
        }
    }
    let group_count: usize = docs.iter().flat_map(|d| d.groups.iter()).count();
    // Per-type counts and unknown-key stats
    let mut schema_sets: Vec<(SchemaCfg, globset::GlobSet)> = Vec::new();
    for sc in &cfg.schema {
        let mut b = globset::GlobSetBuilder::new();
        for p in &sc.file_patterns {
            if let Ok(g) = globset::Glob::new(p) {
                b.add(g);
            }
        }
        if let Ok(set) = b.build() {
            schema_sets.push((sc.clone(), set));
        }
    }
    let mut type_counts: BTreeMap<String, usize> = BTreeMap::new();
    let mut unknown_stats: BTreeMap<String, (usize, usize)> = BTreeMap::new(); // schema -> (docs_with_unknowns, total_unknown_keys)
    let reserved: BTreeSet<String> = [
        "id",
        "tags",
        "status",
        "groups",
        "depends_on",
        "supersedes",
        "superseded_by",
    ]
    .into_iter()
    .map(|s| s.to_string())
    .collect();
    for d in docs {
        let fname = d.file.file_name().and_then(|s| s.to_str()).unwrap_or("");
        let mut sname: Option<String> = None;
        for (sc, set) in &schema_sets {
            if set.is_match(fname) {
                sname = Some(sc.name.clone());
                break;
            }
        }
        if let Some(sname) = sname {
            *type_counts.entry(sname.clone()).or_insert(0) += 1;
            let present: BTreeSet<String> = d.fm.keys().cloned().collect();
            let rule_keys: BTreeSet<String> = cfg
                .schema
                .iter()
                .find(|s| s.name == sname)
                .map(|sc| sc.rules.keys().cloned().collect())
                .unwrap_or_default();
            let mut known: BTreeSet<String> = reserved.union(&rule_keys).cloned().collect();
            let req: BTreeSet<String> = cfg
                .schema
                .iter()
                .find(|s| s.name == sname)
                .map(|sc| sc.required.iter().cloned().collect())
                .unwrap_or_default();
            known = known.union(&req).cloned().collect();
            let allow: BTreeSet<String> = cfg
                .schema
                .iter()
                .find(|s| s.name == sname)
                .map(|sc| sc.allowed_keys.iter().cloned().collect())
                .unwrap_or_default();
            known = known.union(&allow).cloned().collect();
            let unknown: Vec<String> = present.difference(&known).cloned().collect();
            if !unknown.is_empty() {
                let e = unknown_stats.entry(sname).or_insert((0, 0));
                e.0 += 1;
                e.1 += unknown.len();
            }
        }
    }

    let per_base: Vec<serde_json::Value> = cfg
        .bases
        .iter()
        .map(|b| {
            let idx = b.join(&cfg.index_relative);
            let mode = if idx.exists() { "index" } else { "scan" };
            serde_json::json!({
                "base": b,
                "mode": mode
            })
        })
        .collect();

    serde_json::json!({
        "config": cfg_path.as_ref().map(|p| p.display().to_string()).unwrap_or("<defaults>".into()),
        "bases": cfg.bases,
        "per_base": per_base,
        "counts": {"docs": docs.len(), "group_entries": group_count},
        "conflicts": conflicts,
        "types": type_counts,
        "unknown_stats": unknown_stats,
    })
}

pub fn run(cfg: &Config, cfg_path: &Option<PathBuf>, format: &OutputFormat) -> Result<()> {
    let docs = load_docs(cfg)?;
    match format {
        OutputFormat::Json | OutputFormat::Ndjson => {
            let report = build_report(cfg, cfg_path, &docs);
            print_json(&report)?;
        }
        OutputFormat::Plain => {
            // Plain text output
            let report = build_report(cfg, cfg_path, &docs);
            let config_path = report.get("config").and_then(|v| v.as_str()).unwrap_or("");
            println!("Config: {}", config_path);
            println!("Bases:");
            for b in &cfg.bases {
                println!("  - {}", b.display());
            }
            println!("index_relative: {}", cfg.index_relative);
            println!("groups_relative: {}", cfg.groups_relative);
            for item in report
                .get("per_base")
                .and_then(|v| v.as_array())
                .unwrap_or(&Vec::new())
            {
                let base = item.get("base").and_then(|v| v.as_str()).unwrap_or("");
                let mode = item.get("mode").and_then(|v| v.as_str()).unwrap_or("");
                println!("Base {} → {}", base, mode);
            }
            let counts = report
                .get("counts")
                .and_then(|v| v.as_object())
                .cloned()
                .unwrap_or_default();
            let docs_count = counts.get("docs").and_then(|v| v.as_u64()).unwrap_or(0);
            let group_entries = counts
                .get("group_entries")
                .and_then(|v| v.as_u64())
                .unwrap_or(0);
            println!(
                "Found {} ADR-like files; group entries: {}",
                docs_count, group_entries
            );
            if let Some(arr) = report.get("conflicts").and_then(|v| v.as_array()) {
                if !arr.is_empty() {
                    let list: Vec<String> = arr
                        .iter()
                        .filter_map(|v| v.as_str().map(|s| s.to_string()))
                        .collect();
                    println!(
                        "Conflicts (ids with differing title/status): {}",
                        list.join(", ")
                    );
                }
            }
            if let Some(types) = report.get("types").and_then(|v| v.as_object()) {
                if !types.is_empty() {
                    println!("Types:");
                    for (k, v) in types {
                        println!("  - {}: {} notes", k, v.as_u64().unwrap_or(0));
                    }
                }
            }
            if let Some(unknown) = report.get("unknown_stats").and_then(|v| v.as_object()) {
                if !unknown.is_empty() {
                    println!("Unknown key stats:");
                    for (k, v) in unknown {
                        if let Some(arr) = v.as_array() {
                            if arr.len() == 2 {
                                let docs = arr[0].as_u64().unwrap_or(0);
                                let total = arr[1].as_u64().unwrap_or(0);
                                println!(
                                    "  - {}: {} notes with unknowns ({} keys)",
                                    k, docs, total
                                );
                            }
                        }
                    }
                }
            }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/graph.rs">
use anyhow::{anyhow, Result};
use std::collections::{BTreeMap, BTreeSet, HashMap};

use crate::cli::GraphFormat;
use crate::commands::output::print_json;
use crate::config::Config;
use crate::discovery::load_docs;
use crate::graph::compute_cluster;
use crate::model::AdrDoc;

#[derive(Debug, Clone)]
struct Edge {
    from: String,
    to: String,
}

fn sanitize_id(id: &str) -> String {
    id.chars()
        .map(|c| if c.is_ascii_alphanumeric() { c } else { '_' })
        .collect()
}

fn cluster_edges(cluster: &BTreeMap<String, AdrDoc>) -> Vec<Edge> {
    let members: BTreeSet<String> = cluster.keys().cloned().collect();
    let mut edges = Vec::new();
    for (id, doc) in cluster.iter() {
        for dep in &doc.depends_on {
            if members.contains(dep) {
                edges.push(Edge {
                    from: id.clone(),
                    to: dep.clone(),
                });
            }
        }
    }
    edges
}

pub(crate) fn render_mermaid(cluster: &BTreeMap<String, AdrDoc>) -> String {
    let mut out = String::from("flowchart LR\n");
    // Node declarations
    for (id, doc) in cluster.iter() {
        let var = sanitize_id(id);
        let label = format!("{}: {}", id, doc.title.replace('"', "\\\""));
        out.push_str(&format!("  {}[\"{}\"]\n", var, label));
    }
    // Edges
    for e in cluster_edges(cluster) {
        let from = sanitize_id(&e.from);
        let to = sanitize_id(&e.to);
        out.push_str(&format!("  {} --> {}\n", from, to));
    }
    out
}

pub(crate) fn render_dot(cluster: &BTreeMap<String, AdrDoc>) -> String {
    let mut out = String::from("digraph {\n");
    // Nodes
    for (id, doc) in cluster.iter() {
        let label = format!("{}: {}", id, doc.title.replace('"', "\\\""));
        out.push_str(&format!("  \"{}\" [label=\"{}\"];\n", id, label));
    }
    // Edges
    for e in cluster_edges(cluster) {
        out.push_str(&format!("  \"{}\" -> \"{}\";\n", e.from, e.to));
    }
    out.push_str("}\n");
    out
}

pub fn run(
    cfg: &Config,
    format: &GraphFormat,
    id: String,
    depth: Option<usize>,
    include_bidirectional: Option<bool>,
) -> Result<()> {
    let docs = load_docs(cfg)?;
    let depth = depth.unwrap_or(cfg.defaults.depth);
    let include_bidirectional = include_bidirectional.unwrap_or(cfg.defaults.include_bidirectional);
    let mut by_id: HashMap<String, AdrDoc> = HashMap::new();
    for d in &docs {
        if let Some(ref i) = d.id {
            by_id.insert(i.clone(), d.clone());
        }
    }
    if !by_id.contains_key(&id) {
        return Err(anyhow!("ADR not found: {}", id));
    }
    let cluster = compute_cluster(&id, depth, include_bidirectional, &by_id);
    match format {
        GraphFormat::Json => {
            let members: Vec<serde_json::Value> = cluster
                .iter()
                .map(|(oid, d)| {
                    serde_json::json!({
                        "id": oid,
                        "title": d.title,
                        "status": d.status,
                    })
                })
                .collect();
            let edges: Vec<serde_json::Value> = cluster_edges(&cluster)
                .into_iter()
                .map(|e| serde_json::json!({"from": e.from, "to": e.to}))
                .collect();
            let out = serde_json::json!({
                "root": id,
                "members": members,
                "edges": edges,
                "depth": depth,
                "bidirectional": include_bidirectional,
            });
            print_json(&out)?;
        }
        GraphFormat::Dot => {
            let s = render_dot(&cluster);
            println!("{}", s);
        }
        GraphFormat::Mermaid => {
            let s = render_mermaid(&cluster);
            println!("{}", s);
        }
    }
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    fn doc(id: &str, title: &str, deps: Vec<&str>) -> AdrDoc {
        AdrDoc {
            file: PathBuf::from(format!("{}.md", id)),
            id: Some(id.to_string()),
            title: title.to_string(),
            tags: vec![],
            status: None,
            groups: vec![],
            depends_on: deps.into_iter().map(|s| s.to_string()).collect(),
            supersedes: vec![],
            superseded_by: vec![],
            fm: BTreeMap::new(),
            mtime: None,
            size: None,
        }
    }

    #[test]
    fn test_render_mermaid_and_dot() {
        let mut cluster: BTreeMap<String, AdrDoc> = BTreeMap::new();
        cluster.insert("ADR-001".into(), doc("ADR-001", "Root", vec!["ADR-002"]));
        cluster.insert("ADR-002".into(), doc("ADR-002", "Child", vec![]));
        let mm = render_mermaid(&cluster);
        assert!(mm.contains("flowchart LR"));
        assert!(mm.contains("ADR_001 --> ADR_002"));
        let dot = render_dot(&cluster);
        assert!(dot.contains("digraph"));
        assert!(dot.contains("\"ADR-001\" -> \"ADR-002\""));
    }
}
</file>

<file path="src/commands/mod.rs">
pub mod cluster;
pub mod completions;
pub mod doctor;
pub mod get;
pub mod graph;
pub mod group;
pub mod init;
pub mod output;
pub mod path;
pub mod search;
pub mod topics;
pub mod validate_cmd;
pub mod watch_cmd;
</file>

<file path="src/commands/output.rs">
use anyhow::Result;
use std::str::FromStr;

pub fn print_json<T: serde::Serialize>(value: &T) -> Result<()> {
    println!("{}", serde_json::to_string_pretty(value)?);
    Ok(())
}

pub fn print_ndjson_value(value: &serde_json::Value) -> Result<()> {
    println!("{}", serde_json::to_string(value)?);
    Ok(())
}

pub fn print_ndjson_iter<T, I>(iter: I) -> Result<()>
where
    T: serde::Serialize,
    I: IntoIterator<Item = T>,
{
    for item in iter {
        println!("{}", serde_json::to_string(&item)?);
    }
    Ok(())
}

// Optional enum for centralized format selection (scaffold; not yet wired everywhere)
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Format { Plain, Json, Ndjson }

impl FromStr for Format {
    type Err = ();
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "plain" => Ok(Format::Plain),
            "json" => Ok(Format::Json),
            "ndjson" => Ok(Format::Ndjson),
            _ => Ok(Format::Plain),
        }
    }
}
</file>

<file path="src/commands/path.rs">
use anyhow::Result;

use crate::cli::OutputFormat;
use crate::commands::output::print_json;
use crate::config::Config;
use crate::discovery::load_docs;
use crate::graph::bfs_path;

pub fn run(
    cfg: &Config,
    format: &OutputFormat,
    from: String,
    to: String,
    max_depth: usize,
) -> Result<()> {
    let docs = load_docs(cfg)?;
    let mut by_id = std::collections::HashMap::new();
    for d in &docs {
        if let Some(ref i) = d.id {
            by_id.insert(i.clone(), d.clone());
        }
    }
    let res = bfs_path(&from, &to, max_depth, &by_id);
    match format {
        OutputFormat::Json | OutputFormat::Ndjson => {
            let out = serde_json::json!({"from": from, "to": to, "path": res});
            print_json(&out)?;
        }
        OutputFormat::Plain => {
            println!("# Dependency Path: {} → {}\n", from, to);
            match res {
                Some(path) => {
                    println!("**Path Length**: {} steps\n", path.len().saturating_sub(1));
                    println!("## Path");
                    for (i, node) in path.iter().enumerate() {
                        println!("{}. {}", i + 1, node);
                    }
                }
                None => println!("No path found between {} and {}", from, to),
            }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/watch_cmd.rs">
use anyhow::Result;

use crate::config::Config;
use crate::watch::{run_watch, WatchArgs};

pub fn run(cfg: &Config, full_rescan: bool, debounce_ms: u64, dry_run: bool) -> Result<()> {
    run_watch(
        cfg,
        WatchArgs {
            full_rescan,
            debounce_ms,
            dry_run,
            write_groups: false,
        },
    )
}
</file>

<file path="src/config.rs">
use anyhow::{Context, Result};
use globset::Glob;
use globset::GlobSetBuilder;
use serde::Deserialize;
use std::env;
use std::fs;
use std::io::Write;
use std::path::{Path, PathBuf};

#[derive(Debug, Deserialize, Clone)]
pub struct DefaultsCfg {
    #[serde(default = "default_depth")]
    pub depth: usize,
    #[serde(default = "default_true")]
    pub include_bidirectional: bool,
    #[serde(default = "default_true")]
    pub include_content: bool,
}

#[derive(Debug, Deserialize, Clone)]
pub struct SchemaRule {
    #[serde(default)]
    pub allowed: Vec<String>,
    #[serde(rename = "type")]
    pub r#type: Option<String>,
    #[serde(default)]
    pub min_items: Option<usize>,
    #[serde(default)]
    pub regex: Option<String>,
    #[serde(default)]
    pub refers_to_types: Option<Vec<String>>,
    #[serde(default)]
    pub severity: Option<String>, // error | warn
    #[serde(default)]
    pub format: Option<String>, // for date parsing
}

#[derive(Debug, Deserialize, Clone)]
pub struct SchemaCfg {
    pub name: String,
    pub file_patterns: Vec<String>,
    #[serde(default)]
    pub required: Vec<String>,
    #[serde(default)]
    pub unknown_policy: Option<String>, // ignore | warn | error (default ignore)
    #[serde(default)]
    pub allowed_keys: Vec<String>,
    #[serde(default)]
    pub rules: std::collections::BTreeMap<String, SchemaRule>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Config {
    #[serde(default = "default_bases")]
    pub bases: Vec<PathBuf>,
    #[serde(default = "default_index_rel")]
    pub index_relative: String,
    #[serde(default = "default_groups_rel")]
    pub groups_relative: String,
    #[serde(default = "default_file_patterns")]
    pub file_patterns: Vec<String>,
    #[serde(default = "default_ignore_globs")]
    pub ignore_globs: Vec<String>,
    #[serde(default = "default_allowed_statuses")]
    pub allowed_statuses: Vec<String>,
    #[serde(default = "default_defaults")]
    pub defaults: DefaultsCfg,
    // output config removed (was unused)
    #[serde(default)]
    pub schema: Vec<SchemaCfg>,
}

pub fn default_bases() -> Vec<PathBuf> {
    vec![PathBuf::from("docs/masterplan-v2")]
}
pub fn default_index_rel() -> String {
    "index/adr-index.json".to_string()
}
pub fn default_groups_rel() -> String {
    "index/semantic-groups.json".to_string()
}
pub fn default_file_patterns() -> Vec<String> {
    vec!["ADR-*.md".into(), "ADR-DB-*.md".into(), "IMP-*.md".into()]
}
pub fn default_ignore_globs() -> Vec<String> {
    vec!["**/node_modules/**".into(), "**/.obsidian/**".into()]
}
pub fn default_allowed_statuses() -> Vec<String> {
    vec![
        "draft".into(),
        "incomplete".into(),
        "proposed".into(),
        "accepted".into(),
        "complete".into(),
        "design".into(),
        "legacy-reference".into(),
        "superseded".into(),
    ]
}
pub fn default_depth() -> usize {
    2
}
pub fn default_true() -> bool {
    true
}
pub fn default_defaults() -> DefaultsCfg {
    DefaultsCfg {
        depth: default_depth(),
        include_bidirectional: true,
        include_content: true,
    }
}

pub fn find_config_upwards(explicit: &Option<PathBuf>) -> Option<PathBuf> {
    if let Some(p) = explicit {
        return Some(p.clone());
    }
    if let Ok(env_path) = env::var("ADR_RAG_CONFIG") {
        let p = PathBuf::from(env_path);
        if p.exists() {
            return Some(p);
        }
    }
    let mut dir = env::current_dir().ok()?;
    loop {
        let candidate = dir.join(".adr-rag.toml");
        if candidate.exists() {
            return Some(candidate);
        }
        let parent = dir.parent();
        match parent {
            Some(p) => dir = p.to_path_buf(),
            None => return None,
        }
    }
}

pub fn load_config(
    path_opt: &Option<PathBuf>,
    base_override: &Option<Vec<PathBuf>>,
) -> Result<(Config, Option<PathBuf>)> {
    let path = find_config_upwards(path_opt);
    let mut cfg: Config = if let Some(ref p) = path {
        let s = fs::read_to_string(p).with_context(|| format!("reading config {:?}", p))?;
        toml::from_str(&s).with_context(|| format!("parsing TOML config {:?}", p))?
    } else {
        Config {
            bases: default_bases(),
            index_relative: default_index_rel(),
            groups_relative: default_groups_rel(),
            file_patterns: default_file_patterns(),
            ignore_globs: default_ignore_globs(),
            allowed_statuses: default_allowed_statuses(),
            defaults: default_defaults(),
            schema: Vec::new(),
        }
    };
    // Env override for bases (comma-separated)
    if let Ok(env_bases) = env::var("ADR_RAG_BASES") {
        let list: Vec<PathBuf> = env_bases
            .split(',')
            .map(|s| PathBuf::from(s.trim()))
            .filter(|p| !p.as_os_str().is_empty())
            .collect();
        if !list.is_empty() {
            cfg.bases = list;
        }
    }
    if let Some(override_bases) = base_override {
        if !override_bases.is_empty() {
            cfg.bases = override_bases.clone();
        }
    }
    Ok((cfg, path))
}

pub fn write_template(path: &Path) -> Result<()> {
    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent).ok();
    }
    let mut f = fs::File::create(path).with_context(|| format!("creating {:?}", path))?;
    f.write_all(TEMPLATE.as_bytes())?;
    Ok(())
}

// Helper: compile schema globsets once for reuse across modules.
pub fn build_schema_sets(cfg: &Config) -> Vec<(SchemaCfg, globset::GlobSet)> {
    let mut out = Vec::new();
    for sc in &cfg.schema {
        let mut b = GlobSetBuilder::new();
        for p in &sc.file_patterns {
            if let Ok(g) = Glob::new(p) {
                b.add(g);
            }
        }
        if let Ok(set) = b.build() {
            out.push((sc.clone(), set));
        }
    }
    out
}

pub const TEMPLATE: &str = r#"# Repo-local ADR CLI config (adr-rag)

# One or more directories to scan or read an index from.
bases = [
  "docs/masterplan",
  # "docs/notes",
]

# Where to read/write the index and semantic groups (paths are relative to each base).
index_relative = "index/adr-index.json"
groups_relative = "index/semantic-groups.json"

# Discovery and semantics
file_patterns = ["ADR-*.md", "ADR-DB-*.md", "IMP-*.md"]
ignore_globs  = ["**/node_modules/**", "**/.obsidian/**"]
allowed_statuses = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]

[defaults]
depth = 2
include_bidirectional = true
include_content = true

# Note Types (Schema) — Optional, per-type rules and validation
#
# Define one or more [[schema]] blocks to validate different note types
# (e.g., ADR vs IMP). Matching is by file_patterns; first match wins.
# Unknown keys policy lets you treat unexpected front-matter as ignore|warn|error.
#
# [[schema]]
# name = "ADR"
# file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
# required = ["id", "tags", "status", "depends_on"]
# unknown_policy = "ignore"   # ignore | warn | error (default: ignore)
# allowed_keys = ["produces", "files_touched"]  # optional pass-through keys
#
# [schema.rules.status]
# allowed = [
#   "draft", "incomplete", "proposed", "accepted",
#   "complete", "design", "legacy-reference", "superseded"
# ]
# severity = "error"          # error | warn
#
# [schema.rules.depends_on]
# type = "array"
# items = { type = "string", regex = "^(ADR|IMP)-\\d+" }
# refers_to_types = ["ADR", "IMP"]
# severity = "error"
#
# [[schema]]
# name = "IMP"
# file_patterns = ["IMP-*.md"]
# required = ["id","tags","depends_on","status","completion_date"]
# unknown_policy = "warn"
#
# [schema.rules.status]
# allowed = ["in-progress","blocked","on-hold","cancelled","done"]
# severity = "error"
#
# [schema.rules.completion_date]
# type = "date"
# format = "%Y-%m-%d"
# severity = "warn"

# Default schemas (enabled): tweak as needed

[[schema]]
name = "ADR"
file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
required = ["id", "tags", "status", "depends_on"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]
severity = "error"

[[schema]]
name = "IMP"
file_patterns = ["IMP-*.md"]
required = ["id", "tags", "depends_on", "status"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = ["in-progress", "blocked", "on-hold", "cancelled", "done"]
severity = "error"
"#;
</file>

<file path="src/graph.rs">
use std::collections::{BTreeSet, HashMap, HashSet, VecDeque};

use crate::model::AdrDoc;

// Compute a dependency path between two ADR ids using BFS over
// a bidirectional graph (depends_on edges + reverse dependents).
pub fn bfs_path(
    from: &str,
    to: &str,
    max_depth: usize,
    by_id: &HashMap<String, AdrDoc>,
) -> Option<Vec<String>> {
    if from == to {
        return Some(vec![from.into()]);
    }
    let mut q: VecDeque<(String, Vec<String>, usize)> = VecDeque::new();
    let mut visited = HashSet::new();
    q.push_back((from.into(), vec![from.into()], 0));
    visited.insert(from.into());
    while let Some((cur, path, depth)) = q.pop_front() {
        if depth >= max_depth {
            continue;
        }
        if let Some(doc) = by_id.get(&cur) {
            let mut neighbors: BTreeSet<String> = BTreeSet::new();
            for dep in &doc.depends_on {
                neighbors.insert(dep.clone());
            }
            for (oid, other) in by_id.iter() {
                if other.depends_on.iter().any(|d| d == &cur) {
                    neighbors.insert(oid.clone());
                }
            }
            for n in neighbors {
                if n == to {
                    let mut p = path.clone();
                    p.push(n);
                    return Some(p);
                }
                if !visited.contains(&n) {
                    visited.insert(n.clone());
                    let mut p = path.clone();
                    p.push(n.clone());
                    q.push_back((n, p, depth + 1));
                }
            }
        }
    }
    None
}

// Compute a cluster around an ADR id up to a depth, optionally including dependents.
pub fn compute_cluster(
    id: &str,
    depth: usize,
    include_bidirectional: bool,
    by_id: &HashMap<String, AdrDoc>,
) -> std::collections::BTreeMap<String, AdrDoc> {
    let mut visited = HashSet::new();
    let mut cluster: std::collections::BTreeMap<String, AdrDoc> = std::collections::BTreeMap::new();
    fn traverse(
        current: &str,
        depth: usize,
        include_bidir: bool,
        by_id: &HashMap<String, AdrDoc>,
        acc: &mut std::collections::BTreeMap<String, AdrDoc>,
        visited: &mut HashSet<String>,
    ) {
        if depth == 0 || visited.contains(current) {
            return;
        }
        visited.insert(current.to_string());
        if let Some(doc) = by_id.get(current) {
            acc.insert(current.to_string(), doc.clone());
            for dep in &doc.depends_on {
                traverse(dep, depth - 1, include_bidir, by_id, acc, visited);
            }
            if include_bidir {
                for (oid, other) in by_id.iter() {
                    if other.depends_on.iter().any(|d| d == current) {
                        traverse(oid, depth - 1, include_bidir, by_id, acc, visited);
                    }
                }
            }
        }
    }
    traverse(
        id,
        depth,
        include_bidirectional,
        by_id,
        &mut cluster,
        &mut visited,
    );
    cluster
}
</file>

<file path="src/util.rs">
use anyhow::{anyhow, Result};
use std::path::Path;

pub fn try_open_editor(path: &Path) -> Result<()> {
    let editors = vec![
        std::env::var("VISUAL").ok(),
        std::env::var("EDITOR").ok(),
        Some("nano".into()),
        Some("vi".into()),
        Some("vim".into()),
    ];
    for ed in editors.into_iter().flatten() {
        let status = std::process::Command::new(ed.clone()).arg(path).status();
        if let Ok(st) = status {
            if st.success() {
                return Ok(());
            }
        }
    }
    Err(anyhow!("no editor found or editor failed"))
}
</file>

<file path="src/commands/cluster.rs">
use crate::cli::OutputFormat;
use crate::commands::output::{print_json, print_ndjson_iter, print_ndjson_value};
use crate::protocol::ClusterMember;
use anyhow::Result;

use crate::config::Config;
use crate::discovery::load_docs;
use crate::graph::compute_cluster;

pub fn run(
    cfg: &Config,
    format: &OutputFormat,
    id: String,
    depth: Option<usize>,
    include_bidirectional: Option<bool>,
) -> Result<()> {
    let docs = load_docs(cfg)?;
    let depth = depth.unwrap_or(cfg.defaults.depth);
    let include_bidirectional = include_bidirectional.unwrap_or(cfg.defaults.include_bidirectional);
    let mut by_id = std::collections::HashMap::new();
    for d in &docs {
        if let Some(ref i) = d.id {
            by_id.insert(i.clone(), d.clone());
        }
    }
    let cluster = compute_cluster(&id, depth, include_bidirectional, &by_id);
    match format {
        OutputFormat::Json => {
            let members: Vec<ClusterMember> = cluster
                .iter()
                .map(|(oid, d)| ClusterMember {
                    id: oid.clone(),
                    title: d.title.clone(),
                    status: d.status.clone(),
                    groups: d.groups.clone(),
                })
                .collect();
            let out = serde_json::json!({"root": id, "size": cluster.len(), "members": members});
            print_json(&out)?;
        }
        OutputFormat::Ndjson => {
            let header = serde_json::json!({"root": id, "count": cluster.len()});
            print_ndjson_value(&header)?;
            let members = cluster.iter().map(|(oid, d)| ClusterMember {
                id: oid.clone(),
                title: d.title.clone(),
                status: d.status.clone(),
                groups: d.groups.clone(),
            });
            print_ndjson_iter(members)?;
        }
        OutputFormat::Plain => {
            println!("# Dependency Cluster for {}\n", id);
            println!("**Cluster Size**: {}\n", cluster.len());
            println!("## Members");
            for (oid, d) in &cluster {
                println!("- {}: {}", oid, d.title);
            }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/get.rs">
use anyhow::{anyhow, Result};
use std::fs;

use crate::cli::OutputFormat;
use crate::commands::output::print_json;
use crate::config::Config;
use crate::discovery::load_docs;

pub fn run(
    cfg: &Config,
    format: &OutputFormat,
    id: String,
    include_dependents: bool,
) -> Result<()> {
    let docs = load_docs(cfg)?;
    let mut by_id = std::collections::HashMap::new();
    for d in &docs {
        if let Some(ref i) = d.id {
            by_id.insert(i.clone(), d.clone());
        }
    }
    let primary = by_id
        .get(&id)
        .ok_or_else(|| anyhow!("ADR not found: {}", id))?;
    let deps: Vec<crate::model::AdrDoc> = primary
        .depends_on
        .iter()
        .filter_map(|dep| by_id.get(dep).cloned())
        .collect();
    let mut dependents = Vec::new();
    if include_dependents {
        for d in &docs {
            if d.depends_on.iter().any(|dep| dep == &id) {
                dependents.push(d.clone());
            }
        }
    }
    match format {
        OutputFormat::Json | OutputFormat::Ndjson => {
            let out = serde_json::json!({
                "id": id,
                "title": primary.title,
                "file": primary.file,
                "tags": primary.tags,
                "status": primary.status,
                "depends_on": deps.iter().filter_map(|d| d.id.clone()).collect::<Vec<_>>(),
                "dependents": dependents.iter().filter_map(|d| d.id.clone()).collect::<Vec<_>>(),
                "content": fs::read_to_string(&primary.file).unwrap_or_default(),
            });
            print_json(&out)?;
        }
        OutputFormat::Plain => {
            println!("# {}: {}\n", id, primary.title);
            if !primary.depends_on.is_empty() {
                println!("## Depends On");
                for d in &deps {
                    println!("- {}: {}", d.id.clone().unwrap_or_default(), d.title);
                }
                println!();
            }
            if include_dependents && !dependents.is_empty() {
                println!("## Dependents ({})", dependents.len());
                for d in &dependents {
                    println!("- {}: {}", d.id.clone().unwrap_or_default(), d.title);
                }
                println!();
            }
            let content = fs::read_to_string(&primary.file).unwrap_or_default();
            println!("## Content\n\n{}", content);
        }
    }
    Ok(())
}
</file>

<file path="src/commands/group.rs">
use crate::cli::OutputFormat;
use crate::commands::output::{print_json, print_ndjson_iter, print_ndjson_value};
use crate::protocol::GroupMember;
use anyhow::Result;
use std::fs;

use crate::config::Config;
use crate::discovery::load_docs;

pub fn run(
    cfg: &Config,
    format: &OutputFormat,
    topic: String,
    include_content: Option<bool>,
) -> Result<()> {
    let t = topic.to_lowercase();
    let docs = load_docs(cfg)?;
    let mut matches: Vec<crate::model::AdrDoc> = docs
        .into_iter()
        .filter(|d| d.groups.iter().any(|g| g.to_lowercase().contains(&t)))
        .collect();
    matches.sort_by(|a, b| a.title.to_lowercase().cmp(&b.title.to_lowercase()));
    let include_content = include_content.unwrap_or(cfg.defaults.include_content);
    match format {
        OutputFormat::Json => {
            let members: Vec<GroupMember> = matches
                .iter()
                .filter_map(|d| d.id.as_ref().map(|id| (id, d)))
                .map(|(id, d)| GroupMember {
                    id: id.clone(),
                    title: d.title.clone(),
                    status: d.status.clone(),
                    groups: d.groups.clone(),
                    file: Some(d.file.clone()),
                })
                .collect();
            let mut out =
                serde_json::json!({"topic": topic, "count": members.len(), "adrs": members});
            if include_content {
                if let Some(obj) = out.as_object_mut() {
                    let contents: Vec<String> = matches
                        .iter()
                        .map(|d| fs::read_to_string(&d.file).unwrap_or_default())
                        .collect();
                    obj.insert("contents".into(), serde_json::json!(contents));
                }
            }
            print_json(&out)?;
        }
        OutputFormat::Ndjson => {
            let members: Vec<GroupMember> = matches
                .iter()
                .filter_map(|d| d.id.as_ref().map(|id| (id, d)))
                .map(|(id, d)| GroupMember {
                    id: id.clone(),
                    title: d.title.clone(),
                    status: d.status.clone(),
                    groups: d.groups.clone(),
                    file: Some(d.file.clone()),
                })
                .collect();
            let header = serde_json::json!({"topic": topic, "count": members.len()});
            print_ndjson_value(&header)?;
            print_ndjson_iter(members)?;
        }
        OutputFormat::Plain => {
            println!("# Semantic Group: {}\n", topic);
            println!("**ADR Count**: {}\n", matches.len());
            println!("## ADRs in this group");
            for d in &matches {
                println!("- {}: {}", d.id.clone().unwrap_or_default(), d.title);
            }
            if include_content {
                println!("\n## Content\n");
                for d in &matches {
                    let content = fs::read_to_string(&d.file).unwrap_or_default();
                    println!(
                        "### {}: {}\n\n{}\n",
                        d.id.clone().unwrap_or_default(),
                        d.title,
                        content
                    );
                }
            }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/init.rs">
use anyhow::Result;
use std::path::PathBuf;

use crate::config::{find_config_upwards, write_template, TEMPLATE};
use crate::util::try_open_editor;

pub fn run(path: Option<PathBuf>, force: bool, print_template: bool, silent: bool) -> Result<()> {
    let target = path.unwrap_or_else(|| PathBuf::from(".adr-rag.toml"));
    if print_template {
        print!("{}", TEMPLATE);
        return Ok(());
    }
    let existed = target.exists();
    if existed && !force {
        eprintln!(
            "Config exists: {} (not overwriting; use --force to rewrite)",
            target.display()
        );
    }
    if let Some(parent_cfg) = find_config_upwards(&None) {
        if let Ok(cur) = std::env::current_dir() {
            if parent_cfg != target && parent_cfg.parent() != Some(cur.as_path()) {
                eprintln!("Warning: a parent config exists at {} and may be shadowed by creating one here", parent_cfg.display());
            }
        }
    }
    if !existed || force {
        write_template(&target)?;
        println!("Wrote {}", target.display());
    }
    if !silent {
        if let Err(e) = try_open_editor(&target) {
            eprintln!("Note: could not open editor automatically: {}", e);
        }
    }
    Ok(())
}
</file>

<file path="src/commands/search.rs">
use crate::cli::OutputFormat;
use crate::commands::output::{print_json, print_ndjson_iter};
use crate::protocol::SearchResult;
use anyhow::Result;

use crate::config::Config;
use crate::discovery::load_docs;

pub fn run(cfg: &Config, format: &OutputFormat, query: String) -> Result<()> {
    let q = query.to_lowercase();
    let docs = load_docs(cfg)?;
    let mut hits: Vec<&crate::model::AdrDoc> = Vec::new();
    for d in &docs {
        let id = d.id.clone().unwrap_or_default();
        if id.to_lowercase().contains(&q) || d.title.to_lowercase().contains(&q) {
            hits.push(d);
        }
    }
    match format {
        OutputFormat::Json => {
            let arr: Vec<SearchResult> = hits
                .iter()
                .filter_map(|d| d.id.as_ref().map(|id| (id, *d)))
                .map(|(id, d)| SearchResult {
                    id: id.clone(),
                    title: d.title.clone(),
                    file: d.file.clone(),
                    tags: d.tags.clone(),
                    status: d.status.clone(),
                    groups: d.groups.clone(),
                })
                .collect();
            print_json(&arr)?;
        }
        OutputFormat::Ndjson => {
            let it = hits
                .into_iter()
                .filter_map(|d| d.id.as_ref().map(|id| (id.clone(), d)))
                .map(|(id, d)| SearchResult {
                    id,
                    title: d.title.clone(),
                    file: d.file.clone(),
                    tags: d.tags.clone(),
                    status: d.status.clone(),
                    groups: d.groups.clone(),
                });
            print_ndjson_iter::<SearchResult, _>(it)?;
        }
        OutputFormat::Plain => {
            for d in hits {
                println!(
                    "{}\t{}\t{}",
                    d.id.clone().unwrap_or_default(),
                    d.title,
                    d.file.display()
                );
            }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/topics.rs">
use crate::cli::OutputFormat;
use crate::commands::output::{print_json, print_ndjson_iter};
use crate::protocol::TopicCount;
use anyhow::{Context, Result};
use serde_json::Value;

use crate::config::Config;
use crate::discovery::load_docs;
use std::fs;

pub fn run(cfg: &Config, format: &OutputFormat) -> Result<()> {
    use std::collections::BTreeMap;
    let mut groups: BTreeMap<String, usize> = BTreeMap::new();
    let mut used_groups_file = false;
    'outer: for b in &cfg.bases {
        let path = b.join(&cfg.groups_relative);
        if path.exists() {
            used_groups_file = true;
            let s =
                fs::read_to_string(&path).with_context(|| format!("reading groups {:?}", path))?;
            let v: Value =
                serde_json::from_str(&s).with_context(|| format!("parsing groups {:?}", path))?;
            if let Some(sections) = v.get("sections").and_then(|x| x.as_array()) {
                for sec in sections {
                    let title = sec.get("title").and_then(|x| x.as_str()).unwrap_or("");
                    let mut count = 0usize;
                    if let Some(sels) = sec.get("selectors").and_then(|x| x.as_array()) {
                        for sel in sels {
                            if let Some(ids) = sel.get("anyIds").and_then(|x| x.as_array()) {
                                count += ids.len();
                            }
                        }
                    }
                    *groups.entry(title.to_string()).or_insert(0) += count;
                }
            }
            break 'outer;
        }
    }
    if !used_groups_file {
        let docs = load_docs(cfg)?;
        for d in docs {
            for g in d.groups {
                *groups.entry(g).or_insert(0) += 1;
            }
        }
    }
    match format {
        OutputFormat::Json => {
            let arr: Vec<TopicCount> = groups
                .into_iter()
                .map(|(topic, count)| TopicCount { topic, count })
                .collect();
            print_json(&arr)?;
        }
        OutputFormat::Ndjson => {
            let it = groups
                .into_iter()
                .map(|(topic, count)| TopicCount { topic, count });
            print_ndjson_iter::<TopicCount, _>(it)?;
        }
        OutputFormat::Plain => {
            if groups.is_empty() {
                println!("No semantic groups found");
                return Ok(());
            }
            println!("# Available Semantic Topics\n");
            for (name, count) in groups {
                println!("- {}: {} ADRs", name, count);
            }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/validate_cmd.rs">
use anyhow::Result;

use crate::cli::OutputFormat;
use crate::commands::output::{print_json, print_ndjson_iter, print_ndjson_value};
use crate::config::Config;
use crate::discovery::incremental_collect_docs;
use crate::index::{write_groups_config, write_indexes};
use crate::protocol::{ValidateHeader, ValidateIssue};
use crate::validate::validate_docs;

pub fn run(
    cfg: &Config,
    format: &OutputFormat,
    write_groups: bool,
    dry_run: bool,
    full_rescan: bool,
) -> Result<()> {
    let docs = incremental_collect_docs(cfg, full_rescan)?;
    let report = validate_docs(cfg, &docs);
    match format {
        OutputFormat::Json => {
            let errors: Vec<ValidateIssue> = report
                .errors
                .iter()
                .map(|m| ValidateIssue {
                    kind: "error".into(),
                    file: None,
                    message: m.clone(),
                    code: None,
                })
                .collect();
            let warnings: Vec<ValidateIssue> = report
                .warnings
                .iter()
                .map(|m| ValidateIssue {
                    kind: "warning".into(),
                    file: None,
                    message: m.clone(),
                    code: None,
                })
                .collect();
            let obj = serde_json::json!({
                "ok": report.ok,
                "doc_count": docs.len(),
                "errors": errors,
                "warnings": warnings,
            });
            print_json(&obj)?;
        }
        OutputFormat::Ndjson => {
            // Emit a header then each error and warning as individual typed records
            let header = ValidateHeader {
                ok: report.ok,
                doc_count: docs.len(),
            };
            print_ndjson_value(&serde_json::to_value(&header)?)?;
            let errs = report.errors.iter().map(|m| ValidateIssue {
                kind: "error".into(),
                file: None,
                message: m.clone(),
                code: None,
            });
            let warns = report.warnings.iter().map(|m| ValidateIssue {
                kind: "warning".into(),
                file: None,
                message: m.clone(),
                code: None,
            });
            print_ndjson_iter(errs.chain(warns))?;
        }
        OutputFormat::Plain => {
            if report.ok {
                println!("Validation OK ({} docs)", docs.len());
            } else {
                eprintln!("Validation failed:");
                for e in &report.errors {
                    eprintln!(" - {}", e);
                }
            }
            if !report.warnings.is_empty() {
                eprintln!("Warnings:");
                for w in &report.warnings {
                    eprintln!(" - {}", w);
                }
            }
        }
    }
    if report.ok && !dry_run {
        write_indexes(cfg, &docs, true, true)?;
    }
    if write_groups && !dry_run {
        write_groups_config(cfg, &docs)?;
    }
    if !report.ok {
        std::process::exit(1);
    }
    Ok(())
}
</file>

<file path="src/cli.rs">
use clap::{Args, Parser, Subcommand, ValueEnum};
use std::path::PathBuf;

#[derive(ValueEnum, Debug, Clone, Copy)]
pub enum OutputFormat {
    Plain,
    Json,
    Ndjson,
}

#[derive(ValueEnum, Debug, Clone, Copy)]
pub enum GraphFormat {
    Mermaid,
    Dot,
    Json,
}

#[derive(Parser, Debug)]
#[command(
    name = "adr-rag",
    version,
    about = "Per-repo ADR navigator with TOML config"
)]
pub struct Cli {
    #[arg(long, global = true)]
    pub config: Option<PathBuf>,

    #[arg(long, value_delimiter = ',', global = true)]
    pub base: Option<Vec<PathBuf>>,

    /// Global output format
    #[arg(long, value_enum, global = true, default_value_t = OutputFormat::Plain)]
    pub format: OutputFormat,

    #[command(subcommand)]
    pub command: Commands,
}

#[derive(Subcommand, Debug)]
pub enum Commands {
    Init {
        /// Optional path to write config (defaults to ./.adr-rag.toml)
        #[arg(long)]
        path: Option<PathBuf>,
        #[arg(long)]
        force: bool,
        #[arg(long)]
        print_template: bool,
        /// Do not open the config in an editor after creating or detecting it
        #[arg(long, default_value_t = false)]
        silent: bool,
    },
    Doctor {},
    Search {
        #[arg(long, short = 'q')]
        query: String,
    },
    Topics {},
    Group {
        #[arg(long)]
        topic: String,
        #[arg(long)]
        include_content: Option<bool>,
    },
    Get {
        #[arg(long)]
        id: String,
        #[arg(long, default_value_t = false)]
        include_dependents: bool,
    },
    Cluster {
        #[arg(long)]
        id: String,
        #[arg(long)]
        depth: Option<usize>,
        #[arg(long)]
        include_bidirectional: Option<bool>,
    },
    Path {
        #[arg(long, value_name = "FROM")]
        from: String,
        #[arg(long, value_name = "TO")]
        to: String,
        #[arg(long, default_value_t = 5)]
        max_depth: usize,
    },
    /// Export a dependency graph (mermaid|dot|json)
    Graph {
        #[arg(long)]
        id: String,
        #[arg(long)]
        depth: Option<usize>,
        #[arg(long)]
        include_bidirectional: Option<bool>,
        /// Output format
        #[arg(long, value_enum, default_value_t = GraphFormat::Mermaid)]
        format: GraphFormat,
    },
    Validate(ValidateArgs),

    /// Watch bases and incrementally validate + update indexes on changes
    Watch {
        /// Force full rescan on first run
        #[arg(long, default_value_t = false)]
        full_rescan: bool,
        /// Debounce milliseconds for coalescing FS events
        #[arg(long, default_value_t = 400)]
        debounce_ms: u64,
        /// Print only; do not write indexes or groups
        #[arg(long, default_value_t = false)]
        dry_run: bool,
    },

    /// Generate shell completions (bash|zsh|fish)
    Completions {
        #[arg(value_name = "SHELL")]
        shell: String,
    },
}

#[derive(Args, Debug)]
pub struct ValidateArgs {
    #[arg(long, value_enum, default_value_t = OutputFormat::Plain)]
    pub format: OutputFormat,
    #[arg(long, default_value_t = false)]
    pub write_groups: bool,
    /// Do not write index/groups; print results only
    #[arg(long, default_value_t = false)]
    pub dry_run: bool,
    /// Force full rescan instead of incremental
    #[arg(long, default_value_t = false)]
    pub full_rescan: bool,
}
</file>

<file path="src/discovery.rs">
use anyhow::{Context, Result};
use globset::{Glob, GlobSetBuilder};
use globwalk::GlobWalkerBuilder;
use serde_json::Value;
use std::fs;
use std::path::Path;

use crate::config::Config;
use crate::model::{file_mtime, file_size, parse_front_matter_and_title, AdrDoc};

pub fn scan_docs(cfg: &Config) -> Result<Vec<AdrDoc>> {
    let mut docs = Vec::new();
    let mut ig_builder = GlobSetBuilder::new();
    for pat in &cfg.ignore_globs {
        ig_builder.add(Glob::new(pat)?);
    }
    let ignore_set = ig_builder.build()?;
    for base in &cfg.bases {
        for pattern in &cfg.file_patterns {
            let builder = GlobWalkerBuilder::from_patterns(base, &[pattern.as_str()]);
            let walker = builder.build()?;
            for entry in walker.filter_map(Result::ok) {
                let path = entry.path().to_path_buf();
                if path.is_file() {
                    if ignore_set.is_match(&path) {
                        continue;
                    }
                    let content =
                        fs::read_to_string(&path).with_context(|| format!("reading {:?}", path))?;
                    let doc = parse_front_matter_and_title(&content, &path);
                    docs.push(doc);
                }
            }
        }
    }
    Ok(docs)
}

pub fn scan_docs_in_base(base: &Path, cfg: &Config) -> Result<Vec<AdrDoc>> {
    let mut docs = Vec::new();
    let mut ig_builder = GlobSetBuilder::new();
    for pat in &cfg.ignore_globs {
        ig_builder.add(Glob::new(pat)?);
    }
    let ignore_set = ig_builder.build()?;
    for pattern in &cfg.file_patterns {
        let builder = GlobWalkerBuilder::from_patterns(base, &[pattern.as_str()]);
        let walker = builder.build()?;
        for entry in walker.filter_map(Result::ok) {
            let path = entry.path().to_path_buf();
            if path.is_file() {
                if ignore_set.is_match(&path) {
                    continue;
                }
                let content =
                    fs::read_to_string(&path).with_context(|| format!("reading {:?}", path))?;
                let doc = parse_front_matter_and_title(&content, &path);
                docs.push(doc);
            }
        }
    }
    Ok(docs)
}

pub fn load_docs_from_index(base: &Path, cfg: &Config) -> Result<Vec<AdrDoc>> {
    let index_path = base.join(&cfg.index_relative);
    let data = fs::read_to_string(&index_path)
        .with_context(|| format!("reading index {:?}", index_path))?;
    let root: Value =
        serde_json::from_str(&data).with_context(|| format!("parsing index {:?}", index_path))?;
    let mut docs = Vec::new();
    let items_opt: Option<&Vec<Value>> = if let Some(items) = root.as_array() {
        Some(items)
    } else if let Some(items) = root.get("items").and_then(|v| v.as_array()) {
        Some(items)
    } else {
        None
    };
    if let Some(items) = items_opt {
        for item in items {
            let file_rel = item.get("file").and_then(|v| v.as_str()).unwrap_or("");
            let file = base.join(file_rel);
            let id = item
                .get("id")
                .and_then(|v| v.as_str())
                .map(|s| s.to_string());
            let title = item
                .get("title")
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            let tags = item
                .get("tags")
                .and_then(|v| v.as_array())
                .map(|a| {
                    a.iter()
                        .filter_map(|x| x.as_str().map(|s| s.to_string()))
                        .collect()
                })
                .unwrap_or_else(Vec::new);
            let status = item
                .get("status")
                .and_then(|v| v.as_str())
                .map(|s| s.to_string());
            let groups = item
                .get("groups")
                .and_then(|v| v.as_array())
                .map(|a| {
                    a.iter()
                        .filter_map(|x| x.as_str().map(|s| s.to_string()))
                        .collect()
                })
                .unwrap_or_else(Vec::new);
            let depends_on = item
                .get("depends_on")
                .and_then(|v| v.as_array())
                .map(|a| {
                    a.iter()
                        .filter_map(|x| x.as_str().map(|s| s.to_string()))
                        .collect()
                })
                .unwrap_or_else(Vec::new);
            let supersedes = match item.get("supersedes") {
                Some(Value::Array(a)) => a
                    .iter()
                    .filter_map(|x| x.as_str().map(|s| s.to_string()))
                    .collect(),
                Some(Value::String(s)) => vec![s.to_string()],
                _ => Vec::new(),
            };
            let superseded_by = match item.get("superseded_by") {
                Some(Value::Array(a)) => a
                    .iter()
                    .filter_map(|x| x.as_str().map(|s| s.to_string()))
                    .collect(),
                Some(Value::String(s)) => vec![s.to_string()],
                _ => Vec::new(),
            };
            let mtime = item.get("mtime").and_then(|v| v.as_u64());
            let size = item.get("size").and_then(|v| v.as_u64());
            docs.push(AdrDoc {
                file,
                id,
                title,
                tags,
                status,
                groups,
                depends_on,
                supersedes,
                superseded_by,
                fm: std::collections::BTreeMap::new(),
                mtime,
                size,
            });
        }
    }
    Ok(docs)
}

pub fn load_docs(cfg: &Config) -> Result<Vec<AdrDoc>> {
    let mut combined: Vec<AdrDoc> = Vec::new();
    for base in &cfg.bases {
        let index_path = base.join(&cfg.index_relative);
        let use_index = index_path.exists();
        let mut docs = if use_index {
            load_docs_from_index(base, cfg)?
        } else {
            scan_docs_in_base(base, cfg)?
        };
        combined.append(&mut docs);
    }
    Ok(dedupe_by_id(combined))
}

pub fn incremental_collect_docs(cfg: &Config, full_rescan: bool) -> Result<Vec<AdrDoc>> {
    let mut ig_builder = GlobSetBuilder::new();
    for pat in &cfg.ignore_globs {
        ig_builder.add(Glob::new(pat)?);
    }
    let ignore_set = ig_builder.build()?;
    let mut combined: Vec<AdrDoc> = Vec::new();
    for base in &cfg.bases {
        let mut prior = std::collections::HashMap::<String, AdrDoc>::new();
        let index_path = base.join(&cfg.index_relative);
        if index_path.exists() {
            let idx_docs = load_docs_from_index(base, cfg)?;
            for d in idx_docs {
                if let Ok(rel) = d.file.strip_prefix(base) {
                    prior.insert(rel.to_string_lossy().to_string(), d);
                }
            }
        }
        let mut seen_rel: std::collections::HashSet<String> = std::collections::HashSet::new();
        for pattern in &cfg.file_patterns {
            let walker = GlobWalkerBuilder::from_patterns(base, &[pattern.as_str()]).build()?;
            for entry in walker.filter_map(Result::ok) {
                let path = entry.path().to_path_buf();
                if !path.is_file() {
                    continue;
                }
                if ignore_set.is_match(&path) {
                    continue;
                }
                let rel = path
                    .strip_prefix(base)
                    .unwrap_or(&path)
                    .to_string_lossy()
                    .to_string();
                seen_rel.insert(rel.clone());
                let cur_mtime = file_mtime(&path).unwrap_or(0);
                let cur_size = file_size(&path).unwrap_or(0);
                let need_parse = full_rescan
                    || match prior.get(&rel) {
                        None => true,
                        Some(old) => {
                            old.mtime.unwrap_or(0) != cur_mtime || old.size.unwrap_or(0) != cur_size
                        }
                    };
                if need_parse {
                    let content =
                        fs::read_to_string(&path).with_context(|| format!("reading {:?}", path))?;
                    let mut doc = parse_front_matter_and_title(&content, &path);
                    doc.mtime = Some(cur_mtime);
                    doc.size = Some(cur_size);
                    combined.push(doc);
                } else if let Some(mut d_old) = prior.get(&rel).cloned() {
                    d_old.mtime = Some(cur_mtime);
                    d_old.size = Some(cur_size);
                    combined.push(d_old);
                }
            }
        }
        // Removed files implicitly dropped
    }
    Ok(dedupe_by_id(combined))
}

fn dedupe_by_id(mut docs: Vec<AdrDoc>) -> Vec<AdrDoc> {
    use std::collections::HashMap;
    let mut by_id: HashMap<String, AdrDoc> = HashMap::new();
    let mut no_id: Vec<AdrDoc> = Vec::new();
    for d in docs.drain(..) {
        if let Some(id) = &d.id {
            let replace = match by_id.get(id) {
                Some(existing) => {
                    let a = existing.mtime;
                    let b = d.mtime;
                    match (a, b) {
                        (Some(a), Some(b)) => b > a,
                        (None, Some(_)) => true,
                        _ => false,
                    }
                }
                None => true,
            };
            if replace {
                by_id.insert(id.clone(), d);
            }
        } else {
            no_id.push(d);
        }
    }
    let mut out: Vec<AdrDoc> = by_id.into_values().collect();
    out.extend(no_id);
    out
}
</file>

<file path="src/index.rs">
use anyhow::{Context, Result};
use std::fs;
use std::time::{Duration, SystemTime};

use crate::config::{build_schema_sets, Config};
use crate::model::AdrDoc;

pub fn write_indexes(
    cfg: &Config,
    docs: &Vec<AdrDoc>,
    _force: bool,
    _auto_write: bool,
) -> Result<()> {
    let schema_sets = build_schema_sets(cfg);
    for base in &cfg.bases {
        let mut list = Vec::new();
        for d in docs {
            if d.file.starts_with(base) {
                let mut note_type: Option<String> = None;
                let fname = d.file.file_name().and_then(|s| s.to_str()).unwrap_or("");
                for (sc, set) in &schema_sets {
                    if set.is_match(fname) {
                        note_type = Some(sc.name.clone());
                        break;
                    }
                }
                list.push(serde_json::json!({
                    "file": d.file.strip_prefix(base).unwrap_or(&d.file).to_string_lossy(),
                    "id": d.id.clone().unwrap_or_default(),
                    "title": d.title,
                    "tags": d.tags,
                    "status": d.status.clone().unwrap_or_default(),
                    "depends_on": d.depends_on,
                    "supersedes": d.supersedes,
                    "superseded_by": d.superseded_by,
                    "groups": d.groups,
                    "type": note_type,
                    "mtime": d.mtime,
                    "size": d.size,
                }));
            }
        }
        let now = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap_or(Duration::from_secs(0))
            .as_secs();
        let wrapper = serde_json::json!({
            "index_version": 1,
            "generated_at": now,
            "items": list,
        });
        let out_path = base.join(&cfg.index_relative);
        if let Some(parent) = out_path.parent() {
            fs::create_dir_all(parent).ok();
        }
        fs::write(&out_path, serde_json::to_string_pretty(&wrapper)?)
            .with_context(|| format!("writing index to {}", out_path.display()))?;
        eprintln!(
            "Wrote index: {} ({} entries)",
            out_path.display(),
            wrapper
                .get("items")
                .and_then(|v| v.as_array())
                .map(|a| a.len())
                .unwrap_or(0)
        );
    }
    Ok(())
}

pub fn write_groups_config(cfg: &Config, docs: &Vec<AdrDoc>) -> Result<()> {
    use std::collections::{BTreeMap, BTreeSet};
    let mut by_group: BTreeMap<String, BTreeSet<String>> = BTreeMap::new();
    for d in docs {
        if let Some(ref id) = d.id {
            for g in &d.groups {
                by_group.entry(g.clone()).or_default().insert(id.clone());
            }
        }
    }
    let mut sections = Vec::new();
    for (title, ids) in by_group {
        sections.push(serde_json::json!({ "title": title, "selectors": [ { "anyIds": ids.into_iter().collect::<Vec<_>>() } ] }));
    }
    for base in &cfg.bases {
        let out_path = base.join(&cfg.groups_relative);
        if let Some(parent) = out_path.parent() {
            fs::create_dir_all(parent).ok();
        }
        let body = serde_json::json!({ "sections": sections });
        fs::write(&out_path, serde_json::to_string_pretty(&body)?)
            .with_context(|| format!("writing groups to {}", out_path.display()))?;
        eprintln!("Wrote groups: {}", out_path.display());
    }
    Ok(())
}
</file>

<file path="src/lib.rs">
pub mod cli;
pub mod commands;
pub mod config;
pub mod discovery;
pub mod graph;
pub mod index;
pub mod model;
pub mod protocol;
pub mod util;
pub mod validate;
pub mod watch;
</file>

<file path="src/model.rs">
use anyhow::Result;
use regex::Regex;
use serde::Deserialize;
use std::collections::BTreeMap;
use std::fs;
use std::path::{Path, PathBuf};
use std::time::{Duration, SystemTime};

#[derive(Debug, Deserialize, Clone, Default)]
pub struct FrontMatter {
    pub id: Option<String>,
    pub tags: Option<Vec<String>>,
    pub status: Option<String>,
    pub groups: Option<Vec<String>>,
    pub depends_on: Option<Vec<String>>,
    pub supersedes: Option<OneOrMany>,
    pub superseded_by: Option<OneOrMany>,
}

#[derive(Debug, Deserialize, Clone)]
#[serde(untagged)]
pub enum OneOrMany {
    One(String),
    Many(Vec<String>),
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct AdrDoc {
    pub file: PathBuf,
    pub id: Option<String>,
    pub title: String,
    pub tags: Vec<String>,
    pub status: Option<String>,
    pub groups: Vec<String>,
    pub depends_on: Vec<String>,
    pub supersedes: Vec<String>,
    pub superseded_by: Vec<String>,
    pub fm: BTreeMap<String, serde_yaml::Value>,
    pub mtime: Option<u64>,
    pub size: Option<u64>,
}

pub fn parse_front_matter_and_title(content: &str, path: &Path) -> AdrDoc {
    let mut fm: FrontMatter = FrontMatter::default();
    let mut fm_map: BTreeMap<String, serde_yaml::Value> = BTreeMap::new();
    let mut body = content;
    // Normalize line endings for delimiter scanning
    let norm = content.replace("\r\n", "\n");
    if norm.starts_with("---\n") || norm.starts_with("+++\n") {
        let delim = if norm.starts_with("---\n") {
            "---"
        } else {
            "+++"
        };
        let start = 4; // skip delimiter and newline
                       // find closing delimiter on its own line
        let needle = format!("\n{}\n", delim);
        let end_opt = norm[start..].find(&needle).map(|i| start + i);
        let (fm_text, body_start) = if let Some(end) = end_opt {
            (&norm[start..end], end + needle.len())
        } else if norm[start..].ends_with(&format!("\n{}", delim)) {
            let end = norm.len() - (delim.len() + 1);
            (&norm[start..end], end + delim.len() + 1)
        } else {
            ("", start)
        };
        if !fm_text.is_empty() {
            if delim == "---" {
                if let Ok(parsed) = serde_yaml::from_str::<FrontMatter>(fm_text) {
                    fm = parsed;
                }
                if let Ok(mapping) = serde_yaml::from_str::<serde_yaml::Mapping>(fm_text) {
                    for (k, v) in mapping {
                        if let Some(key) = k.as_str() {
                            fm_map.insert(key.to_string(), v);
                        }
                    }
                }
            } else {
                // +++ TOML front matter
                if let Ok(parsed) = toml::from_str::<FrontMatter>(fm_text) {
                    fm = parsed;
                }
                if let Ok(tval) = toml::from_str::<toml::Value>(fm_text) {
                    if let Some(table) = tval.as_table() {
                        for (k, _v) in table.iter() {
                            fm_map.insert(k.clone(), serde_yaml::Value::Null);
                        }
                    }
                }
            }
            // Map body slice back to original content by position if possible
            let offset = body_start.min(norm.len());
            let tail = &norm[offset..];
            body = tail;
        }
    }
    let title_re = Regex::new(r"(?m)^#\s+(.+)$").unwrap();
    let title = title_re
        .captures(body)
        .and_then(|c| c.get(1).map(|m| m.as_str().trim().to_string()))
        .unwrap_or_else(|| {
            path.file_name()
                .unwrap_or_default()
                .to_string_lossy()
                .to_string()
        });

    AdrDoc {
        file: path.to_path_buf(),
        id: fm.id,
        title,
        tags: fm.tags.unwrap_or_default(),
        status: fm.status,
        groups: fm.groups.unwrap_or_default(),
        depends_on: fm.depends_on.unwrap_or_default(),
        supersedes: match fm.supersedes {
            Some(OneOrMany::One(s)) => vec![s],
            Some(OneOrMany::Many(v)) => v,
            None => vec![],
        },
        superseded_by: match fm.superseded_by {
            Some(OneOrMany::One(s)) => vec![s],
            Some(OneOrMany::Many(v)) => v,
            None => vec![],
        },
        fm: fm_map,
        mtime: file_mtime(path).ok(),
        size: file_size(path).ok(),
    }
}

pub fn file_mtime(p: &Path) -> Result<u64> {
    let md = fs::metadata(p)?;
    let m = md.modified()?;
    let d = m
        .duration_since(SystemTime::UNIX_EPOCH)
        .unwrap_or(Duration::from_secs(0));
    Ok(d.as_secs())
}

pub fn file_size(p: &Path) -> Result<u64> {
    let md = fs::metadata(p)?;
    Ok(md.len())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_parse_front_matter_and_title() {
        let md = r#"---
id: ADR-123
tags: [a, b]
status: proposed
groups: ["Tools & Execution"]
depends_on: [ADR-100]
supersedes: ADR-050
---

# ADR-123: Sample

Body here.
"#;
        let path = Path::new("/tmp/ADR-123-sample.md");
        let doc = parse_front_matter_and_title(md, path);
        assert_eq!(doc.id.as_deref(), Some("ADR-123"));
        assert_eq!(doc.title, "ADR-123: Sample");
        assert_eq!(doc.status.as_deref(), Some("proposed"));
        assert_eq!(doc.tags, vec!["a", "b"]);
        assert_eq!(doc.groups, vec!["Tools & Execution".to_string()]);
        assert_eq!(doc.depends_on, vec!["ADR-100".to_string()]);
        assert_eq!(doc.supersedes, vec!["ADR-050".to_string()]);
    }

    #[test]
    fn test_parse_toml_front_matter_and_title() {
        let md = r#"+++
id = "ADR-200"
tags = ["x"]
status = "accepted"
groups = ["G1"]
depends_on = ["ADR-100"]
+++

# ADR-200: TOML Sample

Body here.
"#;
        let path = Path::new("/tmp/ADR-200-sample.md");
        let doc = parse_front_matter_and_title(md, path);
        assert_eq!(doc.id.as_deref(), Some("ADR-200"));
        assert_eq!(doc.title, "ADR-200: TOML Sample");
        assert_eq!(doc.status.as_deref(), Some("accepted"));
        assert_eq!(doc.tags, vec!["x"]);
        assert_eq!(doc.groups, vec!["G1".to_string()]);
        assert_eq!(doc.depends_on, vec!["ADR-100".to_string()]);
    }

    #[test]
    fn test_parse_yaml_crlf_and_no_front_matter() {
        // CRLF front matter + title
        let md_crlf = "---\r\nstatus: proposed\r\n---\r\n\r\n# T\r\n";
        let doc_crlf = parse_front_matter_and_title(md_crlf, Path::new("/t.md"));
        assert_eq!(doc_crlf.status.as_deref(), Some("proposed"));
        assert_eq!(doc_crlf.title, "T");

        // No front matter, title from H1
        let md = "# Hello\nBody";
        let doc = parse_front_matter_and_title(md, Path::new("/hello.md"));
        assert_eq!(doc.id, None);
        assert_eq!(doc.title, "Hello");
    }
}
</file>

<file path="src/validate.rs">
use crate::config::{build_schema_sets, Config, SchemaCfg};
use crate::model::AdrDoc;

#[derive(Debug, Clone)]
pub struct ValidationReport {
    pub ok: bool,
    pub errors: Vec<String>,
    pub warnings: Vec<String>,
    pub doc_count: usize,
    pub id_count: usize,
}

// Validate ADR docs against config: statuses, ids, duplicates/conflicts, references.
pub fn validate_docs(cfg: &Config, docs: &Vec<AdrDoc>) -> ValidationReport {
    use std::collections::{BTreeSet, HashMap};
    let mut errors: Vec<String> = Vec::new();
    let mut warnings: Vec<String> = Vec::new();
    let mut id_to_docs: HashMap<String, Vec<AdrDoc>> = HashMap::new();
    // Pre-compile schema globsets
    let schema_sets: Vec<(SchemaCfg, globset::GlobSet)> = build_schema_sets(cfg);
    let mut doc_schema: HashMap<String, String> = HashMap::new(); // id -> schema name
    for d in docs {
        if let Some(ref id) = d.id {
            id_to_docs.entry(id.clone()).or_default().push(d.clone());
            // assign schema by file name
            let fname = d.file.file_name().and_then(|s| s.to_str()).unwrap_or("");
            for (sc, set) in &schema_sets {
                if set.is_match(fname) {
                    doc_schema.insert(id.clone(), sc.name.clone());
                    break;
                }
            }
        } else {
            errors.push(format!("{}: missing id", d.file.display()));
        }
    }
    // Validate basic status against global list only if no schema rule for status applies
    for d in docs {
        if let Some(ref st) = d.status {
            let mut has_schema_status_rule = false;
            if let Some(ref id) = d.id {
                if let Some(sname) = doc_schema.get(id) {
                    if let Some(sc) = cfg.schema.iter().find(|s| &s.name == sname) {
                        if sc.rules.contains_key("status") {
                            has_schema_status_rule = true;
                        }
                    }
                }
            }
            if !has_schema_status_rule && !cfg.allowed_statuses.iter().any(|s| s == st) {
                errors.push(format!("{}: invalid status '{}'", d.file.display(), st));
            }
        }
    }
    for (id, lst) in &id_to_docs {
        if lst.len() > 1 {
            let mut titles: BTreeSet<String> = BTreeSet::new();
            let mut statuses: BTreeSet<String> = BTreeSet::new();
            for d in lst {
                titles.insert(d.title.clone());
                if let Some(ref s) = d.status {
                    statuses.insert(s.clone());
                }
            }
            let files = lst
                .iter()
                .map(|d| d.file.display().to_string())
                .collect::<Vec<_>>()
                .join(", ");
            if titles.len() > 1 || statuses.len() > 1 {
                errors.push(format!(
                    "conflict for id {} (metadata differ) in: {}",
                    id, files
                ));
            } else {
                errors.push(format!("duplicate id {} in: {}", id, files));
            }
        }
    }
    let id_set: std::collections::BTreeSet<String> = id_to_docs.keys().cloned().collect();
    for d in docs {
        for dep in &d.depends_on {
            if !id_set.contains(dep) {
                errors.push(format!(
                    "{}: depends_on '{}' not found",
                    d.file.display(),
                    dep
                ));
            }
        }
        for s in &d.supersedes {
            if !id_set.contains(s) {
                errors.push(format!(
                    "{}: supersedes '{}' not found",
                    d.file.display(),
                    s
                ));
            }
        }
        for s in &d.superseded_by {
            if !id_set.contains(s) {
                errors.push(format!(
                    "{}: superseded_by '{}' not found",
                    d.file.display(),
                    s
                ));
            }
        }
    }
    // Schema-based validation (required keys, unknown policy, and rules)
    let reserved: BTreeSet<String> = [
        "id",
        "tags",
        "status",
        "groups",
        "depends_on",
        "supersedes",
        "superseded_by",
    ]
    .into_iter()
    .map(|s| s.to_string())
    .collect();
    for d in docs {
        // Skip schema checks if we didn't parse front matter (unchanged in incremental mode)
        if d.fm.is_empty() {
            continue;
        }
        let mut schema_opt: Option<SchemaCfg> = None;
        if let Some(ref id) = d.id {
            if let Some(sname) = doc_schema.get(id) {
                schema_opt = cfg.schema.iter().find(|s| &s.name == sname).cloned();
            }
        } else {
            continue;
        }
        if let Some(sc) = schema_opt {
            // Required keys: present and non-empty
            for key in &sc.required {
                if let Some(v) = d.fm.get(key) {
                    let empty = match v {
                        serde_yaml::Value::Null => true,
                        serde_yaml::Value::String(s) => s.trim().is_empty(),
                        serde_yaml::Value::Sequence(a) => a.is_empty(),
                        _ => false,
                    };
                    if empty {
                        errors.push(format!("{}: required '{}' is empty", d.file.display(), key));
                    }
                } else {
                    errors.push(format!("{}: missing required '{}'", d.file.display(), key));
                }
            }
            // Unknown keys handling
            let present: BTreeSet<String> = d.fm.keys().cloned().collect();
            let rule_keys: BTreeSet<String> = sc.rules.keys().cloned().collect();
            let mut known: BTreeSet<String> = reserved.union(&rule_keys).cloned().collect();
            known = known
                .union(&sc.required.iter().cloned().collect())
                .cloned()
                .collect();
            known = known
                .union(&sc.allowed_keys.iter().cloned().collect())
                .cloned()
                .collect();
            let unknown: Vec<String> = present.difference(&known).cloned().collect();
            let policy = sc.unknown_policy.clone().unwrap_or_else(|| "ignore".into());
            if !unknown.is_empty() {
                match policy.as_str() {
                    "warn" => warnings.push(format!(
                        "{}: unknown keys: {}",
                        d.file.display(),
                        unknown.join(", ")
                    )),
                    "error" => errors.push(format!(
                        "{}: unknown keys: {}",
                        d.file.display(),
                        unknown.join(", ")
                    )),
                    _ => {}
                }
            }
            // Apply rules
            for (k, rule) in sc.rules.iter() {
                let sev_err = rule.severity.as_deref().unwrap_or("error") == "error";
                if let Some(val) = d.fm.get(k) {
                    // type checks
                    if let Some(t) = &rule.r#type {
                        match t.as_str() {
                            "array" => {
                                if !val.is_sequence() {
                                    if sev_err {
                                        errors.push(format!(
                                            "{}: '{}' should be array",
                                            d.file.display(),
                                            k
                                        ));
                                    } else {
                                        warnings.push(format!(
                                            "{}: '{}' should be array",
                                            d.file.display(),
                                            k
                                        ));
                                    }
                                    continue;
                                }
                            }
                            "date" => {
                                if let Some(fmt) = &rule.format {
                                    if let Some(s) = val.as_str() {
                                        if chrono::NaiveDate::parse_from_str(s, fmt).is_err() {
                                            if sev_err {
                                                errors.push(format!(
                                                    "{}: '{}' not a valid date '{}', format {}",
                                                    d.file.display(),
                                                    k,
                                                    s,
                                                    fmt
                                                ));
                                            } else {
                                                warnings.push(format!(
                                                    "{}: '{}' not a valid date",
                                                    d.file.display(),
                                                    k
                                                ));
                                            }
                                        }
                                    }
                                }
                            }
                            _ => {}
                        }
                    }
                    if !rule.allowed.is_empty() {
                        match val {
                            serde_yaml::Value::String(s) => {
                                if !rule.allowed.iter().any(|a| a == s) {
                                    if sev_err {
                                        errors.push(format!(
                                            "{}: '{}' value '{}' not allowed",
                                            d.file.display(),
                                            k,
                                            s
                                        ));
                                    } else {
                                        warnings.push(format!(
                                            "{}: '{}' value '{}' not allowed",
                                            d.file.display(),
                                            k,
                                            s
                                        ));
                                    }
                                }
                            }
                            serde_yaml::Value::Sequence(arr) => {
                                for v in arr {
                                    if let Some(s) = v.as_str() {
                                        if !rule.allowed.iter().any(|a| a == s) {
                                            if sev_err {
                                                errors.push(format!(
                                                    "{}: '{}' contains disallowed '{}'",
                                                    d.file.display(),
                                                    k,
                                                    s
                                                ));
                                            } else {
                                                warnings.push(format!(
                                                    "{}: '{}' contains disallowed '{}'",
                                                    d.file.display(),
                                                    k,
                                                    s
                                                ));
                                            }
                                        }
                                    }
                                }
                            }
                            _ => {}
                        }
                    }
                    if let Some(min) = rule.min_items {
                        if let Some(arr) = val.as_sequence() {
                            if arr.len() < min {
                                if sev_err {
                                    errors.push(format!(
                                        "{}: '{}' must have at least {} items",
                                        d.file.display(),
                                        k,
                                        min
                                    ));
                                } else {
                                    warnings.push(format!(
                                        "{}: '{}' must have at least {} items",
                                        d.file.display(),
                                        k,
                                        min
                                    ));
                                }
                            }
                        }
                    }
                    if let Some(rx) = &rule.regex {
                        if let Ok(re) = regex::Regex::new(rx) {
                            match val {
                                serde_yaml::Value::String(s) => {
                                    if !re.is_match(s) {
                                        if sev_err {
                                            errors.push(format!(
                                                "{}: '{}' does not match regex",
                                                d.file.display(),
                                                k
                                            ));
                                        } else {
                                            warnings.push(format!(
                                                "{}: '{}' does not match regex",
                                                d.file.display(),
                                                k
                                            ));
                                        }
                                    }
                                }
                                serde_yaml::Value::Sequence(arr) => {
                                    for v in arr {
                                        if let Some(s) = v.as_str() {
                                            if !re.is_match(s) {
                                                if sev_err {
                                                    errors.push(format!(
                                                        "{}: '{}' element does not match regex",
                                                        d.file.display(),
                                                        k
                                                    ));
                                                } else {
                                                    warnings.push(format!(
                                                        "{}: '{}' element does not match regex",
                                                        d.file.display(),
                                                        k
                                                    ));
                                                }
                                            }
                                        }
                                    }
                                }
                                _ => {}
                            }
                        }
                    }
                    if let Some(ref_types) = &rule.refers_to_types {
                        // Only applies to arrays of string IDs
                        if let Some(arr) = val.as_sequence() {
                            for v in arr {
                                if let Some(dep_id) = v.as_str() {
                                    if let Some(dep_docs) = id_to_docs.get(dep_id) {
                                        if let Some(dep_doc) = dep_docs.first() {
                                            if let Some(dep_doc_id) = &dep_doc.id {
                                                if let Some(dep_type) = doc_schema.get(dep_doc_id) {
                                                    if !ref_types.iter().any(|t| t == dep_type) {
                                                        if sev_err {
                                                            errors.push(format!("{}: '{}' references {} of type '{}' not in {:?}", d.file.display(), k, dep_id, dep_type, ref_types));
                                                        } else {
                                                            warnings.push(format!("{}: '{}' references '{}' of type '{}' not in {:?}", d.file.display(), k, dep_id, dep_type, ref_types));
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    // Warn on isolated ADRs (no depends_on and no dependents). Valid, but highlighted.
    let mut has_dependent: std::collections::HashMap<String, bool> =
        std::collections::HashMap::new();
    for id in id_to_docs.keys() {
        has_dependent.insert(id.clone(), false);
    }
    for d in docs {
        for dep in &d.depends_on {
            if let Some(x) = has_dependent.get_mut(dep) {
                *x = true;
            }
        }
    }
    for d in docs {
        if let Some(ref id) = d.id {
            let depends = d.depends_on.is_empty();
            let depended = !has_dependent.get(id).copied().unwrap_or(false);
            if depends && depended {
                warnings.push(format!(
                    "{}: '{}' has no graph connections (valid, but isolated)",
                    d.file.display(),
                    id
                ));
            }
        }
    }
    let ok = errors.is_empty();
    ValidationReport {
        ok,
        errors,
        warnings,
        doc_count: docs.len(),
        id_count: id_to_docs.len(),
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::{
        default_allowed_statuses, default_defaults, default_file_patterns, default_groups_rel,
        default_ignore_globs, default_index_rel, Config,
    };
    use std::path::PathBuf;

    #[test]
    fn test_validate_docs_invalid_status_and_refs_and_duplicates() {
        let cfg = Config {
            bases: vec![],
            index_relative: default_index_rel(),
            groups_relative: default_groups_rel(),
            file_patterns: default_file_patterns(),
            ignore_globs: default_ignore_globs(),
            allowed_statuses: default_allowed_statuses(),
            defaults: default_defaults(),
            schema: Vec::new(),
        };
        let d1 = AdrDoc {
            file: PathBuf::from("X.md"),
            id: Some("X".into()),
            title: "X".into(),
            tags: vec![],
            status: Some("weird".into()),
            groups: vec![],
            depends_on: vec!["NOPE".into()],
            supersedes: vec![],
            superseded_by: vec![],
            fm: std::collections::BTreeMap::new(),
            mtime: None,
            size: None,
        };
        let d2 = AdrDoc {
            file: PathBuf::from("A1.md"),
            id: Some("A".into()),
            title: "A v1".into(),
            tags: vec![],
            status: Some("draft".into()),
            groups: vec![],
            depends_on: vec![],
            supersedes: vec![],
            superseded_by: vec![],
            fm: std::collections::BTreeMap::new(),
            mtime: None,
            size: None,
        };
        let d3 = AdrDoc {
            file: PathBuf::from("A2.md"),
            id: Some("A".into()),
            title: "A v2".into(),
            tags: vec![],
            status: Some("draft".into()),
            groups: vec![],
            depends_on: vec![],
            supersedes: vec![],
            superseded_by: vec![],
            fm: std::collections::BTreeMap::new(),
            mtime: None,
            size: None,
        };
        let docs = vec![d1, d2, d3];
        let report = validate_docs(&cfg, &docs);
        assert!(!report.ok);
        let msg = report.errors.join("\n");
        assert!(msg.contains("invalid status"));
        assert!(msg.contains("depends_on 'NOPE' not found"));
        assert!(msg.contains("conflict for id A"));
    }

    #[test]
    fn test_schema_required_unknown_and_refers_to_types() {
        use crate::config::{SchemaCfg, SchemaRule};
        use std::collections::BTreeMap;

        // Build config with two schemas so cross-type ref check can fire
        let mut rules: BTreeMap<String, SchemaRule> = BTreeMap::new();
        rules.insert(
            "depends_on".into(),
            SchemaRule {
                allowed: vec![],
                r#type: Some("array".into()),
                min_items: None,
                regex: None,
                refers_to_types: Some(vec!["ADR".into()]),
                severity: Some("error".into()),
                format: None,
            },
        );
        let sc_adr = SchemaCfg {
            name: "ADR".into(),
            file_patterns: vec!["ADR-*.md".into()],
            required: vec!["id".into(), "tags".into()],
            unknown_policy: Some("warn".into()),
            allowed_keys: vec![],
            rules,
        };
        let sc_imp = SchemaCfg {
            name: "IMP".into(),
            file_patterns: vec!["IMP-*.md".into()],
            required: vec!["id".into()],
            unknown_policy: Some("ignore".into()),
            allowed_keys: vec![],
            rules: BTreeMap::new(),
        };
        let cfg = Config {
            bases: vec![],
            index_relative: default_index_rel(),
            groups_relative: default_groups_rel(),
            file_patterns: default_file_patterns(),
            ignore_globs: default_ignore_globs(),
            allowed_statuses: default_allowed_statuses(),
            defaults: default_defaults(),
            schema: vec![sc_adr, sc_imp],
        };

        // ADR doc with empty required 'tags', unknown key in fm, and depends_on an IMP doc
        let mut fm = BTreeMap::new();
        fm.insert("foo".into(), serde_yaml::Value::String("bar".into()));
        fm.insert(
            "depends_on".into(),
            serde_yaml::Value::Sequence(vec![serde_yaml::Value::String("IMP-002".into())]),
        );
        let d1 = AdrDoc {
            file: PathBuf::from("ADR-001.md"),
            id: Some("ADR-001".into()),
            title: "ADR-001".into(),
            tags: vec![],
            status: Some("draft".into()),
            groups: vec![],
            depends_on: vec!["IMP-002".into()],
            supersedes: vec![],
            superseded_by: vec![],
            fm,
            mtime: None,
            size: None,
        };
        let d2 = AdrDoc {
            file: PathBuf::from("IMP-002.md"),
            id: Some("IMP-002".into()),
            title: "IMP-002".into(),
            tags: vec![],
            status: None,
            groups: vec![],
            depends_on: vec![],
            supersedes: vec![],
            superseded_by: vec![],
            fm: BTreeMap::new(),
            mtime: None,
            size: None,
        };

        let report = validate_docs(&cfg, &vec![d1, d2]);
        assert!(!report.ok);
        let errs = report.errors.join("\n");
        assert!(errs.contains("missing required 'tags'"));
        assert!(errs.contains("references IMP-002"));
        let warns = report.warnings.join("\n");
        assert!(warns.contains("unknown keys: foo"));
    }

    #[test]
    fn test_warn_on_isolated_adrs() {
        let cfg = Config {
            bases: vec![],
            index_relative: default_index_rel(),
            groups_relative: default_groups_rel(),
            file_patterns: default_file_patterns(),
            ignore_globs: default_ignore_globs(),
            allowed_statuses: default_allowed_statuses(),
            defaults: default_defaults(),
            schema: Vec::new(),
        };
        let d = AdrDoc {
            file: PathBuf::from("A.md"),
            id: Some("A".into()),
            title: "A".into(),
            tags: vec![],
            status: None,
            groups: vec![],
            depends_on: vec![],
            supersedes: vec![],
            superseded_by: vec![],
            fm: std::collections::BTreeMap::new(),
            mtime: None,
            size: None,
        };
        let report = validate_docs(&cfg, &vec![d]);
        assert!(report.ok); // warnings allowed
        let warns = report.warnings.join("\n");
        assert!(warns.contains("has no graph connections"));
    }
}
</file>

<file path="src/watch.rs">
use anyhow::Result;
use notify::{Event, RecommendedWatcher, RecursiveMode, Watcher};
use std::time::Duration;

use crate::config::Config;
use crate::discovery::incremental_collect_docs;
use crate::index::{write_groups_config, write_indexes};
use crate::validate::validate_docs;

pub struct WatchArgs {
    pub full_rescan: bool,
    pub debounce_ms: u64,
    pub dry_run: bool,
    pub write_groups: bool,
}

pub fn run_watch(cfg: &Config, args: WatchArgs) -> Result<()> {
    // Initial run
    {
        let docs = incremental_collect_docs(cfg, args.full_rescan)?;
        let report = validate_docs(cfg, &docs);
        if report.ok && !args.dry_run {
            write_indexes(cfg, &docs, true, true)?;
        }
        if !report.errors.is_empty() {
            eprintln!("Validation failed:");
            for e in &report.errors {
                eprintln!(" - {}", e);
            }
        }
        if !report.warnings.is_empty() {
            eprintln!("Warnings:");
            for w in &report.warnings {
                eprintln!(" - {}", w);
            }
        }
    }
    // Set up watchers
    let (tx, rx) = std::sync::mpsc::channel::<notify::Result<Event>>();
    let mut _watchers: Vec<RecommendedWatcher> = Vec::new();
    for base in &cfg.bases {
        let txc = tx.clone();
        let mut w = notify::recommended_watcher(move |res| {
            let _ = txc.send(res);
        })?;
        w.watch(base, RecursiveMode::Recursive)?;
        _watchers.push(w);
    }
    let debounce = Duration::from_millis(args.debounce_ms);
    loop {
        // Wait for an event, then debounce
        let _ = rx.recv();
        // Drain burst
        while rx.try_recv().is_ok() {}
        std::thread::sleep(debounce);
        let docs = incremental_collect_docs(cfg, false)?;
        let report = validate_docs(cfg, &docs);
        if report.ok && !args.dry_run {
            write_indexes(cfg, &docs, true, true)?;
        }
        if args.write_groups && !args.dry_run {
            write_groups_config(cfg, &docs)?;
        }
        if !report.errors.is_empty() {
            eprintln!("Validation failed:");
            for e in &report.errors {
                eprintln!(" - {}", e);
            }
        }
        if !report.warnings.is_empty() {
            eprintln!("Warnings:");
            for w in &report.warnings {
                eprintln!(" - {}", w);
            }
        }
    }
}
</file>

<file path="Cargo.toml">
[package]
name = "adr-rag"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = "1.0"
clap = { version = "4.5", features = ["derive"] }
clap_complete = "4.5"
globwalk = "0.8"
globset = "0.4"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
toml = "0.8"
walkdir = "2.5"
regex = "1.10"
chrono = { version = "0.4", default-features = false, features = ["alloc"] }
notify = "6.1"
rayon = "1.10"

[dev-dependencies]
assert_cmd = "2.0"
assert_fs = "1.1"
predicates = "3.1"
tempfile = "3.10"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
</file>

<file path="README.md">
# cli-rag

A CLI based system for creating and managing Obsidian compliant YAML front matter. This creates a simplifed DAG that allows a local "region" to be called by an LLM.

You've found this way too early! Nothing here is ready for production and all the notes below are LLM written and will be replaced :)

This will all be cleaned up over the next few days. But i would not use this right now.

## Project Layout

- `src/lib.rs`: Library crate exporting modules
  - `config` (TOML config, defaults, schema sets)
  - `model` (front-matter parsing, AdrDoc, file metadata)
  - `discovery` (scan, incremental collection, index loading)
  - `index` (write JSON index and semantic groups)
  - `validate` (schema + rules, returns `ValidationReport`)
  - `graph` (BFS path, dependency cluster)
  - `util` (helpers like `try_open_editor`)
  - `watch` (debounced filesystem watcher orchestration)
  - `cli` (Clap definitions: `Cli`, `Commands`, `ValidateArgs`)
  - `commands/*` (one handler per subcommand)
- `src/bin/adr-rag.rs`: Thin binary that parses CLI and delegates to the library

## Commands
- `init` — Create `.adr-rag.toml` in the current repo and open it in an editor by default. Flags:
  - `--force` overwrite if exists
  - `--print-template` print template to stdout
  - `--silent` do not open the config after creating/detecting it
- `doctor` — Show resolved config, bases, discovery mode (index vs scan), and quick stats.
  - Reports per-type counts when schemas are defined, and unknown-key stats.
  - JSON: use global `--format json`.
- `search --query <substr>` — Fuzzy search by ID/title across discovered ADR files.
- `topics` — List semantic groups derived from front matter (`groups` in ADRs).
- `group --topic "<name>" [--include-content]` — Show ADRs in a group; optionally include full content.
- `get --id ADR-021 [--include-dependents]` — Print an ADR with dependencies (and dependents if requested).
- `cluster --id ADR-021 [--depth N] [--include-bidirectional]` — Traverse dependencies (and dependents) to depth.
- `graph --id ADR-021 [--depth N] [--include-bidirectional] [--format mermaid|dot|json]` — Export a dependency graph around an ADR.
- `path --from ADR-011 --to ADR-038 [--max-depth N]` — Find a dependency path if any.
 - `validate [--format json] [--dry-run] [--full-rescan] [--write-groups]` — Validate front matter/refs; on success writes indexes (unless `--dry-run`).
  - Incremental by default: only reparses changed files using mtime/size. Use `--full-rescan` to force scanning all.
  - Exits non-zero if validation fails.
 - `watch [--debounce-ms 400] [--dry-run] [--full-rescan]` — Watch bases for changes and incrementally validate + update indexes.
  - Debounces rapid events; writes on success (unless `--dry-run`).

## Config: `.adr-rag.toml`
Created by `adr-rag init`. Example:

```
# Repo-local ADR CLI config (adr-rag)

bases = [
  "docs/masterplan",
  # "docs/notes",
]

index_relative = "index/adr-index.json"
groups_relative = "index/semantic-groups.json"

file_patterns = ["ADR-*.md", "ADR-DB-*.md", "IMP-*.md"]
ignore_globs  = ["**/node_modules/**", "**/.obsidian/**"]
allowed_statuses = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]

[defaults]
depth = 2
include_bidirectional = true
include_content = true

## Note Types (Schemas)
- Define `[[schema]]` blocks to validate different note types (e.g., ADR vs IMP).
- Unknown keys policy controls how unexpected front-matter is treated.
- Defaults: `unknown_policy = "ignore"`; required fields must be non-empty.

# Enabled defaults (edit as needed)
[[schema]]
name = "ADR"
file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
required = ["id", "tags", "status", "depends_on"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]
severity = "error"

[[schema]]
name = "IMP"
file_patterns = ["IMP-*.md"]
required = ["id", "tags", "depends_on", "status"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = ["in-progress", "blocked", "on-hold", "cancelled", "done"]
severity = "error"
```

## Build & Run
- From repo root:
  - `cd tools/adr-rag`
  - `cargo build --release`
  - `./target/release/adr-rag init`
  - `./target/release/adr-rag doctor`
  - `./target/release/adr-rag doctor --format json`
  - `./target/release/adr-rag search -q sidecar`
  - `./target/release/adr-rag topics`
  - `./target/release/adr-rag group --topic "Tools & Execution"`
  - `./target/release/adr-rag get --id ADR-021`
  - `./target/release/adr-rag cluster --id ADR-021 --depth 3`
  - `./target/release/adr-rag graph --id ADR-021 --format mermaid`
  - `./target/release/adr-rag graph --id ADR-021 --format dot`
  - `./target/release/adr-rag graph --id ADR-021 --format json`
  - `./target/release/adr-rag path --from ADR-011 --to ADR-038`
  - `./target/release/adr-rag validate --format json --write-groups`

### Global Flags
- `--config <path>`: Explicit config path; otherwise searches upward for `.adr-rag.toml`.
- `--base <path1,path2>`: Override bases from config/env (comma-separated).
- `--format <plain|json>`: Output format for all commands.

Note: The `graph` subcommand uses its own `--format` allowing `mermaid`, `dot`, or `json`.

### Environment Variables
- `ADR_RAG_CONFIG`: Override config path (lower precedence than `--config`).
- `ADR_RAG_BASES`: Comma-separated bases override (lower precedence than `--base`).

Precedence: CLI flags > env vars > nearest `.adr-rag.toml` > tool defaults.

### Shell Completions
- Generate completions: `adr-rag completions bash|zsh|fish`.
- Example (bash): `adr-rag completions bash > ~/.local/share/bash-completion/adr-rag` then `source` it, or add to your shell init.

## Notes
- Multi-base merging is supported; results are de-duplicated by `id` (conflicts will be flagged in future `validate`).
- Front matter groups (e.g., `groups: ["Tools & Execution"]`) drive `topics`/`group`.
- See `docs/masterplan-v2/IMP-004-adr-rag-cli-and-config.md` for design details.

Index and Groups behavior
- Commands load from an index at `<base>/<index_relative>` if present; otherwise they scan markdown.
- `validate` scans markdown and, if there are no blocking errors, writes/upserts per-base JSON indexes by default.
- `validate --dry-run` prints errors/warnings but does not write.
 - Index entries include minimal metadata (`mtime`, `size`) to accelerate incremental runs.
- `validate --write-groups` also writes the semantic groups JSON to `<base>/<groups_relative>`.
- `topics` reads `groups_relative` if present; otherwise derives topics from ADR front matter.

Isolated ADRs
- ADRs with no `depends_on` and no inbound dependents are valid. `validate` will not fail but will emit warnings listing isolated IDs.
</file>

</files>
