This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
src/
  bin/
    adr-rag.rs
  commands/
    cluster.rs
    completions.rs
    doctor.rs
    get.rs
    graph.rs
    group.rs
    init.rs
    mod.rs
    output.rs
    path.rs
    search.rs
    topics.rs
    validate_cmd.rs
    watch_cmd.rs
  cli.rs
  config.rs
  discovery.rs
  graph.rs
  index.rs
  lib.rs
  model.rs
  util.rs
  validate.rs
  watch.rs
.adr-rag.toml
Cargo.toml
DEVLOG.md
README.md
refactor-checklist.md
ROADMAP-CHECKLIST.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/bin/adr-rag.rs">
use anyhow::Result;
use clap::{Parser, CommandFactory};

use adr_rag::cli::{Cli, Commands};
use adr_rag::config::load_config;

fn main() -> Result<()> {
    let cli = Cli::parse();
    match cli.command {
        Commands::Init { path, force, print_template, silent } => {
            adr_rag::commands::init::run(path, force, print_template, silent)?;
        }
        Commands::Doctor {} => {
            let (cfg, cfg_path) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::doctor::run(&cfg, &cfg_path, &cli.format)?;
        }
        Commands::Completions { shell } => {
            let cmd = Cli::command();
            adr_rag::commands::completions::run_completions(cmd, shell);
        }
        Commands::Search { query } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::search::run(&cfg, &cli.format, query)?;
        }
        Commands::Topics {} => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::topics::run(&cfg, &cli.format)?;
        }
        Commands::Group { topic, include_content } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::group::run(&cfg, &cli.format, topic, include_content)?;
        }
        Commands::Get { id, include_dependents } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::get::run(&cfg, &cli.format, id, include_dependents)?;
        }
        Commands::Cluster { id, depth, include_bidirectional } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::cluster::run(&cfg, &cli.format, id, depth, include_bidirectional)?;
        }
        Commands::Path { from, to, max_depth } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::path::run(&cfg, &cli.format, from, to, max_depth)?;
        }
        Commands::Graph { id, depth, include_bidirectional, format } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::graph::run(&cfg, &format, id, depth, include_bidirectional)?;
        }
        Commands::Watch { full_rescan, debounce_ms, dry_run } => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::watch_cmd::run(&cfg, full_rescan, debounce_ms, dry_run)?;
        }
        Commands::Validate(args) => {
            let (cfg, _) = load_config(&cli.config, &cli.base)?;
            adr_rag::commands::validate_cmd::run(&cfg, &args.format, args.write_groups, args.dry_run, args.full_rescan)?;
        }
    }
    Ok(())
}
</file>

<file path="src/commands/cluster.rs">
use anyhow::Result;
use serde_json::Value;
use crate::commands::output::print_json;

use crate::config::Config;
use crate::discovery::load_docs;
use crate::graph::compute_cluster;

pub fn run(cfg: &Config, format: &str, id: String, depth: Option<usize>, include_bidirectional: Option<bool>) -> Result<()> {
    let docs = load_docs(cfg)?;
    let depth = depth.unwrap_or(cfg.defaults.depth);
    let include_bidirectional = include_bidirectional.unwrap_or(cfg.defaults.include_bidirectional);
    let mut by_id = std::collections::HashMap::new();
    for d in &docs { if let Some(ref i) = d.id { by_id.insert(i.clone(), d.clone()); } }
    let cluster = compute_cluster(&id, depth, include_bidirectional, &by_id);
    if format == "json" {
        let arr: Vec<Value> = cluster.iter().map(|(oid, d)| serde_json::json!({
            "id": oid,
            "title": d.title,
            "status": d.status,
        })).collect();
        let out = serde_json::json!({"id": id, "size": cluster.len(), "members": arr});
        print_json(&out)?;
    } else {
        println!("# Dependency Cluster for {}\n", id);
        println!("**Cluster Size**: {}\n", cluster.len());
        println!("## Members");
        for (oid, d) in &cluster { println!("- {}: {}", oid, d.title); }
    }
    Ok(())
}
</file>

<file path="src/commands/completions.rs">
use clap::Command;
use clap_complete::{generate, shells::{Bash, Zsh, Fish}};

pub fn run_completions<S: AsRef<str>>(mut cmd: Command, shell: S) {
    let shell = shell.as_ref();
    match shell {
        "bash" => generate(Bash, &mut cmd, "adr-rag", &mut std::io::stdout()),
        "zsh" => generate(Zsh, &mut cmd, "adr-rag", &mut std::io::stdout()),
        "fish" => generate(Fish, &mut cmd, "adr-rag", &mut std::io::stdout()),
        _ => {
            eprintln!("Unsupported shell: {} (supported: bash|zsh|fish)", shell);
            std::process::exit(2);
        }
    }
}
</file>

<file path="src/commands/doctor.rs">
use anyhow::Result;
use std::collections::{BTreeMap, BTreeSet, HashMap};
use std::path::PathBuf;

use crate::commands::output::print_json;
use crate::config::{Config, SchemaCfg};
use crate::discovery::load_docs;

pub(crate) fn build_report(
    cfg: &Config,
    cfg_path: &Option<PathBuf>,
    docs: &Vec<crate::model::AdrDoc>,
) -> serde_json::Value {
    let mut id_to_docs: HashMap<String, Vec<&crate::model::AdrDoc>> = HashMap::new();
    for d in docs {
        if let Some(ref id) = d.id {
            id_to_docs.entry(id.clone()).or_default().push(d);
        }
    }
    let mut conflicts = Vec::new();
    for (id, lst) in &id_to_docs {
        if lst.len() > 1 {
            let mut titles: BTreeSet<String> = BTreeSet::new();
            let mut statuses: BTreeSet<String> = BTreeSet::new();
            for d in lst.iter() {
                let doc = *d;
                titles.insert(doc.title.clone());
                if let Some(ref s) = doc.status {
                    statuses.insert(s.clone());
                }
            }
            if titles.len() > 1 || statuses.len() > 1 {
                conflicts.push(id.clone());
            }
        }
    }
    let group_count: usize = docs.iter().flat_map(|d| d.groups.iter()).count();
    // Per-type counts and unknown-key stats
    let mut schema_sets: Vec<(SchemaCfg, globset::GlobSet)> = Vec::new();
    for sc in &cfg.schema {
        let mut b = globset::GlobSetBuilder::new();
        for p in &sc.file_patterns {
            if let Ok(g) = globset::Glob::new(p) {
                b.add(g);
            }
        }
        if let Ok(set) = b.build() {
            schema_sets.push((sc.clone(), set));
        }
    }
    let mut type_counts: BTreeMap<String, usize> = BTreeMap::new();
    let mut unknown_stats: BTreeMap<String, (usize, usize)> = BTreeMap::new(); // schema -> (docs_with_unknowns, total_unknown_keys)
    let reserved: BTreeSet<String> = [
        "id",
        "tags",
        "status",
        "groups",
        "depends_on",
        "supersedes",
        "superseded_by",
    ]
    .into_iter()
    .map(|s| s.to_string())
    .collect();
    for d in docs {
        let fname = d.file.file_name().and_then(|s| s.to_str()).unwrap_or("");
        let mut sname: Option<String> = None;
        for (sc, set) in &schema_sets {
            if set.is_match(fname) {
                sname = Some(sc.name.clone());
                break;
            }
        }
        if let Some(sname) = sname {
            *type_counts.entry(sname.clone()).or_insert(0) += 1;
            let present: BTreeSet<String> = d.fm.keys().cloned().collect();
            let rule_keys: BTreeSet<String> = cfg
                .schema
                .iter()
                .find(|s| s.name == sname)
                .map(|sc| sc.rules.keys().cloned().collect())
                .unwrap_or_default();
            let mut known: BTreeSet<String> = reserved.union(&rule_keys).cloned().collect();
            let req: BTreeSet<String> = cfg
                .schema
                .iter()
                .find(|s| s.name == sname)
                .map(|sc| sc.required.iter().cloned().collect())
                .unwrap_or_default();
            known = known.union(&req).cloned().collect();
            let allow: BTreeSet<String> = cfg
                .schema
                .iter()
                .find(|s| s.name == sname)
                .map(|sc| sc.allowed_keys.iter().cloned().collect())
                .unwrap_or_default();
            known = known.union(&allow).cloned().collect();
            let unknown: Vec<String> = present.difference(&known).cloned().collect();
            if !unknown.is_empty() {
                let e = unknown_stats.entry(sname).or_insert((0, 0));
                e.0 += 1;
                e.1 += unknown.len();
            }
        }
    }

    let per_base: Vec<serde_json::Value> = cfg
        .bases
        .iter()
        .map(|b| {
            let idx = b.join(&cfg.index_relative);
            let mode = if idx.exists() { "index" } else { "scan" };
            serde_json::json!({
                "base": b,
                "mode": mode
            })
        })
        .collect();

    serde_json::json!({
        "config": cfg_path.as_ref().map(|p| p.display().to_string()).unwrap_or("<defaults>".into()),
        "bases": cfg.bases,
        "per_base": per_base,
        "counts": {"docs": docs.len(), "group_entries": group_count},
        "conflicts": conflicts,
        "types": type_counts,
        "unknown_stats": unknown_stats,
    })
}

pub fn run(cfg: &Config, cfg_path: &Option<PathBuf>, format: &str) -> Result<()> {
    let docs = load_docs(cfg)?;
    if format == "json" {
        let report = build_report(cfg, cfg_path, &docs);
        print_json(&report)?;
        return Ok(());
    }
    // Plain text output
    let report = build_report(cfg, cfg_path, &docs);
    let config_path = report.get("config").and_then(|v| v.as_str()).unwrap_or("");
    println!("Config: {}", config_path);
    println!("Bases:");
    for b in &cfg.bases { println!("  - {}", b.display()); }
    println!("index_relative: {}", cfg.index_relative);
    println!("groups_relative: {}", cfg.groups_relative);
    for item in report.get("per_base").and_then(|v| v.as_array()).unwrap_or(&Vec::new()) {
        let base = item.get("base").and_then(|v| v.as_str()).unwrap_or("");
        let mode = item.get("mode").and_then(|v| v.as_str()).unwrap_or("");
        println!("Base {} → {}", base, mode);
    }
    let counts = report.get("counts").and_then(|v| v.as_object()).cloned().unwrap_or_default();
    let docs_count = counts.get("docs").and_then(|v| v.as_u64()).unwrap_or(0);
    let group_entries = counts.get("group_entries").and_then(|v| v.as_u64()).unwrap_or(0);
    println!("Found {} ADR-like files; group entries: {}", docs_count, group_entries);
    if let Some(arr) = report.get("conflicts").and_then(|v| v.as_array()) { if !arr.is_empty() { let list: Vec<String> = arr.iter().filter_map(|v| v.as_str().map(|s| s.to_string())).collect(); println!("Conflicts (ids with differing title/status): {}", list.join(", ")); }}
    if let Some(types) = report.get("types").and_then(|v| v.as_object()) { if !types.is_empty() {
        println!("Types:");
        for (k,v) in types { println!("  - {}: {} notes", k, v.as_u64().unwrap_or(0)); }
    }}
    if let Some(unknown) = report.get("unknown_stats").and_then(|v| v.as_object()) { if !unknown.is_empty() {
        println!("Unknown key stats:");
        for (k,v) in unknown { if let Some(arr) = v.as_array() { if arr.len() == 2 { let docs = arr[0].as_u64().unwrap_or(0); let total = arr[1].as_u64().unwrap_or(0); println!("  - {}: {} notes with unknowns ({} keys)", k, docs, total); } } }
    }}
    Ok(())
}
</file>

<file path="src/commands/get.rs">
use anyhow::{Result, anyhow};
use std::fs;

use crate::config::Config;
use crate::commands::output::print_json;
use crate::discovery::load_docs;

pub fn run(cfg: &Config, format: &str, id: String, include_dependents: bool) -> Result<()> {
    let docs = load_docs(cfg)?;
    let mut by_id = std::collections::HashMap::new();
    for d in &docs { if let Some(ref i) = d.id { by_id.insert(i.clone(), d.clone()); } }
    let primary = by_id.get(&id).ok_or_else(|| anyhow!("ADR not found: {}", id))?;
    let deps: Vec<crate::model::AdrDoc> = primary.depends_on.iter().filter_map(|dep| by_id.get(dep).cloned()).collect();
    let mut dependents = Vec::new();
    if include_dependents {
        for d in &docs { if d.depends_on.iter().any(|dep| dep == &id) { dependents.push(d.clone()); } }
    }
    if format == "json" {
        let out = serde_json::json!({
            "id": id,
            "title": primary.title,
            "file": primary.file,
            "tags": primary.tags,
            "status": primary.status,
            "depends_on": deps.iter().filter_map(|d| d.id.clone()).collect::<Vec<_>>(),
            "dependents": dependents.iter().filter_map(|d| d.id.clone()).collect::<Vec<_>>(),
            "content": fs::read_to_string(&primary.file).unwrap_or_default(),
        });
        print_json(&out)?;
    } else {
        println!("# {}: {}\n", id, primary.title);
        if !primary.depends_on.is_empty() {
            println!("## Depends On");
            for d in &deps { println!("- {}: {}", d.id.clone().unwrap_or_default(), d.title); }
            println!("");
        }
        if include_dependents && !dependents.is_empty() {
            println!("## Dependents ({})", dependents.len());
            for d in &dependents { println!("- {}: {}", d.id.clone().unwrap_or_default(), d.title); }
            println!("");
        }
        let content = fs::read_to_string(&primary.file).unwrap_or_default();
        println!("## Content\n\n{}", content);
    }
    Ok(())
}
</file>

<file path="src/commands/graph.rs">
use anyhow::{Result, anyhow};
use std::collections::{BTreeMap, BTreeSet, HashMap};

use crate::commands::output::print_json;
use crate::config::Config;
use crate::discovery::load_docs;
use crate::graph::compute_cluster;
use crate::model::AdrDoc;

#[derive(Debug, Clone)]
struct Edge { from: String, to: String }

fn sanitize_id(id: &str) -> String {
    id.chars()
        .map(|c| if c.is_ascii_alphanumeric() { c } else { '_' })
        .collect()
}

fn cluster_edges(cluster: &BTreeMap<String, AdrDoc>) -> Vec<Edge> {
    let members: BTreeSet<String> = cluster.keys().cloned().collect();
    let mut edges = Vec::new();
    for (id, doc) in cluster.iter() {
        for dep in &doc.depends_on {
            if members.contains(dep) {
                edges.push(Edge { from: id.clone(), to: dep.clone() });
            }
        }
    }
    edges
}

pub(crate) fn render_mermaid(cluster: &BTreeMap<String, AdrDoc>) -> String {
    let mut out = String::from("flowchart LR\n");
    // Node declarations
    for (id, doc) in cluster.iter() {
        let var = sanitize_id(id);
        let label = format!("{}: {}", id, doc.title.replace('"', "\\\""));
        out.push_str(&format!("  {}[\"{}\"]\n", var, label));
    }
    // Edges
    for e in cluster_edges(cluster) {
        let from = sanitize_id(&e.from);
        let to = sanitize_id(&e.to);
        out.push_str(&format!("  {} --> {}\n", from, to));
    }
    out
}

pub(crate) fn render_dot(cluster: &BTreeMap<String, AdrDoc>) -> String {
    let mut out = String::from("digraph {\n");
    // Nodes
    for (id, doc) in cluster.iter() {
        let label = format!("{}: {}", id, doc.title.replace('"', "\\\""));
        out.push_str(&format!("  \"{}\" [label=\"{}\"];\n", id, label));
    }
    // Edges
    for e in cluster_edges(cluster) {
        out.push_str(&format!("  \"{}\" -> \"{}\";\n", e.from, e.to));
    }
    out.push_str("}\n");
    out
}

pub fn run(cfg: &Config, format: &str, id: String, depth: Option<usize>, include_bidirectional: Option<bool>) -> Result<()> {
    let docs = load_docs(cfg)?;
    let depth = depth.unwrap_or(cfg.defaults.depth);
    let include_bidirectional = include_bidirectional.unwrap_or(cfg.defaults.include_bidirectional);
    let mut by_id: HashMap<String, AdrDoc> = HashMap::new();
    for d in &docs { if let Some(ref i) = d.id { by_id.insert(i.clone(), d.clone()); } }
    if !by_id.contains_key(&id) { return Err(anyhow!("ADR not found: {}", id)); }
    let cluster = compute_cluster(&id, depth, include_bidirectional, &by_id);
    match format {
        "json" => {
            let members: Vec<serde_json::Value> = cluster.iter().map(|(oid, d)| serde_json::json!({
                "id": oid,
                "title": d.title,
                "status": d.status,
            })).collect();
            let edges: Vec<serde_json::Value> = cluster_edges(&cluster).into_iter().map(|e| serde_json::json!({"from": e.from, "to": e.to})).collect();
            let out = serde_json::json!({
                "root": id,
                "members": members,
                "edges": edges,
                "depth": depth,
                "bidirectional": include_bidirectional,
            });
            print_json(&out)?;
        }
        "dot" => {
            let s = render_dot(&cluster);
            println!("{}", s);
        }
        _ => { // mermaid default
            let s = render_mermaid(&cluster);
            println!("{}", s);
        }
    }
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    fn doc(id: &str, title: &str, deps: Vec<&str>) -> AdrDoc {
        AdrDoc {
            file: PathBuf::from(format!("{}.md", id)),
            id: Some(id.to_string()),
            title: title.to_string(),
            tags: vec![],
            status: None,
            groups: vec![],
            depends_on: deps.into_iter().map(|s| s.to_string()).collect(),
            supersedes: vec![],
            superseded_by: vec![],
            fm: BTreeMap::new(),
            mtime: None,
            size: None,
        }
    }

    #[test]
    fn test_render_mermaid_and_dot() {
        let mut cluster: BTreeMap<String, AdrDoc> = BTreeMap::new();
        cluster.insert("ADR-001".into(), doc("ADR-001", "Root", vec!["ADR-002"])) ;
        cluster.insert("ADR-002".into(), doc("ADR-002", "Child", vec![]));
        let mm = render_mermaid(&cluster);
        assert!(mm.contains("flowchart LR"));
        assert!(mm.contains("ADR_001 --> ADR_002"));
        let dot = render_dot(&cluster);
        assert!(dot.contains("digraph"));
        assert!(dot.contains("\"ADR-001\" -> \"ADR-002\""));
    }
}
</file>

<file path="src/commands/group.rs">
use anyhow::Result;
use serde_json::Value;
use crate::commands::output::print_json;
use std::fs;

use crate::config::Config;
use crate::discovery::load_docs;

pub fn run(cfg: &Config, format: &str, topic: String, include_content: Option<bool>) -> Result<()> {
    let t = topic.to_lowercase();
    let docs = load_docs(cfg)?;
    let mut matches: Vec<crate::model::AdrDoc> = docs.into_iter().filter(|d| d.groups.iter().any(|g| g.to_lowercase().contains(&t))).collect();
    matches.sort_by(|a,b| a.title.to_lowercase().cmp(&b.title.to_lowercase()));
    let include_content = include_content.unwrap_or(cfg.defaults.include_content);
    if format == "json" {
        let arr: Vec<Value> = matches.iter().map(|d| serde_json::json!({
            "id": d.id,
            "title": d.title,
            "file": d.file,
            "tags": d.tags,
            "status": d.status,
        })).collect();
        let mut out = serde_json::json!({"topic": topic, "count": arr.len(), "adrs": arr});
        if include_content {
            if let Some(obj) = out.as_object_mut() {
                let contents: Vec<String> = matches.iter().map(|d| fs::read_to_string(&d.file).unwrap_or_default()).collect();
                obj.insert("contents".into(), serde_json::json!(contents));
            }
        }
        print_json(&out)?;
    } else {
        println!("# Semantic Group: {}\n", topic);
        println!("**ADR Count**: {}\n", matches.len());
        println!("## ADRs in this group");
        for d in &matches { println!("- {}: {}", d.id.clone().unwrap_or_default(), d.title); }
        if include_content {
            println!("\n## Content\n");
            for d in &matches { let content = fs::read_to_string(&d.file).unwrap_or_default(); println!("### {}: {}\n\n{}\n", d.id.clone().unwrap_or_default(), d.title, content); }
        }
    }
    Ok(())
}
</file>

<file path="src/commands/init.rs">
use anyhow::Result;
use std::path::PathBuf;

use crate::config::{find_config_upwards, write_template, TEMPLATE};
use crate::util::try_open_editor;

pub fn run(path: Option<PathBuf>, force: bool, print_template: bool, silent: bool) -> Result<()> {
    let target = path.unwrap_or_else(|| PathBuf::from(".adr-rag.toml"));
    if print_template { print!("{}", TEMPLATE); return Ok(()); }
    let existed = target.exists();
    if existed && !force {
        eprintln!("Config exists: {} (not overwriting; use --force to rewrite)", target.display());
    }
    if let Some(parent_cfg) = find_config_upwards(&None) {
        if let Some(cur) = std::env::current_dir().ok() {
            if parent_cfg != target && parent_cfg.parent() != Some(cur.as_path()) {
                eprintln!("Warning: a parent config exists at {} and may be shadowed by creating one here", parent_cfg.display());
            }
        }
    }
    if !existed || force {
        write_template(&target)?;
        println!("Wrote {}", target.display());
    }
    if !silent {
        if let Err(e) = try_open_editor(&target) { eprintln!("Note: could not open editor automatically: {}", e); }
    }
    Ok(())
}
</file>

<file path="src/commands/mod.rs">
pub mod init;
pub mod doctor;
pub mod search;
pub mod topics;
pub mod group;
pub mod get;
pub mod cluster;
pub mod path;
pub mod validate_cmd;
pub mod watch_cmd;
pub mod completions;
pub mod output;
pub mod graph;
</file>

<file path="src/commands/output.rs">
use anyhow::Result;

pub fn print_json<T: serde::Serialize>(value: &T) -> Result<()> {
    println!("{}", serde_json::to_string_pretty(value)?);
    Ok(())
}

pub fn print_ndjson_value(value: &serde_json::Value) -> Result<()> {
    println!("{}", serde_json::to_string(value)?);
    Ok(())
}

pub fn print_ndjson_iter<T, I>(iter: I) -> Result<()>
where
    T: serde::Serialize,
    I: IntoIterator<Item = T>,
{
    for item in iter {
        println!("{}", serde_json::to_string(&item)?);
    }
    Ok(())
}
</file>

<file path="src/commands/path.rs">
use anyhow::Result;

use crate::config::Config;
use crate::discovery::load_docs;
use crate::graph::bfs_path;
use crate::commands::output::print_json;

pub fn run(cfg: &Config, format: &str, from: String, to: String, max_depth: usize) -> Result<()> {
    let docs = load_docs(cfg)?;
    let mut by_id = std::collections::HashMap::new();
    for d in &docs { if let Some(ref i) = d.id { by_id.insert(i.clone(), d.clone()); } }
    let res = bfs_path(&from, &to, max_depth, &by_id);
    if format == "json" {
        let out = serde_json::json!({"from": from, "to": to, "path": res});
        print_json(&out)?;
    } else {
        println!("# Dependency Path: {} → {}\n", from, to);
        match res {
            Some(path) => {
                println!("**Path Length**: {} steps\n", path.len().saturating_sub(1));
                println!("## Path");
                for (i, node) in path.iter().enumerate() { println!("{}. {}", i+1, node); }
            }
            None => println!("No path found between {} and {}", from, to),
        }
    }
    Ok(())
}
</file>

<file path="src/commands/search.rs">
use anyhow::Result;
use serde_json::Value;
use crate::commands::output::{print_json, print_ndjson_iter};

use crate::config::Config;
use crate::discovery::load_docs;

pub fn run(cfg: &Config, format: &str, query: String) -> Result<()> {
    let q = query.to_lowercase();
    let docs = load_docs(cfg)?;
    let mut hits: Vec<&crate::model::AdrDoc> = Vec::new();
    for d in &docs { let id = d.id.clone().unwrap_or_default(); if id.to_lowercase().contains(&q) || d.title.to_lowercase().contains(&q) { hits.push(d); } }
    if format == "json" {
        let arr: Vec<Value> = hits.iter().map(|d| serde_json::json!({
            "id": d.id,
            "title": d.title,
            "file": d.file,
            "tags": d.tags,
            "status": d.status,
        })).collect();
        print_json(&arr)?;
    } else if format == "ndjson" {
        let it = hits.into_iter().map(|d| serde_json::json!({
            "id": d.id,
            "title": d.title,
            "file": d.file,
            "tags": d.tags,
            "status": d.status,
        }));
        print_ndjson_iter::<serde_json::Value, _>(it)?;
    } else {
        for d in hits { println!("{}\t{}\t{}", d.id.clone().unwrap_or_default(), d.title, d.file.display()); }
    }
    Ok(())
}
</file>

<file path="src/commands/topics.rs">
use anyhow::{Result, Context};
use serde_json::Value;
use crate::commands::output::{print_json, print_ndjson_iter};

use crate::config::Config;
use crate::discovery::load_docs;
use std::fs;

pub fn run(cfg: &Config, format: &str) -> Result<()> {
    use std::collections::BTreeMap;
    let mut groups: BTreeMap<String, usize> = BTreeMap::new();
    let mut used_groups_file = false;
    'outer: for b in &cfg.bases {
        let path = b.join(&cfg.groups_relative);
        if path.exists() {
            used_groups_file = true;
            let s = fs::read_to_string(&path).with_context(|| format!("reading groups {:?}", path))?;
            let v: Value = serde_json::from_str(&s).with_context(|| format!("parsing groups {:?}", path))?;
            if let Some(sections) = v.get("sections").and_then(|x| x.as_array()) {
                for sec in sections {
                    let title = sec.get("title").and_then(|x| x.as_str()).unwrap_or("");
                    let mut count = 0usize;
                    if let Some(sels) = sec.get("selectors").and_then(|x| x.as_array()) {
                        for sel in sels { if let Some(ids) = sel.get("anyIds").and_then(|x| x.as_array()) { count += ids.len(); } }
                    }
                    *groups.entry(title.to_string()).or_insert(0) += count;
                }
            }
            break 'outer;
        }
    }
    if !used_groups_file {
        let docs = load_docs(cfg)?;
        for d in docs { for g in d.groups { *groups.entry(g).or_insert(0) += 1; } }
    }
    if format == "json" {
        let arr: Vec<Value> = groups.into_iter().map(|(k,v)| serde_json::json!({"topic": k, "count": v})).collect();
        print_json(&arr)?;
    } else if format == "ndjson" {
        let it = groups.into_iter().map(|(k,v)| serde_json::json!({"topic": k, "count": v}));
        print_ndjson_iter::<serde_json::Value, _>(it)?;
    } else {
        if groups.is_empty() { println!("No semantic groups found"); return Ok(()); }
        println!("# Available Semantic Topics\n");
        for (name, count) in groups { println!("- {}: {} ADRs", name, count); }
    }
    Ok(())
}
</file>

<file path="src/commands/validate_cmd.rs">
use anyhow::Result;

use crate::config::Config;
use crate::discovery::incremental_collect_docs;
use crate::index::{write_indexes, write_groups_config};
use crate::commands::output::{print_json, print_ndjson_value};
use crate::validate::validate_docs;

pub fn run(cfg: &Config, format: &str, write_groups: bool, dry_run: bool, full_rescan: bool) -> Result<()> {
    let docs = incremental_collect_docs(cfg, full_rescan)?;
    let report = validate_docs(cfg, &docs);
    if format == "json" {
        let obj = serde_json::json!({ "ok": report.ok, "errors": report.errors, "warnings": report.warnings });
        print_json(&obj)?;
    } else if format == "ndjson" {
        // Emit a header then each error and warning as individual records
        let header = serde_json::json!({ "ok": report.ok, "doc_count": docs.len() });
        print_ndjson_value(&header)?;
        for e in &report.errors { print_ndjson_value(&serde_json::json!({"type":"error","message": e}))?; }
        for w in &report.warnings { print_ndjson_value(&serde_json::json!({"type":"warning","message": w}))?; }
    } else {
        if report.ok { println!("Validation OK ({} docs)", docs.len()); } else {
            eprintln!("Validation failed:");
            for e in &report.errors { eprintln!(" - {}", e); }
        }
        if !report.warnings.is_empty() {
            eprintln!("Warnings:");
            for w in &report.warnings { eprintln!(" - {}", w); }
        }
    }
    if report.ok && !dry_run { write_indexes(cfg, &docs, true, true)?; }
    if write_groups && !dry_run { write_groups_config(cfg, &docs)?; }
    if !report.ok { std::process::exit(1); }
    Ok(())
}
</file>

<file path="src/commands/watch_cmd.rs">
use anyhow::Result;

use crate::config::Config;
use crate::watch::{run_watch, WatchArgs};

pub fn run(cfg: &Config, full_rescan: bool, debounce_ms: u64, dry_run: bool) -> Result<()> {
    run_watch(cfg, WatchArgs { full_rescan, debounce_ms, dry_run, write_groups: false })
}
</file>

<file path="src/cli.rs">
use clap::{Parser, Subcommand, Args};
use std::path::PathBuf;

#[derive(Parser, Debug)]
#[command(name = "adr-rag", version, about = "Per-repo ADR navigator with TOML config")]
pub struct Cli {
    #[arg(long, global = true)]
    pub config: Option<PathBuf>,

    #[arg(long, value_delimiter = ',', global = true)]
    pub base: Option<Vec<PathBuf>>,

    /// Global output format: plain | json
    #[arg(long, global = true, default_value = "plain")]
    pub format: String,

    #[command(subcommand)]
    pub command: Commands,
}

#[derive(Subcommand, Debug)]
pub enum Commands {
    Init {
        /// Optional path to write config (defaults to ./.adr-rag.toml)
        #[arg(long)]
        path: Option<PathBuf>,
        #[arg(long)]
        force: bool,
        #[arg(long)]
        print_template: bool,
        /// Do not open the config in an editor after creating or detecting it
        #[arg(long, default_value_t = false)]
        silent: bool,
    },
    Doctor {},
    Search {
        #[arg(long, short = 'q')]
        query: String,
    },
    Topics {},
    Group {
        #[arg(long)]
        topic: String,
        #[arg(long)]
        include_content: Option<bool>,
    },
    Get {
        #[arg(long)]
        id: String,
        #[arg(long, default_value_t = false)]
        include_dependents: bool,
    },
    Cluster {
        #[arg(long)]
        id: String,
        #[arg(long)]
        depth: Option<usize>,
        #[arg(long)]
        include_bidirectional: Option<bool>,
    },
    Path {
        #[arg(long, value_name = "FROM")] 
        from: String,
        #[arg(long, value_name = "TO")] 
        to: String,
        #[arg(long, default_value_t = 5)]
        max_depth: usize,
    },
    /// Export a dependency graph (mermaid|dot|json)
    Graph {
        #[arg(long)]
        id: String,
        #[arg(long)]
        depth: Option<usize>,
        #[arg(long)]
        include_bidirectional: Option<bool>,
        /// Output format: mermaid | dot | json
        #[arg(long, default_value = "mermaid")]
        format: String,
    },
    Validate(ValidateArgs),

    /// Watch bases and incrementally validate + update indexes on changes
    Watch {
        /// Force full rescan on first run
        #[arg(long, default_value_t = false)]
        full_rescan: bool,
        /// Debounce milliseconds for coalescing FS events
        #[arg(long, default_value_t = 400)]
        debounce_ms: u64,
        /// Print only; do not write indexes or groups
        #[arg(long, default_value_t = false)]
        dry_run: bool,
    },

    /// Generate shell completions (bash|zsh|fish)
    Completions {
        #[arg(value_name = "SHELL")] 
        shell: String,
    }
}

#[derive(Args, Debug)]
pub struct ValidateArgs {
    #[arg(long, default_value = "plain")]
    pub format: String,
    #[arg(long, default_value_t = false)]
    pub write_groups: bool,
    /// Do not write index/groups; print results only
    #[arg(long, default_value_t = false)]
    pub dry_run: bool,
    /// Force full rescan instead of incremental
    #[arg(long, default_value_t = false)]
    pub full_rescan: bool,
}
</file>

<file path="src/config.rs">
use anyhow::{Context, Result};
use globset::Glob;
use globset::GlobSetBuilder;
use serde::Deserialize;
use std::env;
use std::fs;
use std::io::Write;
use std::path::{Path, PathBuf};

#[derive(Debug, Deserialize, Clone)]
pub struct DefaultsCfg {
    #[serde(default = "default_depth")] 
    pub depth: usize,
    #[serde(default = "default_true")] 
    pub include_bidirectional: bool,
    #[serde(default = "default_true")] 
    pub include_content: bool,
}


#[derive(Debug, Deserialize, Clone)]
pub struct SchemaRule {
    #[serde(default)] pub allowed: Vec<String>,
    #[serde(rename = "type")] pub r#type: Option<String>,
    #[serde(default)] pub min_items: Option<usize>,
    #[serde(default)] pub regex: Option<String>,
    #[serde(default)] pub refers_to_types: Option<Vec<String>>,
    #[serde(default)] pub severity: Option<String>, // error | warn
    #[serde(default)] pub format: Option<String>,   // for date parsing
}

#[derive(Debug, Deserialize, Clone)]
pub struct SchemaCfg {
    pub name: String,
    pub file_patterns: Vec<String>,
    #[serde(default)] pub required: Vec<String>,
    #[serde(default)] pub unknown_policy: Option<String>, // ignore | warn | error (default ignore)
    #[serde(default)] pub allowed_keys: Vec<String>,
    #[serde(default)] pub rules: std::collections::BTreeMap<String, SchemaRule>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Config {
    #[serde(default = "default_bases")] 
    pub bases: Vec<PathBuf>,
    #[serde(default = "default_index_rel")] 
    pub index_relative: String,
    #[serde(default = "default_groups_rel")] 
    pub groups_relative: String,
    #[serde(default = "default_file_patterns")] 
    pub file_patterns: Vec<String>,
    #[serde(default = "default_ignore_globs")] 
    pub ignore_globs: Vec<String>,
    #[serde(default = "default_allowed_statuses")] 
    pub allowed_statuses: Vec<String>,
    #[serde(default = "default_defaults")] 
    pub defaults: DefaultsCfg,
    // output config removed (was unused)
    #[serde(default)]
    pub schema: Vec<SchemaCfg>,
}

pub fn default_bases() -> Vec<PathBuf> { vec![PathBuf::from("docs/masterplan-v2")] }
pub fn default_index_rel() -> String { "index/adr-index.json".to_string() }
pub fn default_groups_rel() -> String { "index/semantic-groups.json".to_string() }
pub fn default_file_patterns() -> Vec<String> { vec!["ADR-*.md".into(), "ADR-DB-*.md".into(), "IMP-*.md".into()] }
pub fn default_ignore_globs() -> Vec<String> { vec!["**/node_modules/**".into(), "**/.obsidian/**".into()] }
pub fn default_allowed_statuses() -> Vec<String> { vec!["draft".into(),"incomplete".into(),"proposed".into(),"accepted".into(),"complete".into(),"design".into(),"legacy-reference".into(),"superseded".into()] }
pub fn default_depth() -> usize { 2 }
pub fn default_true() -> bool { true }
pub fn default_defaults() -> DefaultsCfg { DefaultsCfg { depth: default_depth(), include_bidirectional: true, include_content: true } }

pub fn find_config_upwards(explicit: &Option<PathBuf>) -> Option<PathBuf> {
    if let Some(p) = explicit { return Some(p.clone()); }
    if let Ok(env_path) = env::var("ADR_RAG_CONFIG") { let p = PathBuf::from(env_path); if p.exists() { return Some(p); } }
    let mut dir = env::current_dir().ok()?;
    loop {
        let candidate = dir.join(".adr-rag.toml");
        if candidate.exists() { return Some(candidate); }
        let parent = dir.parent();
        match parent { Some(p) => dir = p.to_path_buf(), None => return None }
    }
}

pub fn load_config(path_opt: &Option<PathBuf>, base_override: &Option<Vec<PathBuf>>) -> Result<(Config, Option<PathBuf>)> {
    let path = find_config_upwards(path_opt);
    let mut cfg: Config = if let Some(ref p) = path {
        let s = fs::read_to_string(p).with_context(|| format!("reading config {:?}", p))?;
        toml::from_str(&s).with_context(|| format!("parsing TOML config {:?}", p))?
    } else {
        Config { bases: default_bases(), index_relative: default_index_rel(), groups_relative: default_groups_rel(), file_patterns: default_file_patterns(), ignore_globs: default_ignore_globs(), allowed_statuses: default_allowed_statuses(), defaults: default_defaults(), schema: Vec::new() }
    };
    // Env override for bases (comma-separated)
    if let Ok(env_bases) = env::var("ADR_RAG_BASES") { 
        let list: Vec<PathBuf> = env_bases.split(',').map(|s| PathBuf::from(s.trim())).filter(|p| !p.as_os_str().is_empty()).collect();
        if !list.is_empty() { cfg.bases = list; }
    }
    if let Some(override_bases) = base_override { if !override_bases.is_empty() { cfg.bases = override_bases.clone(); } }
    Ok((cfg, path))
}

pub fn write_template(path: &Path) -> Result<()> {
    if let Some(parent) = path.parent() { fs::create_dir_all(parent).ok(); }
    let mut f = fs::File::create(path).with_context(|| format!("creating {:?}", path))?;
    f.write_all(TEMPLATE.as_bytes())?;
    Ok(())
}

// Helper: compile schema globsets once for reuse across modules.
pub fn build_schema_sets(cfg: &Config) -> Vec<(SchemaCfg, globset::GlobSet)> {
    let mut out = Vec::new();
    for sc in &cfg.schema {
        let mut b = GlobSetBuilder::new();
        for p in &sc.file_patterns {
            if let Ok(g) = Glob::new(p) { b.add(g); }
        }
        if let Ok(set) = b.build() {
            out.push((sc.clone(), set));
        }
    }
    out
}

pub const TEMPLATE: &str = r#"# Repo-local ADR CLI config (adr-rag)

# One or more directories to scan or read an index from.
bases = [
  "docs/masterplan",
  # "docs/notes",
]

# Where to read/write the index and semantic groups (paths are relative to each base).
index_relative = "index/adr-index.json"
groups_relative = "index/semantic-groups.json"

# Discovery and semantics
file_patterns = ["ADR-*.md", "ADR-DB-*.md", "IMP-*.md"]
ignore_globs  = ["**/node_modules/**", "**/.obsidian/**"]
allowed_statuses = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]

[defaults]
depth = 2
include_bidirectional = true
include_content = true

# Note Types (Schema) — Optional, per-type rules and validation
#
# Define one or more [[schema]] blocks to validate different note types
# (e.g., ADR vs IMP). Matching is by file_patterns; first match wins.
# Unknown keys policy lets you treat unexpected front-matter as ignore|warn|error.
#
# [[schema]]
# name = "ADR"
# file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
# required = ["id", "tags", "status", "depends_on"]
# unknown_policy = "ignore"   # ignore | warn | error (default: ignore)
# allowed_keys = ["produces", "files_touched"]  # optional pass-through keys
#
# [schema.rules.status]
# allowed = [
#   "draft", "incomplete", "proposed", "accepted",
#   "complete", "design", "legacy-reference", "superseded"
# ]
# severity = "error"          # error | warn
#
# [schema.rules.depends_on]
# type = "array"
# items = { type = "string", regex = "^(ADR|IMP)-\\d+" }
# refers_to_types = ["ADR", "IMP"]
# severity = "error"
#
# [[schema]]
# name = "IMP"
# file_patterns = ["IMP-*.md"]
# required = ["id","tags","depends_on","status","completion_date"]
# unknown_policy = "warn"
#
# [schema.rules.status]
# allowed = ["in-progress","blocked","on-hold","cancelled","done"]
# severity = "error"
#
# [schema.rules.completion_date]
# type = "date"
# format = "%Y-%m-%d"
# severity = "warn"

# Default schemas (enabled): tweak as needed

[[schema]]
name = "ADR"
file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
required = ["id", "tags", "status", "depends_on"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]
severity = "error"

[[schema]]
name = "IMP"
file_patterns = ["IMP-*.md"]
required = ["id", "tags", "depends_on", "status"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = ["in-progress", "blocked", "on-hold", "cancelled", "done"]
severity = "error"
"#;
</file>

<file path="src/discovery.rs">
use anyhow::{Context, Result};
use globwalk::GlobWalkerBuilder;
use globset::{Glob, GlobSetBuilder};
use serde_json::Value;
use std::fs;
use std::path::Path;

use crate::config::Config;
use crate::model::{AdrDoc, parse_front_matter_and_title, file_mtime, file_size};

pub fn scan_docs(cfg: &Config) -> Result<Vec<AdrDoc>> {
    let mut docs = Vec::new();
    let mut ig_builder = GlobSetBuilder::new();
    for pat in &cfg.ignore_globs { ig_builder.add(Glob::new(pat)?); }
    let ignore_set = ig_builder.build()?;
    for base in &cfg.bases {
        for pattern in &cfg.file_patterns {
            let builder = GlobWalkerBuilder::from_patterns(base, &[pattern.as_str()]);
            let walker = builder.build()?;
            for entry in walker.filter_map(Result::ok) {
                let path = entry.path().to_path_buf();
                if path.is_file() {
                    if ignore_set.is_match(&path) { continue; }
                    let content = fs::read_to_string(&path).with_context(|| format!("reading {:?}", path))?;
                    let doc = parse_front_matter_and_title(&content, &path);
                    docs.push(doc);
                }
            }
        }
    }
    Ok(docs)
}

pub fn scan_docs_in_base(base: &Path, cfg: &Config) -> Result<Vec<AdrDoc>> {
    let mut docs = Vec::new();
    let mut ig_builder = GlobSetBuilder::new();
    for pat in &cfg.ignore_globs { ig_builder.add(Glob::new(pat)?); }
    let ignore_set = ig_builder.build()?;
    for pattern in &cfg.file_patterns {
        let builder = GlobWalkerBuilder::from_patterns(base, &[pattern.as_str()]);
        let walker = builder.build()?;
        for entry in walker.filter_map(Result::ok) {
            let path = entry.path().to_path_buf();
            if path.is_file() {
                if ignore_set.is_match(&path) { continue; }
                let content = fs::read_to_string(&path).with_context(|| format!("reading {:?}", path))?;
                let doc = parse_front_matter_and_title(&content, &path);
                docs.push(doc);
            }
        }
    }
    Ok(docs)
}

pub fn load_docs_from_index(base: &Path, cfg: &Config) -> Result<Vec<AdrDoc>> {
    let index_path = base.join(&cfg.index_relative);
    let data = fs::read_to_string(&index_path).with_context(|| format!("reading index {:?}", index_path))?;
    let root: Value = serde_json::from_str(&data).with_context(|| format!("parsing index {:?}", index_path))?;
    let mut docs = Vec::new();
    let items_opt: Option<&Vec<Value>> = if let Some(items) = root.as_array() {
        Some(items)
    } else if let Some(items) = root.get("items").and_then(|v| v.as_array()) {
        Some(items)
    } else { None };
    if let Some(items) = items_opt {
        for item in items {
            let file_rel = item.get("file").and_then(|v| v.as_str()).unwrap_or("");
            let file = base.join(file_rel);
            let id = item.get("id").and_then(|v| v.as_str()).map(|s| s.to_string());
            let title = item.get("title").and_then(|v| v.as_str()).unwrap_or("").to_string();
            let tags = item.get("tags").and_then(|v| v.as_array()).map(|a| a.iter().filter_map(|x| x.as_str().map(|s| s.to_string())).collect()).unwrap_or_else(|| Vec::new());
            let status = item.get("status").and_then(|v| v.as_str()).map(|s| s.to_string());
            let groups = item.get("groups").and_then(|v| v.as_array()).map(|a| a.iter().filter_map(|x| x.as_str().map(|s| s.to_string())).collect()).unwrap_or_else(|| Vec::new());
            let depends_on = item.get("depends_on").and_then(|v| v.as_array()).map(|a| a.iter().filter_map(|x| x.as_str().map(|s| s.to_string())).collect()).unwrap_or_else(|| Vec::new());
            let supersedes = match item.get("supersedes") {
                Some(Value::Array(a)) => a.iter().filter_map(|x| x.as_str().map(|s| s.to_string())).collect(),
                Some(Value::String(s)) => vec![s.to_string()],
                _ => Vec::new(),
            };
            let superseded_by = match item.get("superseded_by") {
                Some(Value::Array(a)) => a.iter().filter_map(|x| x.as_str().map(|s| s.to_string())).collect(),
                Some(Value::String(s)) => vec![s.to_string()],
                _ => Vec::new(),
            };
            let mtime = item.get("mtime").and_then(|v| v.as_u64());
            let size = item.get("size").and_then(|v| v.as_u64());
            docs.push(AdrDoc { file, id, title, tags, status, groups, depends_on, supersedes, superseded_by, fm: std::collections::BTreeMap::new(), mtime, size });
        }
    }
    Ok(docs)
}

pub fn load_docs(cfg: &Config) -> Result<Vec<AdrDoc>> {
    let mut combined: Vec<AdrDoc> = Vec::new();
    for base in &cfg.bases {
        let index_path = base.join(&cfg.index_relative);
        let use_index = index_path.exists();
        let mut docs = if use_index { load_docs_from_index(base, cfg)? } else { scan_docs_in_base(base, cfg)? };
        combined.append(&mut docs);
    }
    Ok(dedupe_by_id(combined))
}

pub fn incremental_collect_docs(cfg: &Config, full_rescan: bool) -> Result<Vec<AdrDoc>> {
    let mut ig_builder = GlobSetBuilder::new();
    for pat in &cfg.ignore_globs { ig_builder.add(Glob::new(pat)?); }
    let ignore_set = ig_builder.build()?;
    let mut combined: Vec<AdrDoc> = Vec::new();
    for base in &cfg.bases {
        let mut prior = std::collections::HashMap::<String, AdrDoc>::new();
        let index_path = base.join(&cfg.index_relative);
        if index_path.exists() {
            let idx_docs = load_docs_from_index(base, cfg)?;
            for d in idx_docs {
                if let Ok(rel) = d.file.strip_prefix(base) { prior.insert(rel.to_string_lossy().to_string(), d); }
            }
        }
        let mut seen_rel: std::collections::HashSet<String> = std::collections::HashSet::new();
        for pattern in &cfg.file_patterns {
            let walker = GlobWalkerBuilder::from_patterns(base, &[pattern.as_str()]).build()?;
            for entry in walker.filter_map(Result::ok) {
                let path = entry.path().to_path_buf();
                if !path.is_file() { continue; }
                if ignore_set.is_match(&path) { continue; }
                let rel = path.strip_prefix(base).unwrap_or(&path).to_string_lossy().to_string();
                seen_rel.insert(rel.clone());
                let cur_mtime = file_mtime(&path).unwrap_or(0);
                let cur_size = file_size(&path).unwrap_or(0);
                let need_parse = full_rescan || match prior.get(&rel) {
                    None => true,
                    Some(old) => old.mtime.unwrap_or(0) != cur_mtime || old.size.unwrap_or(0) != cur_size,
                };
                if need_parse {
                    let content = fs::read_to_string(&path).with_context(|| format!("reading {:?}", path))?;
                    let mut doc = parse_front_matter_and_title(&content, &path);
                    doc.mtime = Some(cur_mtime);
                    doc.size = Some(cur_size);
                    combined.push(doc);
                } else {
                    if let Some(mut d_old) = prior.get(&rel).cloned() {
                        d_old.mtime = Some(cur_mtime);
                        d_old.size = Some(cur_size);
                        combined.push(d_old);
                    }
                }
            }
        }
        // Removed files implicitly dropped
    }
    Ok(dedupe_by_id(combined))
}

fn dedupe_by_id(mut docs: Vec<AdrDoc>) -> Vec<AdrDoc> {
    use std::collections::HashMap;
    let mut by_id: HashMap<String, AdrDoc> = HashMap::new();
    let mut no_id: Vec<AdrDoc> = Vec::new();
    for d in docs.drain(..) {
        if let Some(id) = &d.id {
            let replace = match by_id.get(id) {
                Some(existing) => {
                    let a = existing.mtime;
                    let b = d.mtime;
                    match (a, b) { (Some(a), Some(b)) => b > a, (None, Some(_)) => true, _ => false }
                }
                None => true,
            };
            if replace { by_id.insert(id.clone(), d); }
        } else {
            no_id.push(d);
        }
    }
    let mut out: Vec<AdrDoc> = by_id.into_values().collect();
    out.extend(no_id);
    out
}
</file>

<file path="src/graph.rs">
use std::collections::{BTreeSet, HashMap, HashSet, VecDeque};

use crate::model::AdrDoc;

// Compute a dependency path between two ADR ids using BFS over
// a bidirectional graph (depends_on edges + reverse dependents).
pub fn bfs_path(
    from: &str,
    to: &str,
    max_depth: usize,
    by_id: &HashMap<String, AdrDoc>,
) -> Option<Vec<String>> {
    if from == to { return Some(vec![from.into()]); }
    let mut q: VecDeque<(String, Vec<String>, usize)> = VecDeque::new();
    let mut visited = HashSet::new();
    q.push_back((from.into(), vec![from.into()], 0));
    visited.insert(from.into());
    while let Some((cur, path, depth)) = q.pop_front() {
        if depth >= max_depth { continue; }
        if let Some(doc) = by_id.get(&cur) {
            let mut neighbors: BTreeSet<String> = BTreeSet::new();
            for dep in &doc.depends_on { neighbors.insert(dep.clone()); }
            for (oid, other) in by_id.iter() { if other.depends_on.iter().any(|d| d == &cur) { neighbors.insert(oid.clone()); } }
            for n in neighbors {
                if n == to { let mut p = path.clone(); p.push(n); return Some(p); }
                if !visited.contains(&n) { visited.insert(n.clone()); let mut p = path.clone(); p.push(n.clone()); q.push_back((n, p, depth+1)); }
            }
        }
    }
    None
}

// Compute a cluster around an ADR id up to a depth, optionally including dependents.
pub fn compute_cluster(
    id: &str,
    depth: usize,
    include_bidirectional: bool,
    by_id: &HashMap<String, AdrDoc>,
) -> std::collections::BTreeMap<String, AdrDoc> {
    let mut visited = HashSet::new();
    let mut cluster: std::collections::BTreeMap<String, AdrDoc> = std::collections::BTreeMap::new();
    fn traverse(
        current: &str,
        depth: usize,
        include_bidir: bool,
        by_id: &HashMap<String, AdrDoc>,
        acc: &mut std::collections::BTreeMap<String, AdrDoc>,
        visited: &mut HashSet<String>,
    ) {
        if depth == 0 || visited.contains(current) { return; }
        visited.insert(current.to_string());
        if let Some(doc) = by_id.get(current) {
            acc.insert(current.to_string(), doc.clone());
            for dep in &doc.depends_on { traverse(dep, depth-1, include_bidir, by_id, acc, visited); }
            if include_bidir {
                for (oid, other) in by_id.iter() {
                    if other.depends_on.iter().any(|d| d == current) {
                        traverse(oid, depth-1, include_bidir, by_id, acc, visited);
                    }
                }
            }
        }
    }
    traverse(id, depth, include_bidirectional, by_id, &mut cluster, &mut visited);
    cluster
}
</file>

<file path="src/index.rs">
use anyhow::{Context, Result};
use std::fs;
use std::time::{Duration, SystemTime};

use crate::config::{Config, build_schema_sets};
use crate::model::AdrDoc;

pub fn write_indexes(cfg: &Config, docs: &Vec<AdrDoc>, _force: bool, _auto_write: bool) -> Result<()> {
    let schema_sets = build_schema_sets(cfg);
    for base in &cfg.bases {
        let mut list = Vec::new();
        for d in docs {
            if d.file.starts_with(base) {
                let mut note_type: Option<String> = None;
                let fname = d.file.file_name().and_then(|s| s.to_str()).unwrap_or("");
                for (sc, set) in &schema_sets { if set.is_match(fname) { note_type = Some(sc.name.clone()); break; } }
                list.push(serde_json::json!({
                    "file": d.file.strip_prefix(base).unwrap_or(&d.file).to_string_lossy(),
                    "id": d.id.clone().unwrap_or_default(),
                    "title": d.title,
                    "tags": d.tags,
                    "status": d.status.clone().unwrap_or_default(),
                    "depends_on": d.depends_on,
                    "supersedes": d.supersedes,
                    "superseded_by": d.superseded_by,
                    "groups": d.groups,
                    "type": note_type,
                    "mtime": d.mtime,
                    "size": d.size,
                }));
            }
        }
        let now = SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).unwrap_or(Duration::from_secs(0)).as_secs();
        let wrapper = serde_json::json!({
            "index_version": 1,
            "generated_at": now,
            "items": list,
        });
        let out_path = base.join(&cfg.index_relative);
        if let Some(parent) = out_path.parent() { fs::create_dir_all(parent).ok(); }
        fs::write(&out_path, serde_json::to_string_pretty(&wrapper)?).with_context(|| format!("writing index to {}", out_path.display()))?;
        println!("Wrote index: {} ({} entries)", out_path.display(), wrapper.get("items").and_then(|v| v.as_array()).map(|a| a.len()).unwrap_or(0));
    }
    Ok(())
}

pub fn write_groups_config(cfg: &Config, docs: &Vec<AdrDoc>) -> Result<()> {
    use std::collections::{BTreeMap, BTreeSet};
    let mut by_group: BTreeMap<String, BTreeSet<String>> = BTreeMap::new();
    for d in docs { if let Some(ref id) = d.id { for g in &d.groups { by_group.entry(g.clone()).or_default().insert(id.clone()); } } }
    let mut sections = Vec::new();
    for (title, ids) in by_group { sections.push(serde_json::json!({ "title": title, "selectors": [ { "anyIds": ids.into_iter().collect::<Vec<_>>() } ] })); }
    for base in &cfg.bases {
        let out_path = base.join(&cfg.groups_relative);
        if let Some(parent) = out_path.parent() { fs::create_dir_all(parent).ok(); }
        let body = serde_json::json!({ "sections": sections });
        fs::write(&out_path, serde_json::to_string_pretty(&body)?).with_context(|| format!("writing groups to {}", out_path.display()))?;
        println!("Wrote groups: {}", out_path.display());
    }
    Ok(())
}
</file>

<file path="src/lib.rs">
pub mod config;
pub mod discovery;
pub mod graph;
pub mod index;
pub mod model;
pub mod util;
pub mod validate;
pub mod watch;
pub mod cli;
pub mod commands;
</file>

<file path="src/model.rs">
use anyhow::Result;
use regex::Regex;
use serde::Deserialize;
use std::collections::BTreeMap;
use std::fs;
use std::path::{Path, PathBuf};
use std::time::{Duration, SystemTime};

#[derive(Debug, Deserialize, Clone, Default)]
pub struct FrontMatter {
    pub id: Option<String>,
    pub tags: Option<Vec<String>>,
    pub status: Option<String>,
    pub groups: Option<Vec<String>>, 
    pub depends_on: Option<Vec<String>>, 
    pub supersedes: Option<OneOrMany>,
    pub superseded_by: Option<OneOrMany>,
}

#[derive(Debug, Deserialize, Clone)]
#[serde(untagged)]
pub enum OneOrMany {
    One(String),
    Many(Vec<String>),
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct AdrDoc {
    pub file: PathBuf,
    pub id: Option<String>,
    pub title: String,
    pub tags: Vec<String>,
    pub status: Option<String>,
    pub groups: Vec<String>,
    pub depends_on: Vec<String>,
    pub supersedes: Vec<String>,
    pub superseded_by: Vec<String>,
    pub fm: BTreeMap<String, serde_yaml::Value>,
    pub mtime: Option<u64>,
    pub size: Option<u64>,
}

pub fn parse_front_matter_and_title(content: &str, path: &Path) -> AdrDoc {
    let mut fm: FrontMatter = FrontMatter::default();
    let mut fm_map: BTreeMap<String, serde_yaml::Value> = BTreeMap::new();
    let mut body = content;
    // Normalize line endings for delimiter scanning
    let norm = content.replace("\r\n", "\n");
    if norm.starts_with("---\n") || norm.starts_with("+++\n") {
        let delim = if norm.starts_with("---\n") { "---" } else { "+++" };
        let start = 4; // skip delimiter and newline
        // find closing delimiter on its own line
        let needle = format!("\n{}\n", delim);
        let end_opt = norm[start..].find(&needle).map(|i| start + i);
        let (fm_text, body_start) = if let Some(end) = end_opt {
            (&norm[start..end], end + needle.len())
        } else if norm[start..].ends_with(&format!("\n{}", delim)) {
            let end = norm.len() - (delim.len() + 1);
            (&norm[start..end], end + delim.len() + 1)
        } else {
            ("", start)
        };
        if !fm_text.is_empty() {
            if delim == "---" {
                if let Ok(parsed) = serde_yaml::from_str::<FrontMatter>(fm_text) { fm = parsed; }
                if let Ok(mapping) = serde_yaml::from_str::<serde_yaml::Mapping>(fm_text) {
                    for (k, v) in mapping { if let Some(key) = k.as_str() { fm_map.insert(key.to_string(), v); } }
                }
            } else {
                // +++ TOML front matter
                if let Ok(parsed) = toml::from_str::<FrontMatter>(fm_text) { fm = parsed; }
                if let Ok(tval) = toml::from_str::<toml::Value>(fm_text) {
                    if let Some(table) = tval.as_table() {
                        for (k, _v) in table.iter() { fm_map.insert(k.clone(), serde_yaml::Value::Null); }
                    }
                }
            }
            // Map body slice back to original content by position if possible
            let offset = body_start.min(norm.len());
            let tail = &norm[offset..];
            body = tail;
        }
    }
    let title_re = Regex::new(r"(?m)^#\s+(.+)$").unwrap();
    let title = title_re
        .captures(body)
        .and_then(|c| c.get(1).map(|m| m.as_str().trim().to_string()))
        .unwrap_or_else(|| path.file_name().unwrap_or_default().to_string_lossy().to_string());

    AdrDoc {
        file: path.to_path_buf(),
        id: fm.id,
        title,
        tags: fm.tags.unwrap_or_default(),
        status: fm.status,
        groups: fm.groups.unwrap_or_default(),
        depends_on: fm.depends_on.unwrap_or_default(),
        supersedes: match fm.supersedes { Some(OneOrMany::One(s)) => vec![s], Some(OneOrMany::Many(v)) => v, None => vec![] },
        superseded_by: match fm.superseded_by { Some(OneOrMany::One(s)) => vec![s], Some(OneOrMany::Many(v)) => v, None => vec![] },
        fm: fm_map,
        mtime: file_mtime(path).ok(),
        size: file_size(path).ok(),
    }
}

pub fn file_mtime(p: &Path) -> Result<u64> {
    let md = fs::metadata(p)?;
    let m = md.modified()?;
    let d = m.duration_since(SystemTime::UNIX_EPOCH).unwrap_or(Duration::from_secs(0));
    Ok(d.as_secs())
}

pub fn file_size(p: &Path) -> Result<u64> {
    let md = fs::metadata(p)?;
    Ok(md.len())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_parse_front_matter_and_title() {
        let md = r#"---
id: ADR-123
tags: [a, b]
status: proposed
groups: ["Tools & Execution"]
depends_on: [ADR-100]
supersedes: ADR-050
---

# ADR-123: Sample

Body here.
"#;
        let path = Path::new("/tmp/ADR-123-sample.md");
        let doc = parse_front_matter_and_title(md, path);
        assert_eq!(doc.id.as_deref(), Some("ADR-123"));
        assert_eq!(doc.title, "ADR-123: Sample");
        assert_eq!(doc.status.as_deref(), Some("proposed"));
        assert_eq!(doc.tags, vec!["a", "b"]);
        assert_eq!(doc.groups, vec!["Tools & Execution".to_string()]);
        assert_eq!(doc.depends_on, vec!["ADR-100".to_string()]);
        assert_eq!(doc.supersedes, vec!["ADR-050".to_string()]);
    }
}
</file>

<file path="src/util.rs">
use anyhow::{anyhow, Result};
use std::path::Path;

pub fn try_open_editor(path: &Path) -> Result<()> {
    let editors = vec![
        std::env::var("VISUAL").ok(),
        std::env::var("EDITOR").ok(),
        Some("nano".into()),
        Some("vi".into()),
        Some("vim".into()),
    ];
    for ed in editors.into_iter().flatten() {
        let status = std::process::Command::new(ed.clone()).arg(path).status();
        if let Ok(st) = status { if st.success() { return Ok(()); } }
    }
    Err(anyhow!("no editor found or editor failed"))
}
</file>

<file path="src/validate.rs">
use crate::config::{Config, SchemaCfg, build_schema_sets};
use crate::model::AdrDoc;

#[derive(Debug, Clone)]
pub struct ValidationReport {
    pub ok: bool,
    pub errors: Vec<String>,
    pub warnings: Vec<String>,
    pub doc_count: usize,
    pub id_count: usize,
}

// Validate ADR docs against config: statuses, ids, duplicates/conflicts, references.
pub fn validate_docs(cfg: &Config, docs: &Vec<AdrDoc>) -> ValidationReport {
    use std::collections::{BTreeSet, HashMap};
    let mut errors: Vec<String> = Vec::new();
    let mut warnings: Vec<String> = Vec::new();
    let mut id_to_docs: HashMap<String, Vec<AdrDoc>> = HashMap::new();
    // Pre-compile schema globsets
    let schema_sets: Vec<(SchemaCfg, globset::GlobSet)> = build_schema_sets(cfg);
    let mut doc_schema: HashMap<String, String> = HashMap::new(); // id -> schema name
    for d in docs {
        if let Some(ref id) = d.id {
            id_to_docs.entry(id.clone()).or_default().push(d.clone());
            // assign schema by file name
            let fname = d.file.file_name().and_then(|s| s.to_str()).unwrap_or("");
            for (sc, set) in &schema_sets {
                if set.is_match(fname) { doc_schema.insert(id.clone(), sc.name.clone()); break; }
            }
        } else {
            errors.push(format!("{}: missing id", d.file.display()));
        }
    }
    // Validate basic status against global list only if no schema rule for status applies
    for d in docs {
        if let Some(ref st) = d.status {
            let mut has_schema_status_rule = false;
            if let Some(ref id) = d.id {
                if let Some(sname) = doc_schema.get(id) {
                    if let Some(sc) = cfg.schema.iter().find(|s| &s.name == sname) {
                        if sc.rules.contains_key("status") { has_schema_status_rule = true; }
                    }
                }
            }
            if !has_schema_status_rule {
                if !cfg.allowed_statuses.iter().any(|s| s == st) {
                    errors.push(format!("{}: invalid status '{}'", d.file.display(), st));
                }
            }
        }
    }
    for (id, lst) in &id_to_docs {
        if lst.len() > 1 {
            let mut titles: BTreeSet<String> = BTreeSet::new();
            let mut statuses: BTreeSet<String> = BTreeSet::new();
            for d in lst {
                titles.insert(d.title.clone());
                if let Some(ref s) = d.status { statuses.insert(s.clone()); }
            }
            let files = lst.iter().map(|d| d.file.display().to_string()).collect::<Vec<_>>().join(", ");
            if titles.len() > 1 || statuses.len() > 1 {
                errors.push(format!("conflict for id {} (metadata differ) in: {}", id, files));
            } else {
                errors.push(format!("duplicate id {} in: {}", id, files));
            }
        }
    }
    let id_set: std::collections::BTreeSet<String> = id_to_docs.keys().cloned().collect();
    for d in docs {
        for dep in &d.depends_on { if !id_set.contains(dep) { errors.push(format!("{}: depends_on '{}' not found", d.file.display(), dep)); } }
        for s in &d.supersedes { if !id_set.contains(s) { errors.push(format!("{}: supersedes '{}' not found", d.file.display(), s)); } }
        for s in &d.superseded_by { if !id_set.contains(s) { errors.push(format!("{}: superseded_by '{}' not found", d.file.display(), s)); } }
    }
    // Schema-based validation (required keys, unknown policy, and rules)
    let reserved: BTreeSet<String> = [
        "id","tags","status","groups","depends_on","supersedes","superseded_by"
    ].into_iter().map(|s| s.to_string()).collect();
    for d in docs {
        // Skip schema checks if we didn't parse front matter (unchanged in incremental mode)
        if d.fm.is_empty() { continue; }
        let mut schema_opt: Option<SchemaCfg> = None;
        if let Some(ref id) = d.id {
            if let Some(sname) = doc_schema.get(id) {
                schema_opt = cfg.schema.iter().find(|s| &s.name == sname).cloned();
            }
        } else {
            continue;
        }
        if let Some(sc) = schema_opt {
            // Required keys: present and non-empty
            for key in &sc.required {
                if let Some(v) = d.fm.get(key) {
                    let empty = match v {
                        serde_yaml::Value::Null => true,
                        serde_yaml::Value::String(s) => s.trim().is_empty(),
                        serde_yaml::Value::Sequence(a) => a.is_empty(),
                        _ => false,
                    };
                    if empty { errors.push(format!("{}: required '{}' is empty", d.file.display(), key)); }
                } else {
                    errors.push(format!("{}: missing required '{}'", d.file.display(), key));
                }
            }
            // Unknown keys handling
            let present: BTreeSet<String> = d.fm.keys().cloned().collect();
            let rule_keys: BTreeSet<String> = sc.rules.keys().cloned().collect();
            let mut known: BTreeSet<String> = reserved.union(&rule_keys).cloned().collect();
            known = known.union(&sc.required.iter().cloned().collect()).cloned().collect();
            known = known.union(&sc.allowed_keys.iter().cloned().collect()).cloned().collect();
            let unknown: Vec<String> = present.difference(&known).cloned().collect();
            let policy = sc.unknown_policy.clone().unwrap_or_else(|| "ignore".into());
            if !unknown.is_empty() {
                match policy.as_str() {
                    "warn" => warnings.push(format!("{}: unknown keys: {}", d.file.display(), unknown.join(", "))),
                    "error" => errors.push(format!("{}: unknown keys: {}", d.file.display(), unknown.join(", "))),
                    _ => {}
                }
            }
            // Apply rules
            for (k, rule) in sc.rules.iter() {
                let sev_err = rule.severity.as_deref().unwrap_or("error") == "error";
                if let Some(val) = d.fm.get(k) {
                    // type checks
                    if let Some(t) = &rule.r#type {
                        match t.as_str() {
                            "array" => if !val.is_sequence() { if sev_err { errors.push(format!("{}: '{}' should be array", d.file.display(), k)); } else { warnings.push(format!("{}: '{}' should be array", d.file.display(), k)); } continue; },
                            "date" => {
                                if let Some(fmt) = &rule.format { if let Some(s) = val.as_str() {
                                    if chrono::NaiveDate::parse_from_str(s, fmt).is_err() {
                                        if sev_err { errors.push(format!("{}: '{}' not a valid date '{}', format {}", d.file.display(), k, s, fmt)); } else { warnings.push(format!("{}: '{}' not a valid date", d.file.display(), k)); }
                                    }
                                } }
                            }
                            _ => {}
                        }
                    }
                    if !rule.allowed.is_empty() {
                        match val {
                            serde_yaml::Value::String(s) => {
                                if !rule.allowed.iter().any(|a| a == s) {
                                    if sev_err { errors.push(format!("{}: '{}' value '{}' not allowed", d.file.display(), k, s)); } else { warnings.push(format!("{}: '{}' value '{}' not allowed", d.file.display(), k, s)); }
                                }
                            }
                            serde_yaml::Value::Sequence(arr) => {
                                for v in arr {
                                    if let Some(s) = v.as_str() { if !rule.allowed.iter().any(|a| a == s) {
                                        if sev_err { errors.push(format!("{}: '{}' contains disallowed '{}'", d.file.display(), k, s)); } else { warnings.push(format!("{}: '{}' contains disallowed '{}'", d.file.display(), k, s)); }
                                    }}
                                }
                            }
                            _ => {}
                        }
                    }
                    if let Some(min) = rule.min_items {
                        if let Some(arr) = val.as_sequence() { if arr.len() < min { if sev_err { errors.push(format!("{}: '{}' must have at least {} items", d.file.display(), k, min)); } else { warnings.push(format!("{}: '{}' must have at least {} items", d.file.display(), k, min)); } } }
                    }
                    if let Some(rx) = &rule.regex {
                        if let Ok(re) = regex::Regex::new(rx) {
                            match val {
                                serde_yaml::Value::String(s) => {
                                    if !re.is_match(s) { if sev_err { errors.push(format!("{}: '{}' does not match regex", d.file.display(), k)); } else { warnings.push(format!("{}: '{}' does not match regex", d.file.display(), k)); } }
                                }
                                serde_yaml::Value::Sequence(arr) => {
                                    for v in arr { if let Some(s) = v.as_str() { if !re.is_match(s) { if sev_err { errors.push(format!("{}: '{}' element does not match regex", d.file.display(), k)); } else { warnings.push(format!("{}: '{}' element does not match regex", d.file.display(), k)); } } } }
                                }
                                _ => {}
                            }
                        }
                    }
                    if let Some(ref_types) = &rule.refers_to_types {
                        // Only applies to arrays of string IDs
                        if let Some(arr) = val.as_sequence() {
                            for v in arr {
                                if let Some(dep_id) = v.as_str() {
                                    if let Some(dep_docs) = id_to_docs.get(dep_id) {
                                        if let Some(dep_doc) = dep_docs.first() {
                                            if let Some(dep_doc_id) = &dep_doc.id {
                                                if let Some(dep_type) = doc_schema.get(dep_doc_id) {
                                                    if !ref_types.iter().any(|t| t == dep_type) {
                                                        if sev_err { errors.push(format!("{}: '{}' references {} of type '{}' not in {:?}", d.file.display(), k, dep_id, dep_type, ref_types)); } else { warnings.push(format!("{}: '{}' references '{}' of type '{}' not in {:?}", d.file.display(), k, dep_id, dep_type, ref_types)); }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    // Warn on isolated ADRs (no depends_on and no dependents). Valid, but highlighted.
    let mut has_dependent: std::collections::HashMap<String, bool> = std::collections::HashMap::new();
    for (id, _ds) in &id_to_docs { has_dependent.insert(id.clone(), false); }
    for d in docs { for dep in &d.depends_on { if let Some(x) = has_dependent.get_mut(dep) { *x = true; } } }
    for d in docs {
        if let Some(ref id) = d.id {
            let depends = d.depends_on.is_empty();
            let depended = !has_dependent.get(id).copied().unwrap_or(false);
            if depends && depended {
                warnings.push(format!("{}: '{}' has no graph connections (valid, but isolated)", d.file.display(), id));
            }
        }
    }
    let ok = errors.is_empty();
    ValidationReport { ok, errors, warnings, doc_count: docs.len(), id_count: id_to_docs.len() }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::{Config, default_index_rel, default_groups_rel, default_file_patterns, default_ignore_globs, default_allowed_statuses, default_defaults};
    use std::path::PathBuf;

    #[test]
    fn test_validate_docs_invalid_status_and_refs_and_duplicates() {
        let cfg = Config { 
            bases: vec![], index_relative: default_index_rel(), groups_relative: default_groups_rel(),
            file_patterns: default_file_patterns(), ignore_globs: default_ignore_globs(),
            allowed_statuses: default_allowed_statuses(), defaults: default_defaults(), schema: Vec::new()
        };
        let d1 = AdrDoc { file: PathBuf::from("X.md"), id: Some("X".into()), title: "X".into(), tags: vec![], status: Some("weird".into()), groups: vec![], depends_on: vec!["NOPE".into()], supersedes: vec![], superseded_by: vec![], fm: std::collections::BTreeMap::new(), mtime: None, size: None };
        let d2 = AdrDoc { file: PathBuf::from("A1.md"), id: Some("A".into()), title: "A v1".into(), tags: vec![], status: Some("draft".into()), groups: vec![], depends_on: vec![], supersedes: vec![], superseded_by: vec![], fm: std::collections::BTreeMap::new(), mtime: None, size: None };
        let d3 = AdrDoc { file: PathBuf::from("A2.md"), id: Some("A".into()), title: "A v2".into(), tags: vec![], status: Some("draft".into()), groups: vec![], depends_on: vec![], supersedes: vec![], superseded_by: vec![], fm: std::collections::BTreeMap::new(), mtime: None, size: None };
        let docs = vec![d1, d2, d3];
        let report = validate_docs(&cfg, &docs);
        assert!(!report.ok);
        let msg = report.errors.join("\n");
        assert!(msg.contains("invalid status"));
        assert!(msg.contains("depends_on 'NOPE' not found"));
        assert!(msg.contains("conflict for id A"));
    }
}
</file>

<file path="src/watch.rs">
use anyhow::Result;
use notify::{RecommendedWatcher, RecursiveMode, Watcher, Event};
use std::time::Duration;

use crate::config::Config;
use crate::discovery::incremental_collect_docs;
use crate::index::{write_indexes, write_groups_config};
use crate::validate::validate_docs;

pub struct WatchArgs {
    pub full_rescan: bool,
    pub debounce_ms: u64,
    pub dry_run: bool,
    pub write_groups: bool,
}

pub fn run_watch(cfg: &Config, args: WatchArgs) -> Result<()> {
    // Initial run
    {
        let docs = incremental_collect_docs(cfg, args.full_rescan)?;
        let report = validate_docs(cfg, &docs);
        if report.ok && !args.dry_run { write_indexes(cfg, &docs, true, true)?; }
        if !report.errors.is_empty() { eprintln!("Validation failed:"); for e in &report.errors { eprintln!(" - {}", e); } }
        if !report.warnings.is_empty() { eprintln!("Warnings:"); for w in &report.warnings { eprintln!(" - {}", w); } }
    }
    // Set up watchers
    let (tx, rx) = std::sync::mpsc::channel::<notify::Result<Event>>();
    let mut _watchers: Vec<RecommendedWatcher> = Vec::new();
    for base in &cfg.bases {
        let txc = tx.clone();
        let mut w = notify::recommended_watcher(move |res| { let _ = txc.send(res); })?;
        w.watch(base, RecursiveMode::Recursive)?;
        _watchers.push(w);
    }
    let debounce = Duration::from_millis(args.debounce_ms);
    loop {
        // Wait for an event, then debounce
        let _ = rx.recv();
        // Drain burst
        while let Ok(_) = rx.try_recv() {}
        std::thread::sleep(debounce);
        let docs = incremental_collect_docs(cfg, false)?;
        let report = validate_docs(cfg, &docs);
        if report.ok && !args.dry_run { write_indexes(cfg, &docs, true, true)?; }
        if args.write_groups && !args.dry_run { write_groups_config(cfg, &docs)?; }
        if !report.errors.is_empty() { eprintln!("Validation failed:"); for e in &report.errors { eprintln!(" - {}", e); } }
        if !report.warnings.is_empty() { eprintln!("Warnings:"); for w in &report.warnings { eprintln!(" - {}", w); } }
    }
}
</file>

<file path=".adr-rag.toml">
# Repo-local ADR CLI config (adr-rag)

# One or more directories to scan or read an index from.
bases = [
  "docs/masterplan",
  # "docs/notes",
]

# Where to read/write the index and semantic groups (paths are relative to each base).
index_relative = "index/adr-index.json"
groups_relative = "index/semantic-groups.json"

# Discovery and semantics
file_patterns = ["ADR-*.md", "ADR-DB-*.md", "IMP-*.md"]
ignore_globs  = ["**/node_modules/**", "**/.obsidian/**"]
allowed_statuses = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]

[defaults]
depth = 2
include_bidirectional = true
include_content = true

[output]
format = "plain" # plain | json

# Note Types (Schema) — Optional, per-type rules and validation
#
# Define one or more [[schema]] blocks to validate different note types
# (e.g., ADR vs IMP). Matching is by file_patterns; first match wins.
# Unknown keys policy lets you treat unexpected front-matter as ignore|warn|error.
#
# [[schema]]
# name = "ADR"
# file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
# required = ["id", "tags", "status", "depends_on"]
# unknown_policy = "ignore"   # ignore | warn | error (default: ignore)
# allowed_keys = ["produces", "files_touched"]  # optional pass-through keys
#
# [schema.rules.status]
# allowed = [
#   "draft", "incomplete", "proposed", "accepted",
#   "complete", "design", "legacy-reference", "superseded"
# ]
# severity = "error"          # error | warn
#
# [schema.rules.depends_on]
# type = "array"
# items = { type = "string", regex = "^(ADR|IMP)-\\d+" }
# refers_to_types = ["ADR", "IMP"]
# severity = "error"
#
# [[schema]]
# name = "IMP"
# file_patterns = ["IMP-*.md"]
# required = ["id","tags","depends_on","status","completion_date"]
# unknown_policy = "warn"
#
# [schema.rules.status]
# allowed = ["in-progress","blocked","on-hold","cancelled","done"]
# severity = "error"
#
# [schema.rules.completion_date]
# type = "date"
# format = "%Y-%m-%d"
# severity = "warn"
</file>

<file path="Cargo.toml">
[package]
name = "adr-rag"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = "1.0"
clap = { version = "4.5", features = ["derive"] }
clap_complete = "4.5"
globwalk = "0.8"
globset = "0.4"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
toml = "0.8"
walkdir = "2.5"
regex = "1.10"
chrono = { version = "0.4", default-features = false, features = ["alloc"] }
notify = "6.1"
rayon = "1.10"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
</file>

<file path="DEVLOG.md">
# adr-rag – Dev Handoff (quick log)

Date: 2025-08-22

Scope: Recent enhancements to the repo‑local ADR CLI (schemas, indexing, doctor, watch, incremental validate) aligned with IMP-004.

What changed
- Init UX: `adr-rag init` opens `.adr-rag.toml` in an editor by default; `--silent` to skip. Upward config discovery kept.
- Commands: `pathcmd` → `path`. Added `completions` (bash|zsh|fish). Global `--format` applies to all commands.
- Validate behavior: Scans markdown and writes per‑base JSON indexes by default on success; `--dry-run` avoids writes; `--full-rescan` forces full parse. Warn (don’t fail) on isolated nodes (no edges).
- Incremental validate: Index now stores `mtime` and `size`; we re‑parse only new/changed files, drop removed, then write updated indexes on success.
- Watch mode: `adr-rag watch` uses FS notifications with debounce to re‑validate incrementally and update indexes continuously.
- Index format: Now `{ generated_at: <unix_secs>, items: [...] }`; entries include `file`, `id`, `title`, `tags`, `status`, `depends_on`, `supersedes`, `superseded_by`, `groups`, `type`, `mtime`, `size`. Loader accepts legacy array‑only indexes as well.
- Schemas (note types): Optional `[[schema]]` blocks in TOML control per‑type rules. Enabled defaults for ADR and IMP. Unknown keys policy per type (ignore|warn|error; default ignore). Basic rules supported: enums (`allowed`), arrays (`type="array"`, `min_items`), regex, dates (`type="date"`, `format`), and `refers_to_types` for cross‑type deps. Index records include `type`.
- Doctor: Adds per‑type counts and unknown‑key stats, alongside config summary and conflict detection.
- README + template: Generalized defaults (no project‑specific paths). Documented schemas, unknown policy, completions, incremental validate, and watch.

Notable file touches
- `tools/adr-rag/src/main.rs`: command surface, schema parsing + validation, incremental collector, watch subcommand, doctor/reporting, index writer/loader.
- `tools/adr-rag/Cargo.toml`: added `notify`, `rayon` (for future parallel parse), `chrono` dropped in favor of std time for timestamps.
- `tools/adr-rag/README.md`: updated usage, flags, schema docs, and behavior notes.

Usage quickies
- Validate (incremental): `adr-rag validate` (writes indexes on success)
- Full rescan: `adr-rag validate --full-rescan`
- Dry run: `adr-rag validate --dry-run`
- Watch: `adr-rag watch` (debounce 400ms; writes on success)
- Doctor JSON: `adr-rag doctor --format json`
- Completions: `adr-rag completions bash|zsh|fish`

Open ideas / next steps
- Graph export: Add `graph` subcommand to emit Graphviz DOT for clusters or filtered views.
- Parallel parse: Wire up Rayon in incremental collector (changed files only) if needed.
- Git‑assisted mode (optional/later): Use git status to prune candidate set in very large repos.
- `config edit` subcommand to open resolved config directly.
- Doctor: optional per‑type unknown key listing when requested.

Notes
- Schema validation currently applies to files we parsed on this run (changed/new); cross‑ref checks still validate across all loaded docs.
- Multi‑base behavior: indexes and groups remain per‑base; read‑path picks up existing indexes when present, otherwise scans.

---

Refactor (2025-08-23)
- Split monolithic `main.rs` (~1.4k LOC) into cohesive modules under `src/`:
  - `config`, `model`, `discovery`, `index`, `validate`, `graph`, `util`, `watch`, plus `cli` and `commands/*`.
- Introduced `src/lib.rs` (library crate) and moved the binary entry to `src/bin/adr-rag.rs` to keep the bin thin.
- Normalized JSON printing via `commands::output::print_json`.
- Kept behavior the same; cargo build/test pass.
</file>

<file path="refactor-checklist.md">
# adr-rag Refactor Checklist

This checklist tracks the stepwise decomposition of `src/main.rs` into cohesive modules and command handlers. Mark items as you complete them.

## Prep
- [x] Write refactor plan and checklist next to DEVLOG (`refactor-checklist.md`).
- [x] Create `src/lib.rs` scaffold (exports, `run` entry placeholder).

## Phase 1 — Core Types & Graph
- [x] Extract `model.rs`: `FrontMatter`, `AdrDoc`, `parse_front_matter_and_title`, `file_mtime`, `file_size`.
- [x] Extract `graph.rs`: `bfs_path`, `compute_cluster` (+ small helpers), pure functions only.
- [x] Extract `config.rs`: `Config` + defaults, `find_config_upwards`, `load_config`, `write_template`.
- [x] Update imports and ensure `cargo build` passes.

## Phase 2 — Discovery & Index IO
- [x] Extract `discovery.rs`: `scan_docs`, `scan_docs_in_base`, `load_docs_from_index`, `load_docs`, `incremental_collect_docs`.
- [x] Extract `index.rs`: `write_indexes`, `write_groups_config`.
- [x] Centralize schema globset compilation (shared utility/state if needed).
- [x] Build and quick-run affected commands (`doctor`, `topics`).

## Phase 3 — Validation
- [x] Extract `validate.rs`: `SchemaCfg`, `SchemaRule`, and `validate_docs`.
- [x] Replace tuple return with `ValidationReport { ok, errors, warnings, doc_count, id_count }` (internal only; keep CLI output the same).
- [x] Move/expand unit tests for validation into module tests.

## Phase 4 — Utilities & Watcher
- [x] Extract `util.rs`: `try_open_editor`, small IO helpers, constants (consider moving `TEMPLATE` here).
- [x] Extract `watch.rs`: notify watcher setup, debounce, incremental validate/index write.
- [x] Keep `watch` logic orchestrational; reuse discovery/index/validate.

## Phase 5 — CLI Split & Output
- [x] Extract `cli.rs`: `Cli`, `Commands`, `ValidateArgs`, completions plumbing.
- [x] Create `src/commands/` and move handlers:
  - [x] `init.rs`
  - [x] `doctor.rs`
  - [x] `search.rs`
  - [x] `topics.rs`
  - [x] `group.rs`
  - [x] `get.rs`
  - [x] `cluster.rs`
  - [x] `path.rs`
  - [x] `validate_cmd.rs`
  - [x] `watch_cmd.rs`
  - [x] `completions.rs`
- [x] Wire `main.rs` to call command handlers and use `cli.rs` types.
- [x] Add small formatting helpers for plain/json output to avoid drift.

## Phase 6 — Main Thin Entry
- [x] Reduce `main.rs` to parse args and delegate to modular command handlers.
- [x] Move entry to `src/bin/adr-rag.rs`; keep logic in `src/lib.rs`.

## Phase 7 — Tests, Build, Docs
- [x] Ensure all existing tests pass; relocate module tests next to code.
- [x] `cargo build` and spot-check core commands.
- [x] Update `tools/adr-rag/README.md` with new structure.
- [ ] Add a short note in `DEVLOG.md` summarizing the refactor.

## Optional Improvements (post-refactor)
- [ ] Introduce `DocIndex` cache (id map, dependents map, compiled globsets).
- [ ] Add `note_type` to `AdrDoc` (computed once).
- [ ] Add `tracing` for watch/discovery debug logging (off by default).
</file>

<file path="ROADMAP-CHECKLIST.md">
# adr-rag — Post-Refactor Roadmap & Checklist

This checklist focuses on hardening core behavior first, then UX polish, then power features. We’ll keep scope tight per phase and prune as we learn.

## Handoff

- Status: Phase A core items implemented (write-index removed, de-dup by id, path-aware globs, index versioning, YAML/TOML front-matter + supersedes arrays, AdrDoc Serialize, initial NDJSON). Next: implement front-matter parser abstraction trait, then move to Phase B (tables/colors).

## Phase A — Core Correctness

- [x] Remove `--write-index` flag (simplify: auto-write on success; keep `--dry-run`)
  - [x] Drop flag from CLI and handler
  - [x] Update help/dispatch (README already aligned)
- [x] De-duplicate by `id` across bases in discovery
  - [x] Implement "newest mtime wins" policy in `load_docs` and incremental
  - [x] Keep `validate` conflict reporting for visibility
- [x] Path-aware ignore globs (Windows-friendly)
  - [x] Use `GlobSet::is_match(&Path)`; avoid lossy string conversions
- [x] Index versioning (backward compatible)
  - [x] Write `{ index_version, generated_at, items }`
  - [x] Read legacy array or object seamlessly
- [x] Front-matter parsing robustness
  - [x] Line-based delimiter scan supporting `---` (YAML) and `+++` (TOML), with CRLF + EOF handling
  - [x] Normalize `supersedes`/`superseded_by` from string|array → Vec
  - [ ] Parser abstraction trait; keep `serde_yaml` initially
- [x] `AdrDoc: Serialize`
  - [x] Derive `Serialize`
  - [ ] Centralize JSON/NDJSON printing helpers
- [x] NDJSON support (initial)
  - [x] Add `--format ndjson` for streaming results (search, topics, validate summary)
  - [ ] Extend to other commands as needed

## Phase B — UX Polish

- [ ] Plain output tables (doctor/topics/etc.) via `comfy_table`
- [ ] Colorize errors/warnings in `validate` via `anstream`
- [ ] Friendlier errors (e.g., “Config not found — run init?”)
- [ ] `--debug` logging toggle (env_logger/tracing)

## Phase C — Editor & CLI Ergonomics

- [ ] `edit --id ADR-XXX` (open file via `try_open_editor`)
- [ ] `config edit` (open `.adr-rag.toml`)
- [ ] Interactive `init` (dialoguer) with `--non-interactive` fallback

## Phase D — Links & Suggestions

- [ ] Obsidian-style `[[links]]` check (exact filename match)
  - [ ] Validate unresolved links; cross-base disambiguation strategy
- [ ] Fuzzy suggestions for unknown IDs (get/cluster/path)
  - [ ] Offer closest matches, accept with prompt or `--yes`

## Phase E — New Notes

- [ ] `new` command
  - [ ] Args: `type` (adr|imp), `title`, `depends_on`, `template?`
  - [ ] Filename from title; scaffold legal front-matter/body

## Phase F — Graph Enhancements

- [ ] DOT subgraphs by group/status (optional)
- [ ] Consistent graph JSON schema + tests

## Phase G — Heavy Lifts (Separate RFCs)

- [ ] `rename --id ADR-012 --new-id ADR-012R`
  - [ ] Dry-run diff, backups, atomic writes, re-validate, rollback plan
- [ ] `supersede --old ADR-008 --new ADR-031`
  - [ ] Update both notes’ front-matter; transactional write

## Dependencies & Parsing (Watchlist)

- [ ] YAML crate deprecation follow-up
  - [ ] Keep behind parsing trait; evaluate `serde_yml` path

## Acceptance Targets (per milestone)

- Phase A: de-dup, Windows globs, index versioning, `AdrDoc: Serialize`, NDJSON
- Phase B/C: tables + colors, editor commands, friendlier errors, debug logging
- Phase D: wiki link checks, ID suggestions
- Phase E: `new` scaffold
- Phase F: graph subgraphs + tests
</file>

<file path="README.md">
# adr-rag (Repo-local ADR CLI)

A small CLI to navigate/search/doctor ADRs using a simple per-repo TOML config. No dependency on Obsidian; works by scanning markdown + front matter.

## Project Layout

- `src/lib.rs`: Library crate exporting modules
  - `config` (TOML config, defaults, schema sets)
  - `model` (front-matter parsing, AdrDoc, file metadata)
  - `discovery` (scan, incremental collection, index loading)
  - `index` (write JSON index and semantic groups)
  - `validate` (schema + rules, returns `ValidationReport`)
  - `graph` (BFS path, dependency cluster)
  - `util` (helpers like `try_open_editor`)
  - `watch` (debounced filesystem watcher orchestration)
  - `cli` (Clap definitions: `Cli`, `Commands`, `ValidateArgs`)
  - `commands/*` (one handler per subcommand)
- `src/bin/adr-rag.rs`: Thin binary that parses CLI and delegates to the library

## Commands
- `init` — Create `.adr-rag.toml` in the current repo and open it in an editor by default. Flags:
  - `--force` overwrite if exists
  - `--print-template` print template to stdout
  - `--silent` do not open the config after creating/detecting it
- `doctor` — Show resolved config, bases, discovery mode (index vs scan), and quick stats.
  - Reports per-type counts when schemas are defined, and unknown-key stats.
  - JSON: use global `--format json`.
- `search --query <substr>` — Fuzzy search by ID/title across discovered ADR files.
- `topics` — List semantic groups derived from front matter (`groups` in ADRs).
- `group --topic "<name>" [--include-content]` — Show ADRs in a group; optionally include full content.
- `get --id ADR-021 [--include-dependents]` — Print an ADR with dependencies (and dependents if requested).
- `cluster --id ADR-021 [--depth N] [--include-bidirectional]` — Traverse dependencies (and dependents) to depth.
- `graph --id ADR-021 [--depth N] [--include-bidirectional] [--format mermaid|dot|json]` — Export a dependency graph around an ADR.
- `path --from ADR-011 --to ADR-038 [--max-depth N]` — Find a dependency path if any.
 - `validate [--format json] [--dry-run] [--full-rescan] [--write-groups]` — Validate front matter/refs; on success writes indexes (unless `--dry-run`).
  - Incremental by default: only reparses changed files using mtime/size. Use `--full-rescan` to force scanning all.
  - Exits non-zero if validation fails.
 - `watch [--debounce-ms 400] [--dry-run] [--full-rescan]` — Watch bases for changes and incrementally validate + update indexes.
  - Debounces rapid events; writes on success (unless `--dry-run`).

## Config: `.adr-rag.toml`
Created by `adr-rag init`. Example:

```
# Repo-local ADR CLI config (adr-rag)

bases = [
  "docs/masterplan",
  # "docs/notes",
]

index_relative = "index/adr-index.json"
groups_relative = "index/semantic-groups.json"

file_patterns = ["ADR-*.md", "ADR-DB-*.md", "IMP-*.md"]
ignore_globs  = ["**/node_modules/**", "**/.obsidian/**"]
allowed_statuses = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]

[defaults]
depth = 2
include_bidirectional = true
include_content = true

## Note Types (Schemas)
- Define `[[schema]]` blocks to validate different note types (e.g., ADR vs IMP).
- Unknown keys policy controls how unexpected front-matter is treated.
- Defaults: `unknown_policy = "ignore"`; required fields must be non-empty.

# Enabled defaults (edit as needed)
[[schema]]
name = "ADR"
file_patterns = ["ADR-*.md", "ADR-DB-*.md"]
required = ["id", "tags", "status", "depends_on"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = [
  "draft", "incomplete", "proposed", "accepted",
  "complete", "design", "legacy-reference", "superseded"
]
severity = "error"

[[schema]]
name = "IMP"
file_patterns = ["IMP-*.md"]
required = ["id", "tags", "depends_on", "status"]
unknown_policy = "ignore"

[schema.rules.status]
allowed = ["in-progress", "blocked", "on-hold", "cancelled", "done"]
severity = "error"
```

## Build & Run
- From repo root:
  - `cd tools/adr-rag`
  - `cargo build --release`
  - `./target/release/adr-rag init`
  - `./target/release/adr-rag doctor`
  - `./target/release/adr-rag doctor --format json`
  - `./target/release/adr-rag search -q sidecar`
  - `./target/release/adr-rag topics`
  - `./target/release/adr-rag group --topic "Tools & Execution"`
  - `./target/release/adr-rag get --id ADR-021`
  - `./target/release/adr-rag cluster --id ADR-021 --depth 3`
  - `./target/release/adr-rag graph --id ADR-021 --format mermaid`
  - `./target/release/adr-rag graph --id ADR-021 --format dot`
  - `./target/release/adr-rag graph --id ADR-021 --format json`
  - `./target/release/adr-rag path --from ADR-011 --to ADR-038`
  - `./target/release/adr-rag validate --format json --write-groups`

### Global Flags
- `--config <path>`: Explicit config path; otherwise searches upward for `.adr-rag.toml`.
- `--base <path1,path2>`: Override bases from config/env (comma-separated).
- `--format <plain|json>`: Output format for all commands.

Note: The `graph` subcommand uses its own `--format` allowing `mermaid`, `dot`, or `json`.

### Environment Variables
- `ADR_RAG_CONFIG`: Override config path (lower precedence than `--config`).
- `ADR_RAG_BASES`: Comma-separated bases override (lower precedence than `--base`).

Precedence: CLI flags > env vars > nearest `.adr-rag.toml` > tool defaults.

### Shell Completions
- Generate completions: `adr-rag completions bash|zsh|fish`.
- Example (bash): `adr-rag completions bash > ~/.local/share/bash-completion/adr-rag` then `source` it, or add to your shell init.

## Notes
- Multi-base merging is supported; results are de-duplicated by `id` (conflicts will be flagged in future `validate`).
- Front matter groups (e.g., `groups: ["Tools & Execution"]`) drive `topics`/`group`.
- See `docs/masterplan-v2/IMP-004-adr-rag-cli-and-config.md` for design details.

Index and Groups behavior
- Commands load from an index at `<base>/<index_relative>` if present; otherwise they scan markdown.
- `validate` scans markdown and, if there are no blocking errors, writes/upserts per-base JSON indexes by default.
- `validate --dry-run` prints errors/warnings but does not write.
 - Index entries include minimal metadata (`mtime`, `size`) to accelerate incremental runs.
- `validate --write-groups` also writes the semantic groups JSON to `<base>/<groups_relative>`.
- `topics` reads `groups_relative` if present; otherwise derives topics from ADR front matter.

Isolated ADRs
- ADRs with no `depends_on` and no inbound dependents are valid. `validate` will not fail but will emit warnings listing isolated IDs.
</file>

</files>
